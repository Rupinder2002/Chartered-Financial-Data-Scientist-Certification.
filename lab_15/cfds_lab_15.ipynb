{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"cfds_logo.png\">\n",
    "\n",
    "###  Lab 15 - \"Long Short-Term Memory (LSTM) Neural Networks (Many to One)\"\n",
    "\n",
    "Chartered Financial Data Scientist (CFDS), Autumn Term 2020/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to enhance the previous **\"one-to-one\"** Long Short-Term Memory (LSTM) neural network training setup towards a **\"many-to-one\"** training setup. This will allow a model to learn from a set of input time series such as the underlying constituents of an Exchange Traded Fund (ETF). In general, such a \"many-to-one\" training setup provides the ability to learn complex pattern across multiple input data sources.\n",
    "\n",
    "\n",
    "We will again use the functionality of the **PyTorch** library to implement and train an LSTM based neural network. The network will be trained on the historic daily (in-sample) returns of an exemplary financial stock. Once the network is trained, we will use the learned model to predict future (out-of-sample) returns. Finally, we will convert the predictions into tradable signals and the backtest the signals accordingly. \n",
    "\n",
    "The figure below illustrates a high-level view on the machine learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. Understand the basic concepts, intuitions and major building blocks of **Long Short-Term Memory (LSTM) Neural Networks**.\n",
    "> 2. Know how to **implement and to train a many-to-one LSTM** model of financial time-series data.\n",
    "> 3. Understand how to apply such a learned model to **predict future data points of a time-series**.\n",
    "> 4. Know how to **interpret the model's prediction results** and backtest the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our NextThought lab discussion forum (https://financial-data-science.nextthought.io), or send us an email (using our fds.ai email addresses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# Nvidia GTC 2016: \"The Deep Learning Revolution\" Opening in Keynote\"\n",
    "# YouTubeVideo('Dy0hJWltsyE', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress potential warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the `PyTorch`, `Numpy`, `Sklearn`, `Matplotlib`, `Seaborn`, `BT` and a few utility libraries throughout the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science and utility libraries\n",
    "import os, sys, itertools, urllib, io\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pandas_datareader as dr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Python `BT` backtesting library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the backtesting library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bt as bt # library to backtest trading signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python machine / deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python plotting libraries and set general plotting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable notebook matplotlib inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create notebook folder structure to store the data, the visuals, as well as the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data sub-directory inside the current directory\n",
    "data_directory = './data'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "    \n",
    "# create visuals sub-directory inside the current directory\n",
    "visuals_directory = './visuals'\n",
    "if not os.path.exists(visuals_directory): os.makedirs(visuals_directory)\n",
    "\n",
    "# create models sub-directory inside the current directory\n",
    "models_directory = './models'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory)\n",
    "    \n",
    "# create statistics sub-directory inside the current directory\n",
    "statistics_directory = './statistics'\n",
    "if not os.path.exists(statistics_directory): os.makedirs(statistics_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed value to obtain reproducable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value); # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable GPU computing by setting the `device` flag and init a `CUDA` seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-12:57:29] notebook with 'cpu' computation enabled\n"
     ]
    }
   ],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] notebook with \\'{}\\' computation enabled'.format(str(now), str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab notebook we will download and access historic daily stock market data ranging from **01/01/2010** to **31/12/2019** of the **\"Invesco QQQ ETF\"** (ticker symbol: \"QQQ\") that tracks the NASDAQ-100 index. Thereby, we will utilize the `datareader` of the `Pandas` library that provides the ability to interface the `Yahoo` finance API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Target Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the data download, let's specify the start and end date of the stock market data download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime(2010, 1, 1)\n",
    "end_date = dt.datetime(2019, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the daily **\"Invesco QQQ ETF\"** (QQQ) market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_price = dr.data.DataReader('QQQ', data_source='yahoo', start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top 10 records of the QQQ market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>46.490002</td>\n",
       "      <td>46.270000</td>\n",
       "      <td>46.330002</td>\n",
       "      <td>46.419998</td>\n",
       "      <td>62822800.0</td>\n",
       "      <td>41.570904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>46.500000</td>\n",
       "      <td>46.160000</td>\n",
       "      <td>46.389999</td>\n",
       "      <td>46.419998</td>\n",
       "      <td>62935600.0</td>\n",
       "      <td>41.570904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>46.549999</td>\n",
       "      <td>46.070000</td>\n",
       "      <td>46.400002</td>\n",
       "      <td>46.139999</td>\n",
       "      <td>96033000.0</td>\n",
       "      <td>41.320183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>46.270000</td>\n",
       "      <td>45.919998</td>\n",
       "      <td>46.209999</td>\n",
       "      <td>46.169998</td>\n",
       "      <td>77094100.0</td>\n",
       "      <td>41.347046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>46.549999</td>\n",
       "      <td>45.930000</td>\n",
       "      <td>46.070000</td>\n",
       "      <td>46.549999</td>\n",
       "      <td>88886600.0</td>\n",
       "      <td>41.687347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>46.639999</td>\n",
       "      <td>46.119999</td>\n",
       "      <td>46.610001</td>\n",
       "      <td>46.360001</td>\n",
       "      <td>104673400.0</td>\n",
       "      <td>41.517189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>46.139999</td>\n",
       "      <td>45.529999</td>\n",
       "      <td>46.080002</td>\n",
       "      <td>45.779999</td>\n",
       "      <td>90673900.0</td>\n",
       "      <td>40.997784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>46.490002</td>\n",
       "      <td>45.610001</td>\n",
       "      <td>45.919998</td>\n",
       "      <td>46.349998</td>\n",
       "      <td>100661000.0</td>\n",
       "      <td>41.508217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>46.520000</td>\n",
       "      <td>46.220001</td>\n",
       "      <td>46.259998</td>\n",
       "      <td>46.389999</td>\n",
       "      <td>75209000.0</td>\n",
       "      <td>41.544056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>46.549999</td>\n",
       "      <td>45.650002</td>\n",
       "      <td>46.470001</td>\n",
       "      <td>45.849998</td>\n",
       "      <td>126849300.0</td>\n",
       "      <td>41.060459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low       Open      Close       Volume  Adj Close\n",
       "Date                                                                          \n",
       "2010-01-04  46.490002  46.270000  46.330002  46.419998   62822800.0  41.570904\n",
       "2010-01-05  46.500000  46.160000  46.389999  46.419998   62935600.0  41.570904\n",
       "2010-01-06  46.549999  46.070000  46.400002  46.139999   96033000.0  41.320183\n",
       "2010-01-07  46.270000  45.919998  46.209999  46.169998   77094100.0  41.347046\n",
       "2010-01-08  46.549999  45.930000  46.070000  46.549999   88886600.0  41.687347\n",
       "2010-01-11  46.639999  46.119999  46.610001  46.360001  104673400.0  41.517189\n",
       "2010-01-12  46.139999  45.529999  46.080002  45.779999   90673900.0  40.997784\n",
       "2010-01-13  46.490002  45.610001  45.919998  46.349998  100661000.0  41.508217\n",
       "2010-01-14  46.520000  46.220001  46.259998  46.389999   75209000.0  41.544056\n",
       "2010-01-15  46.549999  45.650002  46.470001  45.849998  126849300.0  41.060459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_price.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the daily closing prices of the QQQ market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot daily adjusted closing price\n",
    "ax.plot(stock_data_price.index, stock_data_price['Adj Close'], color='#9b59b6')\n",
    "\n",
    "# rotate x-tick labels\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set x-axis labels and limits\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2010'), pd.to_datetime('31-12-2019')])\n",
    "\n",
    "# set y-axis labels and limits\n",
    "ax.set_ylabel('[QQQ closing price]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('Invesco QQQ Trust Series 1 - Daily Historical Adjusted Closing Prices', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the stock data `Pandas` dataframe by only keeping the **daily adjusted closing price** of the QQQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns other than the adjusted closing price\n",
    "stock_data_price = stock_data_price.drop(columns=['High', 'Open', 'Low', 'Close', 'Volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the adjusted closing price column according to the QQQ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_price = stock_data_price.rename(columns={'Adj Close': \"QQQ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Input Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ticker symbols of the top NASDAQ constituents to be retrieved from the `Yahoo` finance API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['MU', 'ISRG', 'TMUS', 'CME', 'ADP', 'INTU', 'BKNG', 'FISV', 'MDLZ', 'GILD', 'QCOM', 'SBUX', 'CHTR', 'TXN', 'AVGO', 'COST', 'PYPL', 'NFLX', 'NVDA', 'ADBE', 'PEP', 'CMCSA', 'CSCO', 'INTC', 'GOOGL', 'AMZN', 'AAPL', 'MSFT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the number of defined NASDAQ constituents ticker symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine number of input stock ticker symbols\n",
    "no_input_stocks = len(symbols)\n",
    "\n",
    "# print number of input stock ticker symboks\n",
    "no_input_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively download the **daily adjusted closing price data** of the defined NASDAQ constituents. Once downloaded we will add them to the stock data `Pandas` dataframe containing the QQQ market data that we initialized above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over distinct ticker symbols\n",
    "for symbol in symbols:\n",
    "    \n",
    "    # retrieve market data of current ticker symbol\n",
    "    symbol_data = dr.data.DataReader(symbol, data_source='yahoo', start=start_date, end=end_date)\n",
    "    \n",
    "    # collect the adjusted daily closing price of current ticker symbol\n",
    "    stock_data_price[symbol] = symbol_data['Adj Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 10 rows of the retrieved NASDAQ constituents daily adjusted closing price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>41.570904</td>\n",
       "      <td>10.85</td>\n",
       "      <td>102.923332</td>\n",
       "      <td>13.207044</td>\n",
       "      <td>44.038776</td>\n",
       "      <td>28.546648</td>\n",
       "      <td>28.015366</td>\n",
       "      <td>223.960007</td>\n",
       "      <td>12.3575</td>\n",
       "      <td>14.167932</td>\n",
       "      <td>...</td>\n",
       "      <td>16.999943</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>44.292305</td>\n",
       "      <td>6.894205</td>\n",
       "      <td>18.463966</td>\n",
       "      <td>14.954695</td>\n",
       "      <td>313.688690</td>\n",
       "      <td>133.899994</td>\n",
       "      <td>6.593423</td>\n",
       "      <td>24.105360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>41.570904</td>\n",
       "      <td>11.17</td>\n",
       "      <td>102.459999</td>\n",
       "      <td>13.240186</td>\n",
       "      <td>44.002773</td>\n",
       "      <td>28.393362</td>\n",
       "      <td>28.015366</td>\n",
       "      <td>226.559998</td>\n",
       "      <td>12.4100</td>\n",
       "      <td>14.860062</td>\n",
       "      <td>...</td>\n",
       "      <td>17.248186</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>44.827515</td>\n",
       "      <td>6.800767</td>\n",
       "      <td>18.381708</td>\n",
       "      <td>14.947535</td>\n",
       "      <td>312.307312</td>\n",
       "      <td>134.690002</td>\n",
       "      <td>6.604822</td>\n",
       "      <td>24.113148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>41.320183</td>\n",
       "      <td>11.22</td>\n",
       "      <td>103.946671</td>\n",
       "      <td>12.345354</td>\n",
       "      <td>43.704395</td>\n",
       "      <td>28.326714</td>\n",
       "      <td>27.624369</td>\n",
       "      <td>219.080002</td>\n",
       "      <td>12.3575</td>\n",
       "      <td>14.963362</td>\n",
       "      <td>...</td>\n",
       "      <td>17.358511</td>\n",
       "      <td>37.619999</td>\n",
       "      <td>44.379101</td>\n",
       "      <td>6.752012</td>\n",
       "      <td>18.262053</td>\n",
       "      <td>14.897401</td>\n",
       "      <td>304.434448</td>\n",
       "      <td>132.250000</td>\n",
       "      <td>6.499765</td>\n",
       "      <td>23.965164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>41.347046</td>\n",
       "      <td>10.84</td>\n",
       "      <td>103.556664</td>\n",
       "      <td>12.461351</td>\n",
       "      <td>43.758408</td>\n",
       "      <td>28.313377</td>\n",
       "      <td>27.533443</td>\n",
       "      <td>216.130005</td>\n",
       "      <td>12.3000</td>\n",
       "      <td>14.927206</td>\n",
       "      <td>...</td>\n",
       "      <td>17.018332</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>44.097023</td>\n",
       "      <td>6.894205</td>\n",
       "      <td>18.344311</td>\n",
       "      <td>14.754156</td>\n",
       "      <td>297.347351</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>6.487749</td>\n",
       "      <td>23.715933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>41.687347</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>41.517189</td>\n",
       "      <td>10.90</td>\n",
       "      <td>103.763336</td>\n",
       "      <td>11.914511</td>\n",
       "      <td>44.918518</td>\n",
       "      <td>28.400021</td>\n",
       "      <td>28.388178</td>\n",
       "      <td>218.910004</td>\n",
       "      <td>12.2275</td>\n",
       "      <td>14.875555</td>\n",
       "      <td>...</td>\n",
       "      <td>16.816057</td>\n",
       "      <td>36.209999</td>\n",
       "      <td>43.901749</td>\n",
       "      <td>6.829206</td>\n",
       "      <td>18.389183</td>\n",
       "      <td>15.004833</td>\n",
       "      <td>300.855865</td>\n",
       "      <td>130.309998</td>\n",
       "      <td>6.473268</td>\n",
       "      <td>23.575750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>40.997784</td>\n",
       "      <td>10.26</td>\n",
       "      <td>101.463333</td>\n",
       "      <td>10.522551</td>\n",
       "      <td>44.007900</td>\n",
       "      <td>28.200068</td>\n",
       "      <td>28.388178</td>\n",
       "      <td>213.979996</td>\n",
       "      <td>12.1200</td>\n",
       "      <td>15.128645</td>\n",
       "      <td>...</td>\n",
       "      <td>16.246025</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>44.516521</td>\n",
       "      <td>6.707326</td>\n",
       "      <td>18.097532</td>\n",
       "      <td>14.761323</td>\n",
       "      <td>295.535522</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>6.399635</td>\n",
       "      <td>23.419973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>41.508217</td>\n",
       "      <td>10.46</td>\n",
       "      <td>102.013336</td>\n",
       "      <td>10.489408</td>\n",
       "      <td>44.144257</td>\n",
       "      <td>28.406679</td>\n",
       "      <td>28.715527</td>\n",
       "      <td>214.300003</td>\n",
       "      <td>12.2225</td>\n",
       "      <td>15.097653</td>\n",
       "      <td>...</td>\n",
       "      <td>16.466686</td>\n",
       "      <td>36.279999</td>\n",
       "      <td>44.878143</td>\n",
       "      <td>6.812954</td>\n",
       "      <td>18.426571</td>\n",
       "      <td>15.011992</td>\n",
       "      <td>293.838837</td>\n",
       "      <td>129.110001</td>\n",
       "      <td>6.489905</td>\n",
       "      <td>23.638054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>41.544056</td>\n",
       "      <td>10.73</td>\n",
       "      <td>102.666664</td>\n",
       "      <td>10.323699</td>\n",
       "      <td>43.953918</td>\n",
       "      <td>28.326714</td>\n",
       "      <td>28.897388</td>\n",
       "      <td>208.770004</td>\n",
       "      <td>12.1875</td>\n",
       "      <td>15.040841</td>\n",
       "      <td>...</td>\n",
       "      <td>16.209249</td>\n",
       "      <td>35.900002</td>\n",
       "      <td>45.420586</td>\n",
       "      <td>6.772330</td>\n",
       "      <td>18.658400</td>\n",
       "      <td>15.384431</td>\n",
       "      <td>295.220215</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>6.452318</td>\n",
       "      <td>24.113148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>41.060459</td>\n",
       "      <td>10.13</td>\n",
       "      <td>102.860001</td>\n",
       "      <td>10.472838</td>\n",
       "      <td>43.373848</td>\n",
       "      <td>28.173401</td>\n",
       "      <td>28.606413</td>\n",
       "      <td>208.770004</td>\n",
       "      <td>11.9850</td>\n",
       "      <td>15.278432</td>\n",
       "      <td>...</td>\n",
       "      <td>15.731152</td>\n",
       "      <td>35.869999</td>\n",
       "      <td>45.051723</td>\n",
       "      <td>6.756077</td>\n",
       "      <td>18.247099</td>\n",
       "      <td>14.897401</td>\n",
       "      <td>290.290283</td>\n",
       "      <td>127.139999</td>\n",
       "      <td>6.344487</td>\n",
       "      <td>24.035263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  QQQ     MU        ISRG       TMUS        CME        ADP  \\\n",
       "Date                                                                        \n",
       "2010-01-04  41.570904  10.85  102.923332  13.207044  44.038776  28.546648   \n",
       "2010-01-05  41.570904  11.17  102.459999  13.240186  44.002773  28.393362   \n",
       "2010-01-06  41.320183  11.22  103.946671  12.345354  43.704395  28.326714   \n",
       "2010-01-07  41.347046  10.84  103.556664  12.461351  43.758408  28.313377   \n",
       "2010-01-08  41.687347  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-11  41.517189  10.90  103.763336  11.914511  44.918518  28.400021   \n",
       "2010-01-12  40.997784  10.26  101.463333  10.522551  44.007900  28.200068   \n",
       "2010-01-13  41.508217  10.46  102.013336  10.489408  44.144257  28.406679   \n",
       "2010-01-14  41.544056  10.73  102.666664  10.323699  43.953918  28.326714   \n",
       "2010-01-15  41.060459  10.13  102.860001  10.472838  43.373848  28.173401   \n",
       "\n",
       "                 INTU        BKNG     FISV       MDLZ  ...       NVDA  \\\n",
       "Date                                                   ...              \n",
       "2010-01-04  28.015366  223.960007  12.3575  14.167932  ...  16.999943   \n",
       "2010-01-05  28.015366  226.559998  12.4100  14.860062  ...  17.248186   \n",
       "2010-01-06  27.624369  219.080002  12.3575  14.963362  ...  17.358511   \n",
       "2010-01-07  27.533443  216.130005  12.3000  14.927206  ...  17.018332   \n",
       "2010-01-08  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-11  28.388178  218.910004  12.2275  14.875555  ...  16.816057   \n",
       "2010-01-12  28.388178  213.979996  12.1200  15.128645  ...  16.246025   \n",
       "2010-01-13  28.715527  214.300003  12.2225  15.097653  ...  16.466686   \n",
       "2010-01-14  28.897388  208.770004  12.1875  15.040841  ...  16.209249   \n",
       "2010-01-15  28.606413  208.770004  11.9850  15.278432  ...  15.731152   \n",
       "\n",
       "                 ADBE        PEP     CMCSA       CSCO       INTC       GOOGL  \\\n",
       "Date                                                                           \n",
       "2010-01-04  37.090000  44.292305  6.894205  18.463966  14.954695  313.688690   \n",
       "2010-01-05  37.700001  44.827515  6.800767  18.381708  14.947535  312.307312   \n",
       "2010-01-06  37.619999  44.379101  6.752012  18.262053  14.897401  304.434448   \n",
       "2010-01-07  36.889999  44.097023  6.894205  18.344311  14.754156  297.347351   \n",
       "2010-01-08  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-11  36.209999  43.901749  6.829206  18.389183  15.004833  300.855865   \n",
       "2010-01-12  35.660000  44.516521  6.707326  18.097532  14.761323  295.535522   \n",
       "2010-01-13  36.279999  44.878143  6.812954  18.426571  15.011992  293.838837   \n",
       "2010-01-14  35.900002  45.420586  6.772330  18.658400  15.384431  295.220215   \n",
       "2010-01-15  35.869999  45.051723  6.756077  18.247099  14.897401  290.290283   \n",
       "\n",
       "                  AMZN      AAPL       MSFT  \n",
       "Date                                         \n",
       "2010-01-04  133.899994  6.593423  24.105360  \n",
       "2010-01-05  134.690002  6.604822  24.113148  \n",
       "2010-01-06  132.250000  6.499765  23.965164  \n",
       "2010-01-07  130.000000  6.487749  23.715933  \n",
       "2010-01-08  133.520004  6.530882  23.879499  \n",
       "2010-01-11  130.309998  6.473268  23.575750  \n",
       "2010-01-12  127.349998  6.399635  23.419973  \n",
       "2010-01-13  129.110001  6.489905  23.638054  \n",
       "2010-01-14  127.349998  6.452318  24.113148  \n",
       "2010-01-15  127.139999  6.344487  24.035263  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_price.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the retrieved NASDAQ constituents daily adjusted closing price data the local data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the filename of the data to be saved\n",
    "filename = 'qqq_nasdaq_30_daily_closing.csv'\n",
    "\n",
    "# save retrieved data to local data directory\n",
    "stock_data_price.to_csv(os.path.join(data_directory, filename), sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will obtain daily returns of the retrieved daily closing prices. Also, we will convert the time-series of daily returns into a set of sequences $s$ of $n$ time steps respectively. The created sequences will then be used to learn a model using an Long Short-Term Memory neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Missing Data Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's always forward propagate the last valid available price information observation to the next available valid price information using the Panda's `reindex()` function. This in order to also obtain market price information of weekend's and holidays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill weekends and holidays\n",
    "stock_data_price = stock_data_price.reindex(index=pd.date_range(stock_data_price.index.min(), stock_data_price.index.max()), method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top padded stock market data records of the QQQ and the underlying NASDAQ Top 28 constituents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>41.570904</td>\n",
       "      <td>10.85</td>\n",
       "      <td>102.923332</td>\n",
       "      <td>13.207044</td>\n",
       "      <td>44.038776</td>\n",
       "      <td>28.546648</td>\n",
       "      <td>28.015366</td>\n",
       "      <td>223.960007</td>\n",
       "      <td>12.3575</td>\n",
       "      <td>14.167932</td>\n",
       "      <td>...</td>\n",
       "      <td>16.999943</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>44.292305</td>\n",
       "      <td>6.894205</td>\n",
       "      <td>18.463966</td>\n",
       "      <td>14.954695</td>\n",
       "      <td>313.688690</td>\n",
       "      <td>133.899994</td>\n",
       "      <td>6.593423</td>\n",
       "      <td>24.105360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>41.570904</td>\n",
       "      <td>11.17</td>\n",
       "      <td>102.459999</td>\n",
       "      <td>13.240186</td>\n",
       "      <td>44.002773</td>\n",
       "      <td>28.393362</td>\n",
       "      <td>28.015366</td>\n",
       "      <td>226.559998</td>\n",
       "      <td>12.4100</td>\n",
       "      <td>14.860062</td>\n",
       "      <td>...</td>\n",
       "      <td>17.248186</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>44.827515</td>\n",
       "      <td>6.800767</td>\n",
       "      <td>18.381708</td>\n",
       "      <td>14.947535</td>\n",
       "      <td>312.307312</td>\n",
       "      <td>134.690002</td>\n",
       "      <td>6.604822</td>\n",
       "      <td>24.113148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>41.320183</td>\n",
       "      <td>11.22</td>\n",
       "      <td>103.946671</td>\n",
       "      <td>12.345354</td>\n",
       "      <td>43.704395</td>\n",
       "      <td>28.326714</td>\n",
       "      <td>27.624369</td>\n",
       "      <td>219.080002</td>\n",
       "      <td>12.3575</td>\n",
       "      <td>14.963362</td>\n",
       "      <td>...</td>\n",
       "      <td>17.358511</td>\n",
       "      <td>37.619999</td>\n",
       "      <td>44.379101</td>\n",
       "      <td>6.752012</td>\n",
       "      <td>18.262053</td>\n",
       "      <td>14.897401</td>\n",
       "      <td>304.434448</td>\n",
       "      <td>132.250000</td>\n",
       "      <td>6.499765</td>\n",
       "      <td>23.965164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>41.347046</td>\n",
       "      <td>10.84</td>\n",
       "      <td>103.556664</td>\n",
       "      <td>12.461351</td>\n",
       "      <td>43.758408</td>\n",
       "      <td>28.313377</td>\n",
       "      <td>27.533443</td>\n",
       "      <td>216.130005</td>\n",
       "      <td>12.3000</td>\n",
       "      <td>14.927206</td>\n",
       "      <td>...</td>\n",
       "      <td>17.018332</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>44.097023</td>\n",
       "      <td>6.894205</td>\n",
       "      <td>18.344311</td>\n",
       "      <td>14.754156</td>\n",
       "      <td>297.347351</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>6.487749</td>\n",
       "      <td>23.715933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>41.687347</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>41.687347</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>41.687347</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>41.517189</td>\n",
       "      <td>10.90</td>\n",
       "      <td>103.763336</td>\n",
       "      <td>11.914511</td>\n",
       "      <td>44.918518</td>\n",
       "      <td>28.400021</td>\n",
       "      <td>28.388178</td>\n",
       "      <td>218.910004</td>\n",
       "      <td>12.2275</td>\n",
       "      <td>14.875555</td>\n",
       "      <td>...</td>\n",
       "      <td>16.816057</td>\n",
       "      <td>36.209999</td>\n",
       "      <td>43.901749</td>\n",
       "      <td>6.829206</td>\n",
       "      <td>18.389183</td>\n",
       "      <td>15.004833</td>\n",
       "      <td>300.855865</td>\n",
       "      <td>130.309998</td>\n",
       "      <td>6.473268</td>\n",
       "      <td>23.575750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>40.997784</td>\n",
       "      <td>10.26</td>\n",
       "      <td>101.463333</td>\n",
       "      <td>10.522551</td>\n",
       "      <td>44.007900</td>\n",
       "      <td>28.200068</td>\n",
       "      <td>28.388178</td>\n",
       "      <td>213.979996</td>\n",
       "      <td>12.1200</td>\n",
       "      <td>15.128645</td>\n",
       "      <td>...</td>\n",
       "      <td>16.246025</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>44.516521</td>\n",
       "      <td>6.707326</td>\n",
       "      <td>18.097532</td>\n",
       "      <td>14.761323</td>\n",
       "      <td>295.535522</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>6.399635</td>\n",
       "      <td>23.419973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>41.508217</td>\n",
       "      <td>10.46</td>\n",
       "      <td>102.013336</td>\n",
       "      <td>10.489408</td>\n",
       "      <td>44.144257</td>\n",
       "      <td>28.406679</td>\n",
       "      <td>28.715527</td>\n",
       "      <td>214.300003</td>\n",
       "      <td>12.2225</td>\n",
       "      <td>15.097653</td>\n",
       "      <td>...</td>\n",
       "      <td>16.466686</td>\n",
       "      <td>36.279999</td>\n",
       "      <td>44.878143</td>\n",
       "      <td>6.812954</td>\n",
       "      <td>18.426571</td>\n",
       "      <td>15.011992</td>\n",
       "      <td>293.838837</td>\n",
       "      <td>129.110001</td>\n",
       "      <td>6.489905</td>\n",
       "      <td>23.638054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  QQQ     MU        ISRG       TMUS        CME        ADP  \\\n",
       "2010-01-04  41.570904  10.85  102.923332  13.207044  44.038776  28.546648   \n",
       "2010-01-05  41.570904  11.17  102.459999  13.240186  44.002773  28.393362   \n",
       "2010-01-06  41.320183  11.22  103.946671  12.345354  43.704395  28.326714   \n",
       "2010-01-07  41.347046  10.84  103.556664  12.461351  43.758408  28.313377   \n",
       "2010-01-08  41.687347  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-09  41.687347  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-10  41.687347  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-11  41.517189  10.90  103.763336  11.914511  44.918518  28.400021   \n",
       "2010-01-12  40.997784  10.26  101.463333  10.522551  44.007900  28.200068   \n",
       "2010-01-13  41.508217  10.46  102.013336  10.489408  44.144257  28.406679   \n",
       "\n",
       "                 INTU        BKNG     FISV       MDLZ  ...       NVDA  \\\n",
       "2010-01-04  28.015366  223.960007  12.3575  14.167932  ...  16.999943   \n",
       "2010-01-05  28.015366  226.559998  12.4100  14.860062  ...  17.248186   \n",
       "2010-01-06  27.624369  219.080002  12.3575  14.963362  ...  17.358511   \n",
       "2010-01-07  27.533443  216.130005  12.3000  14.927206  ...  17.018332   \n",
       "2010-01-08  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-09  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-10  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-11  28.388178  218.910004  12.2275  14.875555  ...  16.816057   \n",
       "2010-01-12  28.388178  213.979996  12.1200  15.128645  ...  16.246025   \n",
       "2010-01-13  28.715527  214.300003  12.2225  15.097653  ...  16.466686   \n",
       "\n",
       "                 ADBE        PEP     CMCSA       CSCO       INTC       GOOGL  \\\n",
       "2010-01-04  37.090000  44.292305  6.894205  18.463966  14.954695  313.688690   \n",
       "2010-01-05  37.700001  44.827515  6.800767  18.381708  14.947535  312.307312   \n",
       "2010-01-06  37.619999  44.379101  6.752012  18.262053  14.897401  304.434448   \n",
       "2010-01-07  36.889999  44.097023  6.894205  18.344311  14.754156  297.347351   \n",
       "2010-01-08  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-09  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-10  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-11  36.209999  43.901749  6.829206  18.389183  15.004833  300.855865   \n",
       "2010-01-12  35.660000  44.516521  6.707326  18.097532  14.761323  295.535522   \n",
       "2010-01-13  36.279999  44.878143  6.812954  18.426571  15.011992  293.838837   \n",
       "\n",
       "                  AMZN      AAPL       MSFT  \n",
       "2010-01-04  133.899994  6.593423  24.105360  \n",
       "2010-01-05  134.690002  6.604822  24.113148  \n",
       "2010-01-06  132.250000  6.499765  23.965164  \n",
       "2010-01-07  130.000000  6.487749  23.715933  \n",
       "2010-01-08  133.520004  6.530882  23.879499  \n",
       "2010-01-09  133.520004  6.530882  23.879499  \n",
       "2010-01-10  133.520004  6.530882  23.879499  \n",
       "2010-01-11  130.309998  6.473268  23.575750  \n",
       "2010-01-12  127.349998  6.399635  23.419973  \n",
       "2010-01-13  129.110001  6.489905  23.638054  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_price.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also inspect the bottom padded stock market data of the QQQ and the underlying NASDAQ Top 28 constituents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>210.056091</td>\n",
       "      <td>55.060001</td>\n",
       "      <td>591.309998</td>\n",
       "      <td>77.400002</td>\n",
       "      <td>193.713379</td>\n",
       "      <td>167.027359</td>\n",
       "      <td>264.660400</td>\n",
       "      <td>2023.260010</td>\n",
       "      <td>116.669998</td>\n",
       "      <td>53.881664</td>\n",
       "      <td>...</td>\n",
       "      <td>238.975662</td>\n",
       "      <td>327.609985</td>\n",
       "      <td>134.014557</td>\n",
       "      <td>42.926853</td>\n",
       "      <td>45.504459</td>\n",
       "      <td>57.519756</td>\n",
       "      <td>1351.219971</td>\n",
       "      <td>1786.500000</td>\n",
       "      <td>69.261856</td>\n",
       "      <td>155.759949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>210.610901</td>\n",
       "      <td>55.400002</td>\n",
       "      <td>597.419983</td>\n",
       "      <td>76.959999</td>\n",
       "      <td>194.440323</td>\n",
       "      <td>165.982773</td>\n",
       "      <td>263.847900</td>\n",
       "      <td>2032.099976</td>\n",
       "      <td>116.330002</td>\n",
       "      <td>53.230022</td>\n",
       "      <td>...</td>\n",
       "      <td>238.426590</td>\n",
       "      <td>328.950012</td>\n",
       "      <td>133.159470</td>\n",
       "      <td>42.936588</td>\n",
       "      <td>46.127808</td>\n",
       "      <td>57.792961</td>\n",
       "      <td>1350.630005</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>70.392090</td>\n",
       "      <td>155.759949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>210.720291</td>\n",
       "      <td>55.419998</td>\n",
       "      <td>598.809998</td>\n",
       "      <td>77.320000</td>\n",
       "      <td>194.516861</td>\n",
       "      <td>165.875381</td>\n",
       "      <td>261.915680</td>\n",
       "      <td>2044.000000</td>\n",
       "      <td>116.629997</td>\n",
       "      <td>53.482903</td>\n",
       "      <td>...</td>\n",
       "      <td>238.226913</td>\n",
       "      <td>329.640015</td>\n",
       "      <td>133.013718</td>\n",
       "      <td>43.355247</td>\n",
       "      <td>45.820923</td>\n",
       "      <td>57.968597</td>\n",
       "      <td>1344.430054</td>\n",
       "      <td>1789.209961</td>\n",
       "      <td>70.459007</td>\n",
       "      <td>155.730255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>210.720291</td>\n",
       "      <td>55.419998</td>\n",
       "      <td>598.809998</td>\n",
       "      <td>77.320000</td>\n",
       "      <td>194.516861</td>\n",
       "      <td>165.875381</td>\n",
       "      <td>261.915680</td>\n",
       "      <td>2044.000000</td>\n",
       "      <td>116.629997</td>\n",
       "      <td>53.482903</td>\n",
       "      <td>...</td>\n",
       "      <td>238.226913</td>\n",
       "      <td>329.640015</td>\n",
       "      <td>133.013718</td>\n",
       "      <td>43.355247</td>\n",
       "      <td>45.820923</td>\n",
       "      <td>57.968597</td>\n",
       "      <td>1344.430054</td>\n",
       "      <td>1789.209961</td>\n",
       "      <td>70.459007</td>\n",
       "      <td>155.730255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>212.579697</td>\n",
       "      <td>55.110001</td>\n",
       "      <td>598.039978</td>\n",
       "      <td>77.400002</td>\n",
       "      <td>194.991379</td>\n",
       "      <td>166.304932</td>\n",
       "      <td>264.432495</td>\n",
       "      <td>2064.320068</td>\n",
       "      <td>116.029999</td>\n",
       "      <td>53.346733</td>\n",
       "      <td>...</td>\n",
       "      <td>238.795975</td>\n",
       "      <td>331.200012</td>\n",
       "      <td>132.819397</td>\n",
       "      <td>43.783646</td>\n",
       "      <td>45.888054</td>\n",
       "      <td>58.368649</td>\n",
       "      <td>1362.469971</td>\n",
       "      <td>1868.770020</td>\n",
       "      <td>71.856941</td>\n",
       "      <td>157.006729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>212.400726</td>\n",
       "      <td>54.029999</td>\n",
       "      <td>597.880005</td>\n",
       "      <td>77.239998</td>\n",
       "      <td>196.376221</td>\n",
       "      <td>166.714981</td>\n",
       "      <td>264.620789</td>\n",
       "      <td>2072.540039</td>\n",
       "      <td>117.080002</td>\n",
       "      <td>53.764946</td>\n",
       "      <td>...</td>\n",
       "      <td>236.479767</td>\n",
       "      <td>330.790009</td>\n",
       "      <td>133.645325</td>\n",
       "      <td>43.910213</td>\n",
       "      <td>45.811337</td>\n",
       "      <td>58.622341</td>\n",
       "      <td>1354.640015</td>\n",
       "      <td>1869.800049</td>\n",
       "      <td>71.829674</td>\n",
       "      <td>157.293686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>212.400726</td>\n",
       "      <td>54.029999</td>\n",
       "      <td>597.880005</td>\n",
       "      <td>77.239998</td>\n",
       "      <td>196.376221</td>\n",
       "      <td>166.714981</td>\n",
       "      <td>264.620789</td>\n",
       "      <td>2072.540039</td>\n",
       "      <td>117.080002</td>\n",
       "      <td>53.764946</td>\n",
       "      <td>...</td>\n",
       "      <td>236.479767</td>\n",
       "      <td>330.790009</td>\n",
       "      <td>133.645325</td>\n",
       "      <td>43.910213</td>\n",
       "      <td>45.811337</td>\n",
       "      <td>58.622341</td>\n",
       "      <td>1354.640015</td>\n",
       "      <td>1869.800049</td>\n",
       "      <td>71.829674</td>\n",
       "      <td>157.293686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>212.400726</td>\n",
       "      <td>54.029999</td>\n",
       "      <td>597.880005</td>\n",
       "      <td>77.239998</td>\n",
       "      <td>196.376221</td>\n",
       "      <td>166.714981</td>\n",
       "      <td>264.620789</td>\n",
       "      <td>2072.540039</td>\n",
       "      <td>117.080002</td>\n",
       "      <td>53.764946</td>\n",
       "      <td>...</td>\n",
       "      <td>236.479767</td>\n",
       "      <td>330.790009</td>\n",
       "      <td>133.645325</td>\n",
       "      <td>43.910213</td>\n",
       "      <td>45.811337</td>\n",
       "      <td>58.622341</td>\n",
       "      <td>1354.640015</td>\n",
       "      <td>1869.800049</td>\n",
       "      <td>71.829674</td>\n",
       "      <td>157.293686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>211.008652</td>\n",
       "      <td>53.209999</td>\n",
       "      <td>591.320007</td>\n",
       "      <td>78.080002</td>\n",
       "      <td>192.889893</td>\n",
       "      <td>165.562988</td>\n",
       "      <td>260.439331</td>\n",
       "      <td>2053.870117</td>\n",
       "      <td>115.470001</td>\n",
       "      <td>53.769836</td>\n",
       "      <td>...</td>\n",
       "      <td>231.937271</td>\n",
       "      <td>328.339996</td>\n",
       "      <td>132.935989</td>\n",
       "      <td>43.988098</td>\n",
       "      <td>45.638714</td>\n",
       "      <td>58.173500</td>\n",
       "      <td>1339.709961</td>\n",
       "      <td>1846.890015</td>\n",
       "      <td>72.255997</td>\n",
       "      <td>155.938049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>211.406387</td>\n",
       "      <td>53.779999</td>\n",
       "      <td>591.150024</td>\n",
       "      <td>78.419998</td>\n",
       "      <td>194.381256</td>\n",
       "      <td>166.451370</td>\n",
       "      <td>259.537659</td>\n",
       "      <td>2053.729980</td>\n",
       "      <td>115.629997</td>\n",
       "      <td>53.848053</td>\n",
       "      <td>...</td>\n",
       "      <td>234.912384</td>\n",
       "      <td>329.809998</td>\n",
       "      <td>132.799957</td>\n",
       "      <td>43.783646</td>\n",
       "      <td>45.993546</td>\n",
       "      <td>58.397919</td>\n",
       "      <td>1339.390015</td>\n",
       "      <td>1847.839966</td>\n",
       "      <td>72.783936</td>\n",
       "      <td>156.046890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   QQQ         MU        ISRG       TMUS         CME  \\\n",
       "2019-12-22  210.056091  55.060001  591.309998  77.400002  193.713379   \n",
       "2019-12-23  210.610901  55.400002  597.419983  76.959999  194.440323   \n",
       "2019-12-24  210.720291  55.419998  598.809998  77.320000  194.516861   \n",
       "2019-12-25  210.720291  55.419998  598.809998  77.320000  194.516861   \n",
       "2019-12-26  212.579697  55.110001  598.039978  77.400002  194.991379   \n",
       "2019-12-27  212.400726  54.029999  597.880005  77.239998  196.376221   \n",
       "2019-12-28  212.400726  54.029999  597.880005  77.239998  196.376221   \n",
       "2019-12-29  212.400726  54.029999  597.880005  77.239998  196.376221   \n",
       "2019-12-30  211.008652  53.209999  591.320007  78.080002  192.889893   \n",
       "2019-12-31  211.406387  53.779999  591.150024  78.419998  194.381256   \n",
       "\n",
       "                   ADP        INTU         BKNG        FISV       MDLZ  ...  \\\n",
       "2019-12-22  167.027359  264.660400  2023.260010  116.669998  53.881664  ...   \n",
       "2019-12-23  165.982773  263.847900  2032.099976  116.330002  53.230022  ...   \n",
       "2019-12-24  165.875381  261.915680  2044.000000  116.629997  53.482903  ...   \n",
       "2019-12-25  165.875381  261.915680  2044.000000  116.629997  53.482903  ...   \n",
       "2019-12-26  166.304932  264.432495  2064.320068  116.029999  53.346733  ...   \n",
       "2019-12-27  166.714981  264.620789  2072.540039  117.080002  53.764946  ...   \n",
       "2019-12-28  166.714981  264.620789  2072.540039  117.080002  53.764946  ...   \n",
       "2019-12-29  166.714981  264.620789  2072.540039  117.080002  53.764946  ...   \n",
       "2019-12-30  165.562988  260.439331  2053.870117  115.470001  53.769836  ...   \n",
       "2019-12-31  166.451370  259.537659  2053.729980  115.629997  53.848053  ...   \n",
       "\n",
       "                  NVDA        ADBE         PEP      CMCSA       CSCO  \\\n",
       "2019-12-22  238.975662  327.609985  134.014557  42.926853  45.504459   \n",
       "2019-12-23  238.426590  328.950012  133.159470  42.936588  46.127808   \n",
       "2019-12-24  238.226913  329.640015  133.013718  43.355247  45.820923   \n",
       "2019-12-25  238.226913  329.640015  133.013718  43.355247  45.820923   \n",
       "2019-12-26  238.795975  331.200012  132.819397  43.783646  45.888054   \n",
       "2019-12-27  236.479767  330.790009  133.645325  43.910213  45.811337   \n",
       "2019-12-28  236.479767  330.790009  133.645325  43.910213  45.811337   \n",
       "2019-12-29  236.479767  330.790009  133.645325  43.910213  45.811337   \n",
       "2019-12-30  231.937271  328.339996  132.935989  43.988098  45.638714   \n",
       "2019-12-31  234.912384  329.809998  132.799957  43.783646  45.993546   \n",
       "\n",
       "                 INTC        GOOGL         AMZN       AAPL        MSFT  \n",
       "2019-12-22  57.519756  1351.219971  1786.500000  69.261856  155.759949  \n",
       "2019-12-23  57.792961  1350.630005  1793.000000  70.392090  155.759949  \n",
       "2019-12-24  57.968597  1344.430054  1789.209961  70.459007  155.730255  \n",
       "2019-12-25  57.968597  1344.430054  1789.209961  70.459007  155.730255  \n",
       "2019-12-26  58.368649  1362.469971  1868.770020  71.856941  157.006729  \n",
       "2019-12-27  58.622341  1354.640015  1869.800049  71.829674  157.293686  \n",
       "2019-12-28  58.622341  1354.640015  1869.800049  71.829674  157.293686  \n",
       "2019-12-29  58.622341  1354.640015  1869.800049  71.829674  157.293686  \n",
       "2019-12-30  58.173500  1339.709961  1846.890015  72.255997  155.938049  \n",
       "2019-12-31  58.397919  1339.390015  1847.839966  72.783936  156.046890  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_price.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the number of records obtained after the data padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3649, 29)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_price.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Daily Log-Return Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the daily log-returns of the QQQ's daily adjusted closing prices using the `log` function of the `NumPy` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep copy of stock price data\n",
    "stock_data_return = stock_data_price.copy(deep=True)\n",
    "\n",
    "# compute the log-returns of the QQQ's daily adjusted closing prices\n",
    "stock_data_return['QQQ'] = np.log(stock_data_return['QQQ']) - np.log(stock_data_return['QQQ'].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the daily returns of the adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.85</td>\n",
       "      <td>102.923332</td>\n",
       "      <td>13.207044</td>\n",
       "      <td>44.038776</td>\n",
       "      <td>28.546648</td>\n",
       "      <td>28.015366</td>\n",
       "      <td>223.960007</td>\n",
       "      <td>12.3575</td>\n",
       "      <td>14.167932</td>\n",
       "      <td>...</td>\n",
       "      <td>16.999943</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>44.292305</td>\n",
       "      <td>6.894205</td>\n",
       "      <td>18.463966</td>\n",
       "      <td>14.954695</td>\n",
       "      <td>313.688690</td>\n",
       "      <td>133.899994</td>\n",
       "      <td>6.593423</td>\n",
       "      <td>24.105360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.17</td>\n",
       "      <td>102.459999</td>\n",
       "      <td>13.240186</td>\n",
       "      <td>44.002773</td>\n",
       "      <td>28.393362</td>\n",
       "      <td>28.015366</td>\n",
       "      <td>226.559998</td>\n",
       "      <td>12.4100</td>\n",
       "      <td>14.860062</td>\n",
       "      <td>...</td>\n",
       "      <td>17.248186</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>44.827515</td>\n",
       "      <td>6.800767</td>\n",
       "      <td>18.381708</td>\n",
       "      <td>14.947535</td>\n",
       "      <td>312.307312</td>\n",
       "      <td>134.690002</td>\n",
       "      <td>6.604822</td>\n",
       "      <td>24.113148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>-0.006049</td>\n",
       "      <td>11.22</td>\n",
       "      <td>103.946671</td>\n",
       "      <td>12.345354</td>\n",
       "      <td>43.704395</td>\n",
       "      <td>28.326714</td>\n",
       "      <td>27.624369</td>\n",
       "      <td>219.080002</td>\n",
       "      <td>12.3575</td>\n",
       "      <td>14.963362</td>\n",
       "      <td>...</td>\n",
       "      <td>17.358511</td>\n",
       "      <td>37.619999</td>\n",
       "      <td>44.379101</td>\n",
       "      <td>6.752012</td>\n",
       "      <td>18.262053</td>\n",
       "      <td>14.897401</td>\n",
       "      <td>304.434448</td>\n",
       "      <td>132.250000</td>\n",
       "      <td>6.499765</td>\n",
       "      <td>23.965164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.000650</td>\n",
       "      <td>10.84</td>\n",
       "      <td>103.556664</td>\n",
       "      <td>12.461351</td>\n",
       "      <td>43.758408</td>\n",
       "      <td>28.313377</td>\n",
       "      <td>27.533443</td>\n",
       "      <td>216.130005</td>\n",
       "      <td>12.3000</td>\n",
       "      <td>14.927206</td>\n",
       "      <td>...</td>\n",
       "      <td>17.018332</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>44.097023</td>\n",
       "      <td>6.894205</td>\n",
       "      <td>18.344311</td>\n",
       "      <td>14.754156</td>\n",
       "      <td>297.347351</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>6.487749</td>\n",
       "      <td>23.715933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.008197</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.10</td>\n",
       "      <td>102.986664</td>\n",
       "      <td>11.765371</td>\n",
       "      <td>44.927525</td>\n",
       "      <td>28.273378</td>\n",
       "      <td>28.260878</td>\n",
       "      <td>216.210007</td>\n",
       "      <td>12.1625</td>\n",
       "      <td>14.942701</td>\n",
       "      <td>...</td>\n",
       "      <td>17.055109</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>43.952370</td>\n",
       "      <td>6.873890</td>\n",
       "      <td>18.441526</td>\n",
       "      <td>14.918890</td>\n",
       "      <td>301.311310</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>6.530882</td>\n",
       "      <td>23.879499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>-0.004090</td>\n",
       "      <td>10.90</td>\n",
       "      <td>103.763336</td>\n",
       "      <td>11.914511</td>\n",
       "      <td>44.918518</td>\n",
       "      <td>28.400021</td>\n",
       "      <td>28.388178</td>\n",
       "      <td>218.910004</td>\n",
       "      <td>12.2275</td>\n",
       "      <td>14.875555</td>\n",
       "      <td>...</td>\n",
       "      <td>16.816057</td>\n",
       "      <td>36.209999</td>\n",
       "      <td>43.901749</td>\n",
       "      <td>6.829206</td>\n",
       "      <td>18.389183</td>\n",
       "      <td>15.004833</td>\n",
       "      <td>300.855865</td>\n",
       "      <td>130.309998</td>\n",
       "      <td>6.473268</td>\n",
       "      <td>23.575750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>-0.012590</td>\n",
       "      <td>10.26</td>\n",
       "      <td>101.463333</td>\n",
       "      <td>10.522551</td>\n",
       "      <td>44.007900</td>\n",
       "      <td>28.200068</td>\n",
       "      <td>28.388178</td>\n",
       "      <td>213.979996</td>\n",
       "      <td>12.1200</td>\n",
       "      <td>15.128645</td>\n",
       "      <td>...</td>\n",
       "      <td>16.246025</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>44.516521</td>\n",
       "      <td>6.707326</td>\n",
       "      <td>18.097532</td>\n",
       "      <td>14.761323</td>\n",
       "      <td>295.535522</td>\n",
       "      <td>127.349998</td>\n",
       "      <td>6.399635</td>\n",
       "      <td>23.419973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.012373</td>\n",
       "      <td>10.46</td>\n",
       "      <td>102.013336</td>\n",
       "      <td>10.489408</td>\n",
       "      <td>44.144257</td>\n",
       "      <td>28.406679</td>\n",
       "      <td>28.715527</td>\n",
       "      <td>214.300003</td>\n",
       "      <td>12.2225</td>\n",
       "      <td>15.097653</td>\n",
       "      <td>...</td>\n",
       "      <td>16.466686</td>\n",
       "      <td>36.279999</td>\n",
       "      <td>44.878143</td>\n",
       "      <td>6.812954</td>\n",
       "      <td>18.426571</td>\n",
       "      <td>15.011992</td>\n",
       "      <td>293.838837</td>\n",
       "      <td>129.110001</td>\n",
       "      <td>6.489905</td>\n",
       "      <td>23.638054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 QQQ     MU        ISRG       TMUS        CME        ADP  \\\n",
       "2010-01-04       NaN  10.85  102.923332  13.207044  44.038776  28.546648   \n",
       "2010-01-05  0.000000  11.17  102.459999  13.240186  44.002773  28.393362   \n",
       "2010-01-06 -0.006049  11.22  103.946671  12.345354  43.704395  28.326714   \n",
       "2010-01-07  0.000650  10.84  103.556664  12.461351  43.758408  28.313377   \n",
       "2010-01-08  0.008197  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-09  0.000000  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-10  0.000000  11.10  102.986664  11.765371  44.927525  28.273378   \n",
       "2010-01-11 -0.004090  10.90  103.763336  11.914511  44.918518  28.400021   \n",
       "2010-01-12 -0.012590  10.26  101.463333  10.522551  44.007900  28.200068   \n",
       "2010-01-13  0.012373  10.46  102.013336  10.489408  44.144257  28.406679   \n",
       "\n",
       "                 INTU        BKNG     FISV       MDLZ  ...       NVDA  \\\n",
       "2010-01-04  28.015366  223.960007  12.3575  14.167932  ...  16.999943   \n",
       "2010-01-05  28.015366  226.559998  12.4100  14.860062  ...  17.248186   \n",
       "2010-01-06  27.624369  219.080002  12.3575  14.963362  ...  17.358511   \n",
       "2010-01-07  27.533443  216.130005  12.3000  14.927206  ...  17.018332   \n",
       "2010-01-08  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-09  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-10  28.260878  216.210007  12.1625  14.942701  ...  17.055109   \n",
       "2010-01-11  28.388178  218.910004  12.2275  14.875555  ...  16.816057   \n",
       "2010-01-12  28.388178  213.979996  12.1200  15.128645  ...  16.246025   \n",
       "2010-01-13  28.715527  214.300003  12.2225  15.097653  ...  16.466686   \n",
       "\n",
       "                 ADBE        PEP     CMCSA       CSCO       INTC       GOOGL  \\\n",
       "2010-01-04  37.090000  44.292305  6.894205  18.463966  14.954695  313.688690   \n",
       "2010-01-05  37.700001  44.827515  6.800767  18.381708  14.947535  312.307312   \n",
       "2010-01-06  37.619999  44.379101  6.752012  18.262053  14.897401  304.434448   \n",
       "2010-01-07  36.889999  44.097023  6.894205  18.344311  14.754156  297.347351   \n",
       "2010-01-08  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-09  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-10  36.689999  43.952370  6.873890  18.441526  14.918890  301.311310   \n",
       "2010-01-11  36.209999  43.901749  6.829206  18.389183  15.004833  300.855865   \n",
       "2010-01-12  35.660000  44.516521  6.707326  18.097532  14.761323  295.535522   \n",
       "2010-01-13  36.279999  44.878143  6.812954  18.426571  15.011992  293.838837   \n",
       "\n",
       "                  AMZN      AAPL       MSFT  \n",
       "2010-01-04  133.899994  6.593423  24.105360  \n",
       "2010-01-05  134.690002  6.604822  24.113148  \n",
       "2010-01-06  132.250000  6.499765  23.965164  \n",
       "2010-01-07  130.000000  6.487749  23.715933  \n",
       "2010-01-08  133.520004  6.530882  23.879499  \n",
       "2010-01-09  133.520004  6.530882  23.879499  \n",
       "2010-01-10  133.520004  6.530882  23.879499  \n",
       "2010-01-11  130.309998  6.473268  23.575750  \n",
       "2010-01-12  127.349998  6.399635  23.419973  \n",
       "2010-01-13  129.110001  6.489905  23.638054  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_return.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the obtained daily adjusted returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot daily market returns\n",
    "ax.plot(stock_data_return.index, stock_data_return['QQQ'])\n",
    "\n",
    "# rotate x-tick labels\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels and limits\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2010'), pd.to_datetime('31-12-2019')])\n",
    "ax.set_ylabel('[daily QQQ returns]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('Invesco QQQ Trust Series 1 - Daily Historical Adjusted Log-Returns', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the daily log-returns of the remaining NASDAQ Top 30 daily adjusted closing prices using the `log` function of the `NumPy` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over distinct ticker symbols\n",
    "for symbol in symbols:\n",
    "    \n",
    "    # compute the log-return of the ticker symbols daily adjusted closing prices\n",
    "    stock_data_return[symbol] = np.log(stock_data_return[symbol]) - np.log(stock_data_return[symbol].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the daily returns of the adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029067</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>-0.005384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011542</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.047696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.016313</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>-0.013646</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>-0.006049</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.014406</td>\n",
       "      <td>-0.069977</td>\n",
       "      <td>-0.006804</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.014055</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.010053</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>-0.025532</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>-0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.000650</td>\n",
       "      <td>-0.034455</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>-0.013557</td>\n",
       "      <td>-0.004664</td>\n",
       "      <td>-0.002419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019792</td>\n",
       "      <td>-0.019595</td>\n",
       "      <td>-0.006376</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.023555</td>\n",
       "      <td>-0.017160</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.010454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-0.057471</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.026077</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>-0.005436</td>\n",
       "      <td>-0.003286</td>\n",
       "      <td>-0.002951</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.011103</td>\n",
       "      <td>0.013243</td>\n",
       "      <td>0.026717</td>\n",
       "      <td>0.006626</td>\n",
       "      <td>0.006873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>-0.004090</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.012596</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014116</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>-0.001513</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>-0.012802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>-0.012590</td>\n",
       "      <td>-0.060510</td>\n",
       "      <td>-0.022415</td>\n",
       "      <td>-0.124236</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.007065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022778</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034486</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>-0.018008</td>\n",
       "      <td>-0.015987</td>\n",
       "      <td>-0.016362</td>\n",
       "      <td>-0.017842</td>\n",
       "      <td>-0.022977</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>-0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>-0.005758</td>\n",
       "      <td>0.013726</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.009269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 QQQ        MU      ISRG      TMUS       CME       ADP  \\\n",
       "2010-01-04       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2010-01-05  0.000000  0.029067 -0.004512  0.002506 -0.000818 -0.005384   \n",
       "2010-01-06 -0.006049  0.004466  0.014406 -0.069977 -0.006804 -0.002350   \n",
       "2010-01-07  0.000650 -0.034455 -0.003759  0.009352  0.001235 -0.000471   \n",
       "2010-01-08  0.008197  0.023702 -0.005519 -0.057471  0.026367 -0.001414   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-11 -0.004090 -0.018182  0.007513  0.012596 -0.000200  0.004469   \n",
       "2010-01-12 -0.012590 -0.060510 -0.022415 -0.124236 -0.020481 -0.007065   \n",
       "2010-01-13  0.012373  0.019306  0.005406 -0.003155  0.003094  0.007300   \n",
       "\n",
       "                INTU      BKNG      FISV      MDLZ  ...      NVDA      ADBE  \\\n",
       "2010-01-04       NaN       NaN       NaN       NaN  ...       NaN       NaN   \n",
       "2010-01-05  0.000000  0.011542  0.004239  0.047696  ...  0.014497  0.016313   \n",
       "2010-01-06 -0.014055 -0.033573 -0.004239  0.006927  ...  0.006376 -0.002124   \n",
       "2010-01-07 -0.003297 -0.013557 -0.004664 -0.002419  ... -0.019792 -0.019595   \n",
       "2010-01-08  0.026077  0.000370 -0.011242  0.001038  ...  0.002159 -0.005436   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2010-01-11  0.004494  0.012411  0.005330 -0.004504  ... -0.014116 -0.013169   \n",
       "2010-01-12  0.000000 -0.022778 -0.008831  0.016871  ... -0.034486 -0.015306   \n",
       "2010-01-13  0.011465  0.001494  0.008422 -0.002051  ...  0.013491  0.017237   \n",
       "\n",
       "                 PEP     CMCSA      CSCO      INTC     GOOGL      AMZN  \\\n",
       "2010-01-04       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2010-01-05  0.012011 -0.013646 -0.004465 -0.000479 -0.004413  0.005883   \n",
       "2010-01-06 -0.010053 -0.007195 -0.006531 -0.003360 -0.025532 -0.018282   \n",
       "2010-01-07 -0.006376  0.020841  0.004494 -0.009662 -0.023555 -0.017160   \n",
       "2010-01-08 -0.003286 -0.002951  0.005286  0.011103  0.013243  0.026717   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-11 -0.001152 -0.006522 -0.002842  0.005744 -0.001513 -0.024335   \n",
       "2010-01-12  0.013906 -0.018008 -0.015987 -0.016362 -0.017842 -0.022977   \n",
       "2010-01-13  0.008091  0.015625  0.018018  0.016839 -0.005758  0.013726   \n",
       "\n",
       "                AAPL      MSFT  \n",
       "2010-01-04       NaN       NaN  \n",
       "2010-01-05  0.001727  0.000323  \n",
       "2010-01-06 -0.016034 -0.006156  \n",
       "2010-01-07 -0.001850 -0.010454  \n",
       "2010-01-08  0.006626  0.006873  \n",
       "2010-01-09  0.000000  0.000000  \n",
       "2010-01-10  0.000000  0.000000  \n",
       "2010-01-11 -0.008861 -0.012802  \n",
       "2010-01-12 -0.011440 -0.006629  \n",
       "2010-01-13  0.014007  0.009269  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_return.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the first row corresponding to a `nan` daily return of the stock data dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_return = stock_data_return.iloc[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the daily returns of the adjusted closing prices for the removed row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029067</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>-0.005384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011542</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.047696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.016313</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>-0.013646</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>-0.006049</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.014406</td>\n",
       "      <td>-0.069977</td>\n",
       "      <td>-0.006804</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.014055</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.010053</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>-0.025532</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>-0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.000650</td>\n",
       "      <td>-0.034455</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>-0.013557</td>\n",
       "      <td>-0.004664</td>\n",
       "      <td>-0.002419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019792</td>\n",
       "      <td>-0.019595</td>\n",
       "      <td>-0.006376</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.023555</td>\n",
       "      <td>-0.017160</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.010454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-0.057471</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.026077</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>-0.005436</td>\n",
       "      <td>-0.003286</td>\n",
       "      <td>-0.002951</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.011103</td>\n",
       "      <td>0.013243</td>\n",
       "      <td>0.026717</td>\n",
       "      <td>0.006626</td>\n",
       "      <td>0.006873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>-0.004090</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.012596</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014116</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>-0.001513</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>-0.012802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>-0.012590</td>\n",
       "      <td>-0.060510</td>\n",
       "      <td>-0.022415</td>\n",
       "      <td>-0.124236</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.007065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022778</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034486</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>-0.018008</td>\n",
       "      <td>-0.015987</td>\n",
       "      <td>-0.016362</td>\n",
       "      <td>-0.017842</td>\n",
       "      <td>-0.022977</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>-0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>-0.005758</td>\n",
       "      <td>0.013726</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.009269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>-0.015924</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>-0.002819</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>-0.026144</td>\n",
       "      <td>-0.002868</td>\n",
       "      <td>-0.003770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.010529</td>\n",
       "      <td>0.012015</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.012503</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>-0.013726</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>0.019899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 QQQ        MU      ISRG      TMUS       CME       ADP  \\\n",
       "2010-01-05  0.000000  0.029067 -0.004512  0.002506 -0.000818 -0.005384   \n",
       "2010-01-06 -0.006049  0.004466  0.014406 -0.069977 -0.006804 -0.002350   \n",
       "2010-01-07  0.000650 -0.034455 -0.003759  0.009352  0.001235 -0.000471   \n",
       "2010-01-08  0.008197  0.023702 -0.005519 -0.057471  0.026367 -0.001414   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-11 -0.004090 -0.018182  0.007513  0.012596 -0.000200  0.004469   \n",
       "2010-01-12 -0.012590 -0.060510 -0.022415 -0.124236 -0.020481 -0.007065   \n",
       "2010-01-13  0.012373  0.019306  0.005406 -0.003155  0.003094  0.007300   \n",
       "2010-01-14  0.000863  0.025485  0.006384 -0.015924 -0.004321 -0.002819   \n",
       "\n",
       "                INTU      BKNG      FISV      MDLZ  ...      NVDA      ADBE  \\\n",
       "2010-01-05  0.000000  0.011542  0.004239  0.047696  ...  0.014497  0.016313   \n",
       "2010-01-06 -0.014055 -0.033573 -0.004239  0.006927  ...  0.006376 -0.002124   \n",
       "2010-01-07 -0.003297 -0.013557 -0.004664 -0.002419  ... -0.019792 -0.019595   \n",
       "2010-01-08  0.026077  0.000370 -0.011242  0.001038  ...  0.002159 -0.005436   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2010-01-11  0.004494  0.012411  0.005330 -0.004504  ... -0.014116 -0.013169   \n",
       "2010-01-12  0.000000 -0.022778 -0.008831  0.016871  ... -0.034486 -0.015306   \n",
       "2010-01-13  0.011465  0.001494  0.008422 -0.002051  ...  0.013491  0.017237   \n",
       "2010-01-14  0.006313 -0.026144 -0.002868 -0.003770  ... -0.015757 -0.010529   \n",
       "\n",
       "                 PEP     CMCSA      CSCO      INTC     GOOGL      AMZN  \\\n",
       "2010-01-05  0.012011 -0.013646 -0.004465 -0.000479 -0.004413  0.005883   \n",
       "2010-01-06 -0.010053 -0.007195 -0.006531 -0.003360 -0.025532 -0.018282   \n",
       "2010-01-07 -0.006376  0.020841  0.004494 -0.009662 -0.023555 -0.017160   \n",
       "2010-01-08 -0.003286 -0.002951  0.005286  0.011103  0.013243  0.026717   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-11 -0.001152 -0.006522 -0.002842  0.005744 -0.001513 -0.024335   \n",
       "2010-01-12  0.013906 -0.018008 -0.015987 -0.016362 -0.017842 -0.022977   \n",
       "2010-01-13  0.008091  0.015625  0.018018  0.016839 -0.005758  0.013726   \n",
       "2010-01-14  0.012015 -0.005981  0.012503  0.024507  0.004690 -0.013726   \n",
       "\n",
       "                AAPL      MSFT  \n",
       "2010-01-05  0.001727  0.000323  \n",
       "2010-01-06 -0.016034 -0.006156  \n",
       "2010-01-07 -0.001850 -0.010454  \n",
       "2010-01-08  0.006626  0.006873  \n",
       "2010-01-09  0.000000  0.000000  \n",
       "2010-01-10  0.000000  0.000000  \n",
       "2010-01-11 -0.008861 -0.012802  \n",
       "2010-01-12 -0.011440 -0.006629  \n",
       "2010-01-13  0.014007  0.009269  \n",
       "2010-01-14 -0.005808  0.019899  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_return.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace potential missing values `nan` or infinity values (division by zero) `inf` with 0.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_return = stock_data_return.replace([np.nan, np.inf, -np.inf], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the daily returns of the adjusted closing prices for the replaced values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QQQ</th>\n",
       "      <th>MU</th>\n",
       "      <th>ISRG</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>CME</th>\n",
       "      <th>ADP</th>\n",
       "      <th>INTU</th>\n",
       "      <th>BKNG</th>\n",
       "      <th>FISV</th>\n",
       "      <th>MDLZ</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>PEP</th>\n",
       "      <th>CMCSA</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>INTC</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029067</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>-0.005384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011542</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.047696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.016313</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>-0.013646</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>-0.006049</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.014406</td>\n",
       "      <td>-0.069977</td>\n",
       "      <td>-0.006804</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.014055</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.010053</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>-0.025532</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>-0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.000650</td>\n",
       "      <td>-0.034455</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>-0.013557</td>\n",
       "      <td>-0.004664</td>\n",
       "      <td>-0.002419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019792</td>\n",
       "      <td>-0.019595</td>\n",
       "      <td>-0.006376</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.023555</td>\n",
       "      <td>-0.017160</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.010454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-0.057471</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.026077</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>-0.005436</td>\n",
       "      <td>-0.003286</td>\n",
       "      <td>-0.002951</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.011103</td>\n",
       "      <td>0.013243</td>\n",
       "      <td>0.026717</td>\n",
       "      <td>0.006626</td>\n",
       "      <td>0.006873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>-0.004090</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.012596</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014116</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>-0.001513</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>-0.012802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>-0.012590</td>\n",
       "      <td>-0.060510</td>\n",
       "      <td>-0.022415</td>\n",
       "      <td>-0.124236</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.007065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022778</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034486</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>-0.018008</td>\n",
       "      <td>-0.015987</td>\n",
       "      <td>-0.016362</td>\n",
       "      <td>-0.017842</td>\n",
       "      <td>-0.022977</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>-0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>-0.005758</td>\n",
       "      <td>0.013726</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.009269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>-0.015924</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>-0.002819</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>-0.026144</td>\n",
       "      <td>-0.002868</td>\n",
       "      <td>-0.003770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.010529</td>\n",
       "      <td>0.012015</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.012503</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>-0.013726</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>0.019899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 QQQ        MU      ISRG      TMUS       CME       ADP  \\\n",
       "2010-01-05  0.000000  0.029067 -0.004512  0.002506 -0.000818 -0.005384   \n",
       "2010-01-06 -0.006049  0.004466  0.014406 -0.069977 -0.006804 -0.002350   \n",
       "2010-01-07  0.000650 -0.034455 -0.003759  0.009352  0.001235 -0.000471   \n",
       "2010-01-08  0.008197  0.023702 -0.005519 -0.057471  0.026367 -0.001414   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-11 -0.004090 -0.018182  0.007513  0.012596 -0.000200  0.004469   \n",
       "2010-01-12 -0.012590 -0.060510 -0.022415 -0.124236 -0.020481 -0.007065   \n",
       "2010-01-13  0.012373  0.019306  0.005406 -0.003155  0.003094  0.007300   \n",
       "2010-01-14  0.000863  0.025485  0.006384 -0.015924 -0.004321 -0.002819   \n",
       "\n",
       "                INTU      BKNG      FISV      MDLZ  ...      NVDA      ADBE  \\\n",
       "2010-01-05  0.000000  0.011542  0.004239  0.047696  ...  0.014497  0.016313   \n",
       "2010-01-06 -0.014055 -0.033573 -0.004239  0.006927  ...  0.006376 -0.002124   \n",
       "2010-01-07 -0.003297 -0.013557 -0.004664 -0.002419  ... -0.019792 -0.019595   \n",
       "2010-01-08  0.026077  0.000370 -0.011242  0.001038  ...  0.002159 -0.005436   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2010-01-11  0.004494  0.012411  0.005330 -0.004504  ... -0.014116 -0.013169   \n",
       "2010-01-12  0.000000 -0.022778 -0.008831  0.016871  ... -0.034486 -0.015306   \n",
       "2010-01-13  0.011465  0.001494  0.008422 -0.002051  ...  0.013491  0.017237   \n",
       "2010-01-14  0.006313 -0.026144 -0.002868 -0.003770  ... -0.015757 -0.010529   \n",
       "\n",
       "                 PEP     CMCSA      CSCO      INTC     GOOGL      AMZN  \\\n",
       "2010-01-05  0.012011 -0.013646 -0.004465 -0.000479 -0.004413  0.005883   \n",
       "2010-01-06 -0.010053 -0.007195 -0.006531 -0.003360 -0.025532 -0.018282   \n",
       "2010-01-07 -0.006376  0.020841  0.004494 -0.009662 -0.023555 -0.017160   \n",
       "2010-01-08 -0.003286 -0.002951  0.005286  0.011103  0.013243  0.026717   \n",
       "2010-01-09  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2010-01-11 -0.001152 -0.006522 -0.002842  0.005744 -0.001513 -0.024335   \n",
       "2010-01-12  0.013906 -0.018008 -0.015987 -0.016362 -0.017842 -0.022977   \n",
       "2010-01-13  0.008091  0.015625  0.018018  0.016839 -0.005758  0.013726   \n",
       "2010-01-14  0.012015 -0.005981  0.012503  0.024507  0.004690 -0.013726   \n",
       "\n",
       "                AAPL      MSFT  \n",
       "2010-01-05  0.001727  0.000323  \n",
       "2010-01-06 -0.016034 -0.006156  \n",
       "2010-01-07 -0.001850 -0.010454  \n",
       "2010-01-08  0.006626  0.006873  \n",
       "2010-01-09  0.000000  0.000000  \n",
       "2010-01-10  0.000000  0.000000  \n",
       "2010-01-11 -0.008861 -0.012802  \n",
       "2010-01-12 -0.011440 -0.006629  \n",
       "2010-01-13  0.014007  0.009269  \n",
       "2010-01-14 -0.005808  0.019899  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_return.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the obtained and validated daily return stock market data to the local data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the filename of the data to be saved\n",
    "filename = 'qqq_nasdaq_30_daily_returns.csv'\n",
    "\n",
    "# save retrieved data to local data directory\n",
    "stock_data_return.to_csv(os.path.join(data_directory, filename), sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Train-Validation Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice, to divide the dataset into a **training set** or **\"in-sample\"** data (the fraction of data records solely used for training purposes) and a **evaluation set** or **\"out-of-sample\"** data (the fraction of data records solely used for evaluation purposes). Pls. note, the **evaluation set** will never be shown to the model as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px\" src=\"traintestsplit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the split fraction of training sequences to **90%** of the total number of obtained sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.9\n",
    "split_row = int(stock_data_return.shape[0] * split_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split obtained returns into training (\"in-sample\") returns $r^{i}_{train}$ and validation (\"out-of-sample\") returns $r^{i}_{valid}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stock_data_return = stock_data_return.iloc[:split_row]\n",
    "valid_stock_data_return = stock_data_return.iloc[split_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the obtained train and validation stock returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAFVCAYAAAC+WIHiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOydeZwdVZn3f7eTkBBIIEAQRETG5aAOMh91FAUUHBccdXSc0VFneR1FcEEHQWUTBTVsISSQBMgGIUASEgIhO1lIZ+ktSSeddHe6T7o7vaQ7ve/b3ev9o6rurVu3llN1697bffv5+sH0re08derUqfOc5znP45MkCQRBEARBEARBEETuk5dtAQiCIAiCIAiCIIjMQAogQRAEQRAEQRDEBIEUQIIgCIIgCIIgiAkCKYAEQRAEQRAEQRATBFIACYIgCIIgCIIgJgikABIEQRAEQRAEQUwQJmdbAIIgJg6MsQ8AWMs5vz7bsmhhjF0KYB6ADwMIATgD4G7OeZuy/6sAHgAwSfnvDQDzOecSY2wKgIcA3ApgRDn/j5zzEl0Ze5RzrwHQAaAHwC7O+RyXMl8LYBbnfL9u+9cB/A6AD8B0AAs5568JXvM+AO9yzg+5kUm5xnQAuwD8lHNeLXjOjwH8BcBpyBOTEoBHOOfvWpyzAMDTAH4CoI1z/oJAGddwzu/TbFsL4AUA0wC8n3O+1OTcfwVQwjk/K3AvlwH4E+f8l3bHas65GcDPOec/MNhXBqCAc/4rk3OLAfwAwM0AejjnmxyUexGAWznnqwWPLwbwA855g2bbSgCfhNyefQAuBjCPc/6SxXVuB/AS5zwkKqsTlHdjofLzegCHAEQBzOWcb7U5V/j5McZugdxOv6DZdimAQgAf4ZxHdcf/GPL7v8CoDMbY4wCqOecrTcp7P4DrOOeb1fbPOW+yk9PkWnkAngJwLYCpAIYB/Ipzftrk+CTZGWNfANDHOT/hRgaCILIHKYAEQUxoGGM+AJsBzFEHz4yxLwPYwhj7LIDrADwO4Buc81bG2GQAz0NWsuYq+yIAruecRxljVwHYyhj7Fue8Xi2Hc/5PyrVXQlaCd6Qo+r8BaAOwX7d9CYBPcM77GGMzABxnjO3inHfYXZBz/ngqAjHGPg1ZoXqfi9NXq8oZY+w9APYzxr6oKuF6OOd3Kce6FVd7Lbtn8X8Afg7AVgFU5BVW/qxgjN0AoBzAlxhjMzjngxblrnRRxCcA/AsAIQXQgj+odagolZWMsZWcc7NEww8AWAV5ssRzOOflkBViMMYaAHyVc+4XPNfJ88sHcBlj7GrNu/7fAFbplb8UytDyJchK2Ga1/afArQDeyzn/CgAwxr4DYD6Ab1udpJP9JwDWAiAFkCDGGaQAEgSRFRhj+QDKAPw9gJkAvgd58DGLc/4IY2wqgOOQB6l3APgRZMvQWs75s4yx7wK4F/Ig8ixkK8jFAF4GcCFka8T/AOgE8KpSxmTI1jmtZelzADq0lhPO+W7GWC2ALwD4IWTlsFXZF2aM3QPgqDIL/30AV6sDPs55I2NsEYAfA/izQD1UADgFIAigGoo1izF2DYAXOOc3M8bmALhFkX+Dcj8/BhBkjB3VWez6APwfY+wNACcBfJRzHmCMXQBghVJHAPAbznk5Y6xRKfckgFmQB3R7ICtyH4Zskfsj5zxfLwfn/And7UwF8K8AXrG7bys45+2MsQ0AvskYWwdgOeRn+l4Aiznnzyvt5+fqOYyxRwG0cM4XM8ZmAdjNOf+USHka68bDANYBuACy9fRBAFMA/AOAVYyxGwH8GnJbCwPYzzm/lzH2MIDPAzgfwE8hW7euZ4x9E3Ib8AE4qsj7XQC/Uq4rQa4vM34G2dp8BsD/A7BIkXcO5AH8GQCXKNsehjwhUA2NNZEx1sY5v8zkfXkQwHWKRW47gKUAzgUwCuB2zvkZo7JsuAyAX7GOX6m/JoCvKsesVd4fI1lXQm6nF0OeZLkd8vvxd5Df/zlG92OldCnX/wDkyZ5uANsAlEB+PnmQn92PlHLWKs/vBIB9kPsgCcC3Oef96vWUe1wBWen7i7L5fwD8M2PsTsjP+jwAXdA8Z60nBGPs3wD8EXI/dQ6AasbYJMgTOVcCuBzAJkXO+wBMZ4wVArgbcntqg0H/Zie7Ut6nGWP/Afl9f1upEzDGTgI4AODjkC27P9TLDrkN3wrgk4yxk24tkQRBZAdaA0gQRDY5xDn/MmSXwR9CVhy+r1jl/gXAFgAfAvAfAG4EcBOA7zDZ7PNDyC5dNyrHzYQ8kNrEOf88gHsAfEbZtktx0/oegBXK9VWugux6qKcBwAeM9nPOByAP7C6F7HYXNjlXhPMB/NXI/U/Df0IenN4E2eWqBcBKyC5genfNr0JWXtYAaAVwv3K/DwDYwzm/BfKA+nnl+CsB/Ihz/lvNNW4D0KXU2bcBLDaSQy8k57yAc35G6K7taYescHwI8mD5q8q93W1y/HLIg28oMhq5vf6IMZav/gfZoqLlg0qZ34LcviYrLoNlyrUZZIX/88p/H1aUPACoUtrdKAAoluJFkC3HnwZQC9ky+hFl242Qle6vGd0MY2wm5Da/FcBLAH6hbP805ImJf1RkmmFSH3qM3pc5kF1+l0J2B3yWc36z8vfjDsp6kjF2gDHWBNkt93vK9qRrcs5XQFZarNo7FLk+D6AX8jv4b5DdOf9gcT8iXAbZIvgkZAXnvxT53tTIrTITwBrO+RcBtAD4usH1VkLun8AY+wzkd78VsvL6Zc75ZyErZv+oP1FxH38awJcht4MRZdeVAIo551+D3If9nHMegextsFrn5mvWv1nKzjk/DHmC4TsAKgEcgTwZBsj9x2tK3VZDnoCD7vxSADsgW39J+SOIcQYpgARBZJNjyr9nAEzjnPcq226EbOFaDtlCeBXkWeo9kAdWH4asCHyJMbYP8mA8CnmAXgQAnPNCLq99+ygUN0lFcRqArLipNEK2LOj5COSBU9J+xZoWhbyW72JlsG90rijcYJtWSf1PyIO/dyBbwgxRLF9Xcc7v5Zx/AsCnIM/SfxPyWp+fKIrPMgAXKad1cc67dZe6FrIVIx+yxXEyY+wSUTks5NuiKF8L7Y/GVQCaISuC32GMvQp5sDvF6GBl7dIgY+xjipyrDA5bzTm/Wf0PwLu6a1RCtrysAfAckr+R10AemIcU90bVSgIkP8NLAPSqrrec8yeVgXIHgJcZYy9Bts4Y3o9yD3mQlZuFAC5njP0T5LZ1hHMeVSYiyk3OV1HbkdH7ouVaAA8oz/xPAN7joKw/cM5vgmyRugJAncU1RWQFEuuznHMe5pwPQ1GwBe7HjHrOeVD5uwXAs4rF8RYYP4uEPooxdqdmEuEKznk7ZKvd5yC7RC5VLJFBAGsUC+H7TK49G/IEUrfSngqV7T0A/pEx9hpkt8ypFvdj1b8lyK49iTH2CfkU/kPISvH9ANYpymOIx9cWF0LuVwmCyCFIASQIIpsYrRFaBuAuAOdyOYgIhzxDfYsyaF8Jec3J7QAeVma4fZBdrKqgzLQzxr7AGHtC2XaTsu0KyG6OWoWnCMB7GGP/ohyzkjE2F7LlaS9kheBBxtjlTGYzgBchuyKGILsMzmGM5THG7mKMPQvZTdA0CIYB6uDVD9nlC5ADa0Bxhf0eZIvHLQB+rKwzjCK5D58K4HVlDR0gWyLaAAQgz+TPV+rw+5DdxrRla6mGbD24GbLlYD2AQRM5hOGcf1NRvn5tdRxj7HLIlsdtkC25RZzz/1Lk8FmcugxyQJ5mznmXE9mUcq8FMINz/g3ILpeqoqrWdTWAzzLGJisD5S9Adt9Vj9HSAeBCZU0cGGPPMsa+COARyNav2yArM2b3cxuAb3HOb+Wc3wq5Tf0KstXwM0p7Ow/Ax3TnxdqQ8nxURd/ofdG2oWoA9yrP/A7IdW1XVgKc820ANkJ2+zS7JjTlmsmqHqNi1E8Y3Y8I2usuA/C/nPMfQ3YjNXoWCWVzzhdpJhHUSZ5lkC2k1wPYrihX3+Gc/wfk55Zncm21jcxWfqtWwh9DtvT/J+TgVNOV9mb0zlv1b2ZrMAHZ6vgXxlieonxWAhhW/p7CGLtOOe4GZZ8RRvIQBDEOoBeXIIgxBed8H2Sr30rl93HIlr+DjLEjkK1/LZAj+21hcnTNyyBbSh4F8G3F4vAIZOXtUciWgv2QB6e3a102lQHPNwF8jzFWBNmicy1kxekaxdXpXshWIXU2/wLIlr9pkF3SApAVye9BVtzOIm4ZcsLriFvePqnIF4BsESiGrJDuBNAEoBTAnUyORKjeSxuA3yj1UqScc5RzvhOyu9/3lWvvAFBhIccSANco1pVCAI0WcniF6p65B7L75v9yznsgr9n6lSLLXQDCilJsxFuQB7YrXMpQA+Bmpa2sh2y1AuQ6WAW53a0DUAC5/TVAblNJKFagX0IOCHQQsgKwXzm3CLL1cBTyusYEGGOfBOBTLJIqGyBbxrshr9c7DHktlj64zxEAfYyxEsjvgBqcxOh9qQNwLWPsLshBjf6s1PMqACc452U2ZRnxVwAfY4x9w+iayjEHICv3pSayimB0P055FcABxlgBZPfWpGchyE4AXwHwlvLcawEMK9fdBXkiJunaSj90J4B3GGO7Ia8BBOT+7lalHT4PuV2+F7IF9tuMMa37rGX/ZsGzAIYAlCntcw3ktYwq9yrbr4DcHxhRAtlV+KMC5REEMYbwSZLVBBFBEMTEhMnh4M/jnNeZ7P8c5NQASRY0RTH8GOf8aJrFJDQwOQXFPgCftQsIkmswOVhLHef8xWzLQoxvmBw59RouGDmVIIjxB0UBJQiCMICbpB/Q7C+y2OeHHPWRyBCMsc9DtlQ8MgGVv/+F7DIr6gZJEARBTGDIAkgQBEEQBEEQBDFBoDWABEEQBEEQBEEQEwRSAAmCIAiCIAiCICYIpAASBEEQBEEQBEFMEHIuCEw4HJF6e0eyLUbOMGvWdFB9egfVp7dQfXoH1aW3UH16C9Wnt1B9egfVpbdQfXrH7NkzTPPm5pwFcPLkSdkWIaeg+vQWqk9vofr0DqpLb6H69BaqT2+h+vQOqktvofrMDDmnABIEQRAEQRAEQRDGkAJIEARBEARBEAQxQSAFkCAIgiAIgiAIYoJACiBBEARBEARBEMQEgRRAgiAIgiAIgiCICQIpgARBEARBEARBEBMEUgAJgiAIgiAIgiAmCKQAEgRBEARBEARBTBAmZ6ogxlgegOcAXAcgAOA2znmtZv/PANwBIAzgb5zzLYyx8wA8D+BqAOcA+DXn/FCmZCYIgiAIgiAIgsglMmkB/A6AaZzzzwG4D8A8dQdj7DIAvwFwA4CvAXiMMTYVwO8BVHDObwLwMwAsg/ISBEEQBEEQBJFGAsEIXt5RjZbOoWyLMmHIpAJ4I4AdAMA5Lwbwac2+zwAo4JwHOOf9AGoBfAKyMhhkjL0D4CEA72RQXoIgCIIgCIIg0sjeYy3YV3YWc9ccy7YoE4aMuYACmAmgX/M7whibzDkPG+wbBHABgEsAzOKcf40x9j8AngLwP3YFzZ49wzupCapPj6H69BaqT++guvQWqk9vofr0FqpP76C6TI28yZMAAAMjIQBUn5kgkwrgAADtE81TlD+jfTMA9AHoBrBJ2bYZsuuoLZ2dg6lJSsSYPXsG1aeHUH16C9Wnd1BdegvVp7dQfXoL1ad3UF2mzshIIOE31ac3WCnSmXQBLQDwzwDAGLseQLlm3yEANzHGpjHGLgDwUQAVAA6q5wD4AoDKzIlLEARBEARBEASRW2TSAvgWgK8wxgoB+AD8L2PsbgC1nPNNjLFnARyArJQ+yDn3M8YeBbCcMVYEIAQB90+CIAiCIAiCIAjCmIwpgJzzKICf6zZXa/YvA7BMd04PgO+mXzqCIAiCIAiCIIjchxLBEwRBEARBEARBTBBIASQIgiAIgiAIgpggkAJIEARBEARBEAQxQSAFkCAIgiAIgiAIYoJACiBBEARBEARBEMQEgRRAgiAIgiAIgiCICQIpgARBEARBEARBEBMEUgAJgiAIgiAIgiAmCKQAEgRBEARBEARBTBBIASQIgiAIgiAIgpggkAJIEARBEARBEAQxQSAFkCAIgiAIgiAIYoJACiBBEARBEARBEMQEgRRAD9h95Ay2FzdmWwyCIAiCIAiCIAhLJmdbgFxg9e4aAMDXr78qy5IQBEEQBEEQBEGYQxZAgiAIgiAIgiCICQIpgARBEARB2NLeO4LVu04hEIxkWxSCIAgiBcgFlCAIgiAIWxasO4723lHMmD4F37rh6myLQxAEQbiELIAEQRAEQdjSNxQEAIwEwlmWhCAIgkgFUgAJgiAIgrDHJ/8jSdkVgyAIgkgNUgCJnCcQjGBzYQP6h4PZFoUgCIIgCIIgsgopgETOs6WoAW/tP41lmyuzLQpBEARBEARBZBVSAImcp3cwAADo6B3NsiQEQRDjF1+2BSAIgiA8gRRAgiAIgiCEoTWABEEQ4xtSAIkJAw1aCIIg3ONTg8CAOlOCIIjxDCmABEEQBEEIENMACYIgiHEMKYDEhMFHC1gIgiBcQ10oQRBEbkAKIDFhIBdQgiCI1KGulCAIYnxDCiAxYege8GdbBIIgiHELeVEQBEHkBqQAEgRBEAQhDpkACYIgxjWkABIEQRAEIQxFASUIIl2Eo5FsizAhIAWQyHnIa4kgCCJ1fOQDShBEGlD7Ft85o/jR+juxsXZbliXKfSZnqiDGWB6A5wBcByAA4DbOea1m/88A3AEgDOBvnPMtmn1fBPAq5/zKTMlL5A40V00QBOEd1KcSBJEO8mb2AAB2NeXjOx/65yxLk9tk0gL4HQDTOOefA3AfgHnqDsbYZQB+A+AGAF8D8BhjbKqy70oAdwOYkkFZiRwmEo1mWwSCIAiCIAiCyAqZVABvBLADADjnxQA+rdn3GQAFnPMA57wfQC2ATzDGpgF4AcAvMygnkWNonZZGA2H87Ml8vLitKmvyEARBjGvIBEgQRDqgviVjZFIBnAmgX/M7whibbLJvEMAFABYBeIpz3pIZEYlcp61nBABw8ERrliUhCIIYX6hLAGmMRhAEMb7J2BpAAAMAZmh+53HOwyb7ZgAIArgJwIcYY38GcBFjbC3n/Ad2Bc2ePcPukLSQrXLTzXi/r6nT4t7Ds2ZNj/1N7SQ3oPr0DqpLb8m1+pyUJ88ZT5s2JSv3lmv1mW2oPr2D6jI1zpt+TtI2qtP0kkkFsADAtwCsY4xdD6Bcs+8QgDmKy+dUAB8FcIhzztQDGGNtIsofAHR2DnontQOyVW46mT17xri/r4A/FPu7t3ck9nc27isX6nMsQfXpHVSX3pKL9RmVZNuffzSY8XvLxfrMJlSf3kF1mTrDI8GkbVSnqWOlRGdSAXwLwFcYY4WQl2X9L2PsbgC1nPNNjLFnARyA7Jb6IOfcn0HZCIIgCIIgCIIgcp6MKYCc8yiAn+s2V2v2LwOwzOL8y9IkGkEQBEEQgtAaQIIgiPENJYJPA/WtA2jvGbE/kCAIgiDGCWpEZYk0QIIgiHENKYBp4K8vH8H9S4uzLQZhgM9nfwxBEARhQKz/JA2QINLBwHAQ0oSeYaFBWqYgBZAgPKK1exgL1h9HzwAtXyUIIvegoRlBpI8TdV24a+FBbCpoyLYoxASAFECC8IjlW6pwoq4br79bm21RCIIg0saENlAQRJooq+kCAOw92pxlSYiJACmABOERoXAUABCORLMsCUEQhPf4yIeeINIPvWdEBiAFkCAIgiAIYcgASBAEMb4hBZDIfXzaP53NrA2NhhCNig13mjuHHF2bIAhiXEIaIEF4Ti68VpIkoXcwkG0xCAFIASRyH5e96tBoCL955gCeWnvMW3kIgiDGIeSZRhDpZzy/Zm8dOI17FhfgRF1XtkUhbCAFkCBM6OofBQBUN/VlWRKCIIixg5QTtgqCGFvkQnClvUdbAADldT1ZloSwgxRAIvfJ8HRaLnTiBEEQesazZYIgiMxBk0RjH1IACcIEp+sFCYIgJgQ0tiMIghjXkAJIEARBEIQA42dSLBKNoqa5D5EopeUhxgvKzMr4ec0c09YzgkNV7eYH0ORSxiAFMEvsKW3GTx9/F/1DFC2JGDuEwhGM+MPZFkOIHSVNePdIU7bFIIgJgxoEZjyM0bYWNuKxV49iWzH1EcT4Yjzrf3a5Qh9YWowX3q5Ez4A/QxIRZpACmCVe23UKEoDjdd3ZFoUgYvzmmYO4c8H+bIshxLq9tZi/hiK0EkSmGQ/rnCsb5CAU1Y29WZaEIMQYD++VE6oaevDQ8hLDtBD+YCQLEhFaSAEkCCJGIESdMkEQdoyfkaqUa6NqghgnzF9/Ai1dw9h95Ey2RSEMIAVwnCGalJzIHjTgIAgiFxlPeQDHkagEkTMYjn/oZRyTkAI4jlj8Zjlue3IvLWpPgUwPYE6d6aOEqARBEARBCGG3ji63mcj3nllIARxHlJ7qBEC+0+OJx187igXrT2RbDIIgiJRRU+OMBR+HgeFgtkUgcoSWrmEUn2zLthhj4r1yy+BI0KAOze9oQuu4Y4TJ2RYg1+kbCmBwJIQrLz3fs2uSh+HYhh4PQRA5iTpoy3InV8o7sPitCnzv5g/i69dfZXwQjTAJQR5aXgIAuOb9s3Dh+VOzJ8g4HjzMX3ccDW2DCdukWFYLehfHImQBTDN3LyrAn1885Om6MFpj5g0TrR4DoQilHSEIImWKT7Zj9a5TeOUdLhQ4qqVzCKGwd54rpVz2htl3/Kz5QROsfydSJ0hB0FyjV/60jNe5mNbhdmw5vRNRKTeXXZECmCHoUzT2uH1uvqOgOpIkYV9Zi2FI4/HA7xYX4LeLCiac4ksQhDdox3G7S5ux91gL9h5tsTynsW0QD604hGc3lKdXOIIgskJZbXKcg1wYZTx2aAG2N+xGeVdVtkVJC6QAeogkSaaDa08tgJ5daWJg5n4QiUoIhc1ndvSzVmU1XXh5B8djr5Z6KV7GGFYSvFP7IQjCK/zBsOX+Mx1DAIDK+p5MiBNnvJodiAnPeGu6r7zDU77GWLzliCRbhP3h3ExaTwqgx5gNrn/2ZD6G/aGMykLISJqnkooe3qe4T3b1j+3OoLljCIeq2s0PIA2QIAg3uBiZRsnjgJjARKMSKut7hNxLpXH6cTbrFnLl1R+vz8UOUgA9xK6J1Jzpz0xBhCn6FzkXX+w/vXgIL7xdiRG/8cx8Lt4zQXhFa/cwfvdcAXhTb7ZFGXO4maUnBZCYyBw4cRbzXi8Ts5LFgqbkFuPNoqknV3swUgC9xmVLoY9k+tC6gBpVcziS7AY6Gghj6eaT6RQr7RjdF5A7s3IEkQ42FzagZyCAl7ZVZ1uUMYfRQM4uZ5nkYJ11OqD+jsgmanCU8ky7QI9xzPoNSTLvT1qGWhGKWrucp4Uc7URIAfQSmzZiZnkZ8Ydx2xN7sWZ3jRfFEBboFe2lm07i9rn5SetY9h5rwdmu4cSTBaexcrSvIIgJQez9Heez1mOFbOl/9PiIscBEbodeehvV9Nbh0UPzsbJytWfXFCVXh3SkAHqMmwbf3Ckvkt915IxgIcZlhMJRRKK5Ga7WM3RVp0av6tat6/MHx384aLOWSAoqQZijBuyayAM3L3ESaZkgssnYaak50PskVGbq99M0KEcbLuusSPlazhk7LcNLSAH0EAmS48G1m+igZmfc8VQ+7llU4Ph6EwnR6s7tVAm5fG8E4Q12ro169pW1oKa5L03SjF+yvbyBejtiTCDwHozXtmraU3p4Q077Y8IeUgA9RP9+P72uTOg8p+3aqh8ZGKFIo1aIDkZSGbOMmSArpilJnF+qvWcE89cdR2ffaIpCWROVJPz+uUK8tutUWsshCDNUg5WTfjkYiigpYo6mR6gxjF01ifa5kWgUXf3e9S8TZbz47BsnMOeVI9kWgzDDRUPMhbZ7hHfERkJe3I5ZOq9MMGbGdB5DCmAaqTitW/TrURvKbetU6nhhVc32rHU6Ue+spXPIPFKo7v5f2l6N8tPdWJ1mxSwQjKB7wI89pc1pLYcgTHHx7ocjudtfaLGbhR8YCWLn4TMIheMu9KIuoEs2ncQfni9CoxI0gxCjrLYLdS0D2RaDsGFi9BBxBg2MEdpxlVN1Ls+XRXUlRx8eKYBjAKem7VzSTUpOtuOt/ac9u96IP4SfPrEX6/fWGu4XVQ5zWsmWgMGRIB5acQh/XF5sdkgCobC8tjSSy/VCEIi3fSf9ci5PGGmxq5Glmyqxdk8Ndh6Or2cXXQJ4pLoDAFDf5l6ZaWofxIL1x9HeO+L6GgThJU5Gd7najahdaV2L+1RoZAH0HlIAPcbNC+y0WeeScrJkUyU2FzZ4dr3GdjmgzvaSJsP9ZlWnfwYpVfEYeTymQWAgxVyF+4aCYtdSKiTPZlA87/UyvOmhQm8nD0F4jgsX0MhECXRiVCeabc2dcuRkbVCtTL6rCzeU40RdNw6dbI9vzFD5p88OYGiUlmAQxrhphiUn21FQ3uq9MFkiFU+JbLrFupV6NOxHTW/dmB2vTM62ALmE/IzT/6DHZlNKDUmSHM24D4wEcaqpD59isx2dJ/oiGrktjTe3fLNbFaoCCQk3rJ5jVweV9T2orO/Bd7/wdwKFuCcX3wFibKBa85y87xGTnJuEiyigNocHghHTQ1QFbNjEtT1d9A4G8LdVRzBz+hQs+M1NGS2byCUSW/aSTZUA5Pb1uY9fhosvmJYNoQRwPjpy+g3PpgXQzYjj1ar1KGo9DAD45XU/wccvvsZroVImYwogYywPwHMArgMQAHAb57xWs/9nAO4AEAbwN875FsbY+wG8qMjpA3A755xnSuaM4TgIjHVj3FzYgG1FjZj/6xsw7RzrR7yjpAmhSBTf+vwHnAnhMVFJwiQHitzcNcfQ0jmM337/Olz7dxeLl2OyXV+jRlWsneV3qrCOOWzakKTTAGOh8cfIPTd3DGVbhKxTUd+NDfmncfd/XIcZ08/Jtjg5iHhbnygWQDdvf1Rw8kiEngE/fvdcYez38GgILZ1DuGL2+QnHZbqbGhyRPSkoCBvhBfr2++b+0yioaMMnP3wJrv/4Zbjy0vONTxzjaG/LqVVMdOzhD/txzqRzsrtmEIgpfwBwdisSdy0AACAASURBVKhtTCqAmayh7wCYxjn/HID7AMxTdzDGLgPwGwA3APgagMcYY1MB/BXAIs75zQAeBfBYBuV1gfM0EIDzmQ1tEVFJQlvPSMLL9Nb+0wiEImhqtx8kr9tb6+kaPLc4TV/YoroaDfhtjtQh+HyiBgdqI1P+9Im9pu4+2RoKRiUJvYMB2+MkyV5GfTuOr4uyOidzd/7i1qqMlTVWefr142hsH8T+42ezLUpOEbN2kwuoAWKVoq2NmPt4nvNz9egDxAz7w3hoxaGEoDNOrucVkyfRahrCBF2zDwQjKKxotWyzRrT3jGB7SRMefvGQh8JlBi8mjn0C6kowEsI9+/+E+UefT7k8LakObcbKxLmeTPZaNwLYAQCc82IAn9bs+wyAAs55gHPeD6AWwCcA3ANgq3LMZAAOR/uZxU0jkZBaGogN++rwwNJiHFYW0IvCm3rx0IoSZwULEo1KCIacdW6ZUh6EF/MKHFbb7H5Bczp4cWsV7lkczwNpXqcuoqRmKIeRcJ5GD8rKJNGohIa2AUqKPY4gF9BkjL5VPoO/te9xzKU2jWOgUFguI13BGiRJwtsH61HfahygZtKksTnAI8Yea9+twfItVdhU0ODq/In6BbGLPwAAwyHZMHC6v9Hj0nOz1k39Axljj9qdzDl/wEFZMwFoR8wRxthkznnYYN8ggAs4512KLAzAU5CtiLbMnj3DgVjeccnsGZYD5Rkzz02Sbcb503DhhdNjv0Vkv/DC6bHjSk7Kil9da3Lo7AsuSC5P5f+ePZAQpteqXKf1+au576KpbRCb530bgDz4HQmEcf65U0zPueji83HuVOceyTNnTEuQr1UTfEDdfq6m3JkzzzUu/6LzEq4zXedSZ1QHF15oXL/nTJmUtF37O13ts7CiLeH3rIvOw+xZ05OOu/ji84HJ8bo2kueSS87HlMmTYr8nKTPc06ZNMZVfOwh2e49aq6rVNSZpZty9rs/O3lHsLGnEv//ThzF1yiT7EwRYv+cUVm2rwo+/8TH825c+7Mk1Vc6dfo5ndZCtvjPbhMIRvFPciJv+4QpMVp75lHOS32MzBoPGbT/X6nPy5OQ54/POmxq7T1UROvfceD8x/Vy5L/X5fEL1cf75U02Pm3mBcf996aUzcO7UybGZ9unTp+Kcc+ScglMM+mOnnGrqxdsH6/H2wfrYd01LdFK8n8jWM3dTbq61z1S56KLzMPsSd+6VZnUZb//yMc1dspLS0e/HJZecj8LyVnz86otx4YypmDpVHqtMmpTn6Zgs3dhNgJynfKPa+uMeSrNm6cZc501NOq/H14HKjlP47se+jplD8Xff7P7zRsK2x7jBNzWa0vW0feRYwmrE/a8AHrfYfy8AJwrgAABtDeQpyp/RvhkA+gCAMXYL5LWD/y26/q+zMzt5hNrbByxnKQYGRpNkGxzyo68vrqB0dAxgNBDB9Gnmj6anZxjTlO9wVPGd9PuT3RH7+kZM60Kvp5odN3v2DMN9kiQhHJEwxWBA0KS46ajnLVh/HCfqurHgNzdipslapc7OQVcK4NBQIEG+Pk2icnX7qEap6OszDg/e2zOM6ZpOLKCrT6M6ONs+gMrJPlyqU7KCoUjS8Z2dsqvenqPNeOC/PuWZYmFFd/cQfAZuJp1dQ+gbinfERvfW2TmYoACqaSBqmnqxu6ge133okqRzwhoF0O07OKypd6trhDX35fX7/shLh9HYPohQMIRvfO4Dnlyz6ITspllS0YovXHuZJ9dUGda9A25R3/Wm9kHsOnwG//VVhqnnpL+djgW2FjVgw77TOFjWgkmKq2I4HBWu167uuLu9es55M6bhj88X4Juf/4CjdcpjmXA42dI5PBxvf6or7OhoMLZtZFReHydJie9qz4Af975QhP+5leGmT7w3tn1o0G9Y77Nnz8CASaJ49fshqeWPBBEMysOLsEF/7JQOzflG1+rTLEXI1vjDablm3/aJTE/3MKa48Eayqku/Mv6IRiW0dwwgHJLfoWAgjH1HmjBvbRkuv3g65vzs+tg4LhqRLJ/NK1sqcdN1l+O8aeaT6pkkYhPdc2RE7g+0469uzTgWAEaGk5evPLTnKQAAO49hyGbMAgC9/uR+2AvWlm/CTbNvdH2+to/MNFaKp5UL6B855y+b/QfgIYdyFAD4ZwBgjF0PoFyz7xCAmxhj0xhjFwD4KIAKRfl7BsCtnPMjDsvLOLs0uY+coNUZl205iTsX7E8Io63HyMqYaRfjZZtP4o6n8hMG7GacqOsGAHT0Gn+8gczl0TLzwNNv1q9XqTub7O65fEsV7ltikEfP5F5Wbq/GmfYh1DT3iYiaRFVDD55944Tw2gHLKrWp7qQ1gMqGrn4/nnnjhFD5bhgLLqBqDjGjRLZuSae8Xl/7idXHUFDRhr3HWgAAJ+q6YjnacpUupb9t7hgSjnirxWgAtOfwGdQ092P+uuMAgL3HWnCyoSdVUbOKXZ04qbMj1R2IRCW8tK06YbsnGXh8BttSwG4NzxiN8k44JJ2Pcdgfxp3z96OxPa4IqOO81u7Eiekui/EfIMdueG3nKctjMont+NNov4OXJhyNCMXKGKtr7fS0j3QiEnW2TCodmCqAnPMNAMAY+zJj7FbG2D8zxuoYYz/S7nfAWwD8jLFCAPMB/JYxdjdj7F84520AngVwAMC7AB7knPsBLABwDoCXGWP5jLElju8wzWiVseqmXss1COY56OKNtrhSzl/U1G4+WzAWvjXFSp6lth7xhLtW62TSujYqIZ2BWDn6fmTOqlIPBXLH3LVlKKvtwhHeKXS86QpAF+v5RKrNi0GQ8FrQNDaXsfB+OcJjgUcDsuVEXce7YP0JPLexAm09IxMiz5nahzsLApPct2kntSRJwivvcDy1tixl+cYcBhWVaYVI32/4kNnBoFdrD1u7h3Goqt3+QCItpDsWgT+YOOg3Ky8qSejoM58wB5yNvcYKPqGJGYP+BOM86rqG6p4a/KV4Ltbyt7ItilAaiDkAfgRgMeQonesArHZaEOc8CuDnus3Vmv3LACzTnXOd03IyRSAUwZHqDnyKzU5PAZbRFuV/h0ZDwom804XVrIw+VcITq4/hxfu+ZHKs2/IFkEz+tkBkwbFrUvzGSJKEs13DOF7bhVs/+37zjtGkUiUREUwsgHZypYroFdL6mR5nGmC6rOf66z6wtBg+H7DiXuN3WM+uw2cQjkTx9euvSod46SMWBVS8DzCav9JaBcd6k6pt7se+shb8v69fYx3R0qZK1DorP90d3+g0DaDLYGpW7Dx8Bh9+3wW4+vKZzi+eQR5cJgdm+8iVF+LC85PXRBFjj6HRENZvOI5brrscl5isUXVDW/ewZ9fSEwpHDZfvpIuG1kFsK27EB98bf/+2FjXitm9+FJPyxOTIbh5A7zjVWwcAKG47gv/86L9nVRaRmh8B0A4grFjqxvq3LCO8kV+HFVursCE/MYWC04+XmyatDrRf31PjvtwMvEtORHI9iHV4H6KGRtGQ5UCy4pPuF0SSgIdWlGB9fh1ONvZaHme8w15Z089oC1kA7Q/J0EVSFSH9UQvHMuptG70rTl7TNXtqsD6/Do+/WorNBfWeyJYutPcVz3np8gIKWqvgWI/++uirpSioaMPRU9beBXaDMLXujCYm0zqnJiX+q6WjdxRr99Tgry+7X0ViOyHo8eMNOIyinUuEsxhR180w5O2D9dhe2BBL2p6E1WS+ze908Ob+07jjqXy0plHB1FNW24U38utwtiteZsnJdhRViFu7RSbkxqqSqJUrLMkeNpN82V9fL6IADkBO37COMfYrALm9GESQM4qL5pnO1BNSO/0wNihBVvqG3Vv/9EW6td54lRfO7lAzBVHohU84RNQF1IkCKHyoAwnEyhz1h82PSaH85DyAmdEARdtMWl11YmvAxubHREWrqHmpYKiTH17V8anmfrx1wFgB7B0MYOOB02NqwBt18fyNairRBTRFoTJEyCDISwI2aSD0t1nKO3GmI/VvpB36thoMxe8jaLFmev/xs2KudJnV/1yRyTys6WLnoSbcPjcfDW3G6TbSjZsaHFHiIPR77I2Vrse5pbABAFBRn/n1yIFQYv9iuKTA5L7zBPrjdKWB8RJ17d/kvPGhAH4fwO2c81UA9gH4z/SK5B3tPSO4e9FBVI7hhfdumuvK7dX2B1mwtagBA7oAF+nobJxc02oAW1nfg9ue2Iuy2q6kfW/sq0tYL2nXRYjK5GjyX/cU+4eDWLb5JLpsfPjTjemAwMXDFrMAZs4FNJ0YBZIYkyjybSlswG1P7nWce9P0ssp1MzGefG5jOTYVNOCdQ02uzq9t7sfASOoDL8P5HgfP31AB1LqAZmFw3t4zgodfPJSUPN0KN2ImrOvRnB+ORLH4rfKUB5qRaNRWMdWLvedoM6osvCM27KvDKzs5Vm6vxgNLDQJ5OcTrp+vOM8hjIbLA+nzZPe5Itdg6d89xUYnqZLF5jAer8vTFxzdoI3WLEo1KWVlHLzpf7tYLQL+cyEtOdFaiqie9AXXerN2CXr8c/C8syd/p8WIBvBTANxljfwLwXQD3pVck79he0oS+oSCWbz7p+bXVdyfZkub8Wple3Lph3+mkbUYWtt7BAH73zH7UtpgnPPdKdCsX0B0lclLPrUUNSfsGhoN4+KXD1hfXXNq0nBQ6Q/0lWzqHUVTZhhVbq9xf1ANSuVX1XDXiqFlUUJHynGB3jYHhoNBxXsgw1vU/vVuamwGDEWp/lInIvO098iRJvwtvht7BAB59tRQPejCA16K2bUcrZIzWAKqupDBur8dqOrF0U2Xa6nnd3lo0dQw56ofsJnHsJ9ji57u1Suv7lj88X4Q7nsq3OUf+V9R9cGtRI/YebRGWyUlfUH66G4UVrQ7OSEYNBueETEXSzgRuxhXhSBSBYGqTYG5qUO2H9e9O72AgFlHaDS/vEMp6lsCvnzmAv1i4Om86OLZc8R2stBHyyND2HRtrt2HB0Rdsz1lS/jIWlS0XF8Qljx6aj6HQMPr88nh6cp7ztGdeI/KNWw85UXu75r9xQvJITpIkLFh/HDt1KRtWbq/G7iPiaRy87GpdDzQ97PCNPtZbihrAm3rxrMuw/07Es5q1iivb5jVVZ6GkajGbSe4fDqJ3MD6AdvIBMhN92MI9MxOY1akkiT2bVTuqccdT+9A7GBBaE+j1+GP93tqE3/uPn8VdCw+ioLxV6P0b8YcM6yAYitgoS+NEA9Th1URSbECTyQGli6LUyQCv3zP1tk8192Pxm+WCUXOTj1H71Lw8n+HgfOGGchSfbEdDq7v8UIeq2vHu0WaBI63lr9J4yARDqa2/8mQSCHI0TDX1jrZftir4bZPBrSfN2IFryfx1x7F8S2qTfxvH2EB9PPDbhQfxi6f3pXYRN23FxGPinsUFuH9JcUa/I6OBsKXFP9vtKuldFPxmSUj+vnUYpFLQ9sO7mvJR05ds7MgWI+FR3HvgEVR0y33DZAMLoCRJ8Ie9mcgVQUQBHOSc/5FzvkT9L+1SeYTRTP5oIIITdd1YqwmgsvPwGew/fhard9dAGNOOwqoHMRmQG2zL9PqjiMvZWms5xa9pVbzIB9wyN51GRLPZ8Hmvl+GexQX2BRlgPkA0dgFLdUCSGQ8PCfllcvLyhrYBwTWBqc/4a//eXpLoFqjmpTsskJOudzCAOxccwHMbK5L23bekCHcvKrC1Fnj6DqZBl9J/EL2SNpMuoKngpeNEQhAYzfbSU51iCqZBXWkVQMsJLpcV/cLblXhVlwvsbNcwfrvoIKobew0nBPqHAnhq7bGEQWJVUzwv6Wu7TsXSgBjhpM7t3q/OPvNcZw8uK3GUeicqwVQB9AK7vqB7IHODNjNyYQ2gyog/jD2lzY7c2r2YCHJnAZT/dWOBzeYTGwutxW0XXt1Tg0eK52INfzNhezrfgb5AP16qXI3uUXPXcidMMrAAvlK1Dvfsfwh9ATGDRqqIKIAVjLEfMJmPMMY+knapPCJmOdJ8tYw+YFpl0Cn8TGJSbzfNz7DR2rwZhRWtqG5yl1BcWAYDgqEIeJP5C1ByMm4g9soCGMOmTqJR75YAO5IdJi6RJn+boR14eRIJzcwFVMiikfgjKdKpkQXQ5FrtPSMJ9/PC2xWY84rspjI0GsJPn9iLTUqUSCvR/Er9TDtnku0DUgNPlBrkTFQjFJpZg/XrmETJtAtWujzHM+kCGi80c0VZYuCvOWKhEFmhTqrl+XzWE1yurm7MlsIG9A8FsXJHfJ24ev1QOIpNBQ042dCLRW+aT5iNWA6krR+U1pPEbkJ1j5D1MnVSeU/U99/qGrypF3PXHHNfiEdUNfaifRzmhtOivnp7j7XgtV2nsKWoMcPlO38b82zWANoU6OIk5VSHxxv15y2dQ3gjv84wj2lGEH43E2Wv628AABS1HrY4yhn+cAB/KZ6LolZjF9r1pzbhSHsZ1gikQBdpR5N8yepXSZs86dUy1GZ7vheIKID/AOAOAC8AWKL8Oz5I4/gl29GGlm+pcm21M0L0Uiu3V+OJ1cYfu4a2gYRQyF4FgRFZkzM0GsLdiw6ixYOorE6RJKetQT66tXs4poS8uf80fjV/P+pbB7DpYD1un5uPli7jMM3a0qzKTWUArz31uY0VGBjWBw0yUHg1m55ZfxxdfaNo7hzC/UuLE9yID1V1oK5FjvJW2yzPdG1UokRa1aSaRPfcqZNt25bIoM/umO0ljbh9bn5iTjMT+ocCuO2JvbZrLLxU2pIupdmQX9aCvS4H2HlpsgA2dw55OkObrrXTEZ2M971QZHuO0V2JWgBT/ZRsLqg3nKiI1Y4EDI4EccdT+TEretiiv02lWrXv73GDoF2CF3F+iqWF1Z0YlfU9uH1uPooqrAdj3MOJ2FRYsP4E7vd4PWy26UhhDV3GsHGZN7MeD4wEk9NApHFYmTThKUl4+KXD2FbciEdfOYqn1h5zHw3e5QyecXqVVPp1Y/n7A4O291bdW4P2kU68WrXOcL8/7Ff+DaA/MIC1/C0MBI3dbRsG3AU1UzFyD00HIgrgNs75LZr/xLIAjwHs8nn1DJi7oAhc3Hizw/fn7QP1pgN9QJ6NzYRrh+iC/aM1iRYVbd3qwyA7UYssx0dxUy4AoL7VOET0wEjImRuvlTxOjrX3AE26YHPHEB5cVoKFG2TFSA3NXH66O+anX15nr3i4IRCK2Id71xCJSknKpPEaQM3gr64br+w8hWZFITeLBJjURkzq8rVdp2KBQqZOmZRysAor1NtQ/80/Zh8sQo04uPFgfeZcsXz6n/ENq3ZwvLLTXWQzpxZAf1DMQvanFYdw4IRJcAwXVZYO9a9/KBibnHCCUVWpE3ST8nzW/ZvJzVc39qJCYPLhrQPyhFF+WUvilTQVpE/F4DP5G7B+7nbKofYz4vY1sDzNpPxUXzmjb3B+mfzebytutLzvsRx8Zby5hbqZWO/qH7XNXylcvqb4jt4RofQ0bl3m61sHHX2Hk8p1eLxRWWofVd86gJMNvRgNjJ10PHpE2rLRu3i6vxEPFPwV62veTtjuD/tNl58Ylh/TJ3x4/dRGHGgpwrpTbxseG46mVo81fafRMNCEqJRey6yIAvh1xlj245U6pO5sPwrK5Zm7hDxFmmf8u+cKXV/fK2u/mjLAiO5+P+5csB/Lt5xMuSPvHQxY+tMbfsQcFpnKbJbVRzSm/0G2IqSS0NcON/UsSTCsq0T9T9vRxAdkFfU9mPd6WWyfl4Nas1t5cFkJHn01vr6msKI1FnAhfq5NZyjQXNzkdjMrdU9p3JplN6AGIFSRwmspJdlSeef8/QkpRxKO0fz9f88eFLtwiqTLAuZkQFPT3IdfPr3fMEKvEZW6iQD9Lew6ciYhKIklY8VtFIBRy1Un1Xw+m/7NZNeTa47h6XXHhSVYtYPHXPD1ufn0VWXVdpx2gQnXkgz/TJLJa5xNNiYf+9DykqSlDephLV3DKCzPjEuW1/x2UQF+8vi7uH9pseXazjGDi2HOfS8UY9Gb5Z6KMTAcxH1LioXGGnZBs6y66UzmP9WP/4ykTXdAev31fVYzUTq08p41dZFMvqtTvXIwuX3N8fF++3AH7tn/pwQFzq4PUZ+vD8BQUJ4wGgoae5zlGbh3OmF7w27MPbIIzx1/MaXr2CEi5WwAZxljxYyxIsaYe60pg5gtIE/FdfPYqU7sP37W9flO8CGe8L2osh1/XF5iebzVXQVDEdyzuAAPrTC/hpUFULhTSEFHFZlF9fniYePTRdyt1sGAQvlf0naLe9K67yYMikUqW1RxETxw+ZaqpPfF7kwJEupa+hPdvHQnCc2M6w2Ahq6lum0CVdTUbu8KbCaeUb2t2VODkUAYr79ba3BGooyGyW1dIkkSgqEIGtoG8MpOnuDql5SCxkDuR1467NhC4SQKqDrzvqWwEVFJwsINJ3DghLs+MhSOYs3uGsxdW2Z/sMekOvCxqyqhSJYe4/aWrJ670TX7BgMx63ziuZmzPjlbb268XR89UXsvu0vN3amtyg5Hommxwq3ZXWO4vlmPGim3vWdEyJXdS7r7/XjitaOmk2aiSJKEw9Udpvk+9f3biD/kOh2E2oeq7+tZC+8sFbXvcLMiJ7kPTx8hgfXsmTYYi/ZRw6ERbDn9Tux3Waexwi/6rqnRQfe3xNUZycbaFv+++my/F8aurc6xy0/YF+hHTa/7SKciCuA3AXwGwH8A+AGAH7ouLWvEH0YqDXzhm+WxJOymg0dP17jE/27ttvGFtyhWDWJgFXVN1AXUytdbJFCI+bmiO9PbQzV3DuEXT+9DWY34x1KSzFwiTf6GmHLkZLLi9Nlkt7WUmqLNuZIEzHmlNCH6qv75u8kDZnSG0/sY8YfwhpJQ2LosUw1QV76Eiy+YBsDcbTxdH84Xt1bh5/P24S8rj2Dv0ZaEQZ/+G2MkQ2P7oOlgaMO+Ovz+ucKkAACxAY3J89MGmlD7A0mS0Nk7imM1XXhpW7Urtzin52TCWi58vtFGRcDBkZClJWHXYfv0Q3Vn+7HxwGmXHgpSUmOxGp8k9lsSdh05g9ZuZSBscN7u0mb8dqFs9bZzAe3u99svvbC4xwITS5yTWjFrZ3lOkpIJlB0KR3H73Hw891ZyJOJU6B8OYteRM1j8lrdWL6/ZeOA0+Jk+w0jMKiP+ELYXN1oGHjpe143nN1bg6dfFJobuXHAA9y4pQiAUwfItJy3TIugReb38wTCOVHfE+s2U0uZkMAe0vj83Elf/btQ292PRm+Up51c0LVPw/tfXbMLZYXsrvOgTMB5nWJ+tKo0iIotYALXjj6gUxUjI+XrXhwofw4JjL2DQxBJph4gC+P8M/htX+HzyrPyqdzg6+7yyICU3lp7BAOY7cNmxw0nXYGyBSvzXCrfrGPSKjV4qUSyDwCj/+nwCrn8psutwMwLBSGztmgiPvWpmbTbHaX1r3XeMzvzbquQBZip1ZWsBNFJ4db+FghQJTIHq68oH66AaolEbzS2Ayb+nTJIFdRt4yY3ngSRJKNAFoAiG4x9i/WSM0wHI1qJGdA/4kwL8qANhs8tpA03E3EWRuMbE7H02+3hKsf+zZufhMzhR12V9MUEkSTK1KniC4OMoFVi/NGdVKTYVNAhZtgHIdaONfq3fbXGq9n2rbenHmt01eHBZiXKeXZ1LBn/F+f3zhbZLL7TnPaTzfDFb6+XFxOukPP37JHaeWdmq9U3k+TrBzcSaFzS0DbjygIpYWJ7W7KnB+vw6rFNyvurvLCoBpVxO+yPc9iHX/cETrSisaMOcV8TTiRihDyz38g6O5zZW4N2j8hpRvct8z4Af3f1i8SXe2q+z3jhoxw1tgzHLuwhuxoGPvlqKo6c6cbBcXrsdjkRRd7Yf89Yew6C27xTsivXfQdEevHtUbFmA+Hc2+biI4Ho7n/I/q/KcLs949thS/P7Aw47OARBbIzga9iMYCTruB0UUQDX5eweA9wF4v0MZxwTbihqRf6wFz6xPVNDcLsI1queO3tGY22aq+HzwbIp73V5jtzUtwnn49FYH3do20WtaluFgnxVuQhsXVTpf69HaPWIso4WGIZkNkA0vI+EBk+huVY29ppFPnXYGj7ykCaksuCA6sTzdb5EGoD/H4BAvBzzamWbR+pE/5tYvo9s2ajURYBRaXmtxE7EAqsda3euSTZUIGSiWTlKzSJKU4GLkzvpr0+YkCWv31GDB+hPaopMIhaN47NVSFJ+0fpdf23UKdz17MLHduyVDY3InaUkSltdYhIy1akfDo87WjXntrGEVIC2xYPFrmrXrSZPyhI5LtZx0o7pge13+X1Yewcrt1cIu7r48+0kzdUmH2cT8keoOU6uvHepkmZN3xqjKHlpxKOG3ulZU/e76dBbA3z1XiN8/n5mVUn/SLOuxX7dv3x7MDolGJYwGwrh9bj7mrCpFZUMvdh2x91xILiDx5/7jZxNSh5nLJfYMRdu80XERKf4NLDhrvlxKOwmmv8xoeBQtQ63wCalWcVJNWN8X6Mdv9/1RKEWFFlspNQngX+Cc3w7gCrdCZgufL94ZDIwkdl53PJWfBYnssVpzYIhFuxd5wdwOshNfAPtrmLl6WAeBkffZBVPQs/eoffRG77DrfBN/m1W3fjC2cMMJ/HbhQdOZvvxjLUkfKDGJkmnUrNVwYgE0eyZ2z0povZ/BdXw+a/n0VopS3onBkSDuW1KEOxfs15Ql/xsKRy0X47d0DccW0Met6okS6JUXUwuY5u/NhQ247Ym9CS5xvYOB2JoZszyf20sa8dMn9iYlPjar7weWleBhCyXn1Jk+HKrqiMvoYE2LNgdWggXQhTun3XyN/pJmk6ynzvShprkfSzcZB9dSUWfwG9sHUVBuEp1UkHSmBUpY9+nC6mn3KJImbjQbnN6X9rk7UUBSDYZhVZKRVckI/bodYVuC2aBZ8HwzTjb0YP/xs+gfcrZ+9KVt1WhqH3Q8KbX3aDNeMmLduAAAIABJREFUeLvC9rmJKlSqRdU6B6a806X3rTUm5Q6OBE3X9rl5j9VmEwxHM26dHdSMZ/WP7ZGVhxMCDIqIZiq/L1lJdzO/oP8uNLQNJqQOMyMseRssx3C+XrN1dbWFIuXzwae4eI6GR/FGzSb0+uVv9WOHnsGjh+Zj0CQ9hBtGQiM42nHCMgF944CsjBecPYQdDe9iW/0uoWvbKoBq8nflvy8CuEpM7LGDDz5MPcd9INOm9sGEhczO8745p+J0j6PcKqnK4zYIjNk6N/l3cvoA7QBctHz15px+I5o7h7GjpAkHzULPpxm9ihD/SxL7UEjAsZqupEkLlwI4O9V20Bg/IBKRkrYBysyvzWPV77ZbSykqn5bKhh7sPdaCjl7dB0z5966FB/CLefssr1Hfpq6xlM/6zTMHsFgTeU4vzz2LC2zlUt1/KjVRL+9ZXICHbQK3rN9rv75Ry8BwMCkNgB51UNfeOxKz5DlN4xJOsAAaH3eirhtz1xxLikjX1jOCea9bJ9MWVSrdDCZTzqnq4PSfPP4ufvL4u8LHb9MkwxbV/3y6Y/WKo2ifnnRfdpH6tN8D60MT2KdJteJmYGmdB1DAzQDApEnx9Ccb9tUJBy8xe0/05Ta0DeC+JUW276LKU2vLsHJ7taNIsCrD/rDjSZhXdp7CoaoOBFNITaBFVaitc/wqf6RhLZzZ/d+zuAB/XF5i6P01Z1WpbS5XFfXy2omD0wYpqpw8hlR6IX07bGwbTPBospu01B8z7I+PO7x6Ouk2iot+s1IZwff5+2L1cXa4DXvPHMTKk2sAAN1++Vsusiavdbg9FqHUit8feBgrKl7Fn4seNz1Gez+bT+/AVq8UQMjJ39X/7gNwt9CVxxjTpogrgFFJwuI3y1GsvDwPv3Q4YQY9Y3l/HLx1qYokek9WkQeTBvMAVmyxnoWPHWtpAVQL9zmq+0mTfELur15g7AJqfrzZfdQ09xtud0M626n20qqrbZKV02ZQbbTf6AwjC6AVSaGmYWw5UducSO6jnoFAgnzD/rDl2p7+4SAOVbUnpK8ww+gxhV0MwlLVYbr7R3H/kuJYPk8hD1DVAggpUQE0OdkfjKCqsRfHdTkuqxp7Ud9qPeAWnV0XCebhZO2MCF68aZIkYfmWk7HvjkpNS2p9glPZtM9OX+WO1qWL6V0APLAAWk00afbxpl7TtqlarCpO92BrUSP6hsTaiGnZuu2v7jyFjt5Rx98krcI4NBrCzsMCSaYl60nGitPdeHFzpXFwD5sGI9oG1PdwaDSU4IU0GgjH3EjVustELJTa5n4s21yJsDJhaWbJ3CioAKokyG40WZkh/3BtOzQKCicyHtC2mQcS1np784DMJPBqrCLsdeBwvKalY7QryThjlhDejmeOLRU+VoKEsk7jgEpG9z0SGgHvse5rJguU+zTnfLP6gzH2fYFzskrSx8QHnONAAezoHUXpqU6UnurE9R+/LGl/1MayYUZ6uwGxwbQZVi/goJUFKsECmPzFL6pMdj+VJAm9gwGs1YTUV/udcCSKFVurcNMnLoc/GMHfX31R/COhK8+OyXmp5WJxgp1LgdYdNRo1HzSIzDpnYm1JdZO5uwEgW4pUVOtJkhukJFmOFkTWTa7YehLfv+VD1sIKYLy20vl1JMm4/o22vfC27NryT596n7Ul1OBcN7PwqbQLCUCnzkIqonCp9Sq7gGoUB+GowsYcru7AzOlTwN4/K7ZN1EpnNVjxB8N492iLUJRYJ3jxSvYOBlBY0YbCijbD7w4g14G8/jO12cFE62DiPn8wgl8v2I+vX38VLr3w3Nh2fQ5HgYITfqWS9Nq2JEEL4BOrj2HRXV8wPE5VAP1Bh/nyTIrWf1MTvmMueXl7tVBQGcmgfC2qVbG4/Czm/Oz6hH1eDMY7+0YTJr+WbKrEdR+6GNPOmYxfzZe9gM4/d0pMEXTi7SSKvg961CRYmwhVjb3I8yGhP1Insexkd1SdLqt+77EWXDRjauz3E6uPJux/c38dthfrJg4MytI+e8txH4DCijZcdtF03HDt5eKCmlRGwqRXCs0vFQugfpuT72mewzV/bllWvgqLv/Rk0naj+3nm2FI0D53Fjex50+uZKoCMsW8CuAHADxljn1M25wH4NoB1zsTOLHfOT3QzHB4NCUevenL1UfzkGx+1PCYSNc77ZofTAZqzKKCplScrJRI6ekdx6axzhQcYljOvJtsjUQmv7TqFYzXxHHKqrKW8EyUn22Mzhrd88or4lXzOOlPVpScjGM38abZtLmyI/f3C2xX495s/aHgZbb2n+hlORSHQrh0w4rFX4x8Y1QVUZFbNbo2QfktBeRv+/Wa9AmgdBdRol1Fz1l8jFI6ioMLeXdhogGRX01b7jfQavYukitXgNNVxm76ORHNzxsvXWo6sz33lHY6PfWCW6f7nldDxL973JcPrW6F1yYpGpQSL4H1LimPRGd0SCEVwpmMIH7riAusDHXY/IvX9qBLRcPm9t1gXrbeCJ1nFfRgNhLG9pCmpPupbBzDsD+ON/Dr899dYbPs8mzD8qaQBShWropLlMD5a7XudKkDmLqDG5aZiTRENiiMBMUuXFUbppbxQAO99oShpm14ebTAZnw+eJ3O3uwsn96kG5Fpx7y0xy7D63tgmM0/zeyBJEl55hyds00+2bClshAhOHn3vYAArtlbhhmsvF+7qzK5/+uwAZp53jnjhZtc3rGx375uTMX6eLw8tQ/GxQ6asvrHyDIprHrLXeazU1uMAqgGMAuDKf5UYB3kA9TPFw/6wfR49BbPAC1rcWgCdkqrVval9SHjQFI1K2FLUiPuXFseibmnP5E29KDQYHEuQ0DPgN1zvZ1Z2KBxNGuCq5+qttzVn+jQzpz5HL5Y+rHc6sZNLG7AjEpVMI6l5lW8HyNwATH3fROb3tbOysjUtcb+RzKKuovFrJO/1G9Sr/rDdR85g1Q6edFyyPEaFmh+/bPPJuEuOhSuqFjML4OrdNabliES9jUYltHYPG5apd50s5Z14VpPn0Yg8kwkLOwvg0GgImwsabOXVor3k9pJGnO0y7tO1aUD+/FJigKRUlb+9R5vxi3n78OgrpTqLmAcvm9klDJ7Vcc3kmRs6+kaxYV8dthQ2JE2Oap+pfmBphVduXP3DzoKeANZ9nV4us6YZO8zhbZiVbVaum++6k0iWslAGKQYEsbO0u33KVmMRH8xTfFhdo63HfFxn1xzdxD14VxdYTpKkhPel1mAJRzTNg0XXKbwcXsvq2yPaqM3ca1OZFDneWYm6vgb5h4H4m0/vSNpmFFVUf6ple01aU+3D06VxS1u6l4nV9tWjbTgeuE1yGXLK1ALIOT8D4GXG2CsAPgjgwwBOAMhkaMWsYOdp5DZggNNG0d7rIGehwaXX7a3FlMlipumoJMWsbsfrunDjJxLN+k+slmfApumC6Rw80YoDJ1rxgy99KGkGx+x2w5FoUodR2zKAD7/vQoNO2Re7NZ9TC2AmFUAbC6CerUXGM3IiOexeFlBS5PIzowHG8jwZWQB0j0D77hha0gS22bkCG3WFRvWtzwNnNZjQnuPUAmiXWsToMS3cYK14GfGXlUdww7WX4aff+Jjh/td2nQIA7Cltxs+//fGk/UYf4bJaY0WjqX0Q73/PjITnm2ABFOgjRwJhZ27qmmsaBcKRJAkv76jG/uPxiaqWTsE0AoK8svNU7O8zHUP4+NUXKWXHj3lwWTG+fePVjq4bCkcs1sckb+sTiAxpN6jSD2bj59le2pBIxOD9d0F+mfHMtaNALwk7xY5VtzuV2+z4YxqFZsO+upQm9w5XdeBzf3+Z8LORJAlHeIdum9i5+cda8I/XXIrLLz7P5Nrxv3sH5XY4S+N+aIZVlyCqAEQlCZM0x75poeTaff+EgtPo0CupEhLfF8P1nWn+DNvN+/3BQUoKq/ytp87YG0fcoo/A64Sl5S8DAL5x9Vdw1cwrhc4ReSRRSLjgnBnoF1jfl+fLgz8Sj+btViETZf7RRLfOpkGHWQMURLSDXwJ4AcAcAP8GYKGrksYRh2zSJkSjLg28Dk9aYzXjInhpddBnx7zXy2KhkZ18AA8oETbXvlubFMTBTJkJR6Qko/yWwgY8tbbMQLmOW1uddhH6vE6ZJ7WeP9V1SplyQgiEIjjZ0IP7lhjkKtQIcaS6Q2cBTJTwmMkMsP6japcGQtjqDWB7sZhrTOzauus3tg1i/rrjji1LjZp8oUYKpajHgh6rnFl7Sptj63JONiSu8Txc1YFlG8VdsEYDYXT1jWLjgfiMbmLwEPtn4GR+JhiKWKZqqGzoQfnpngTlL92Un+423N7aPSKv/3TwAt7x1L6EQBk7D8XX61Q1Jq/H7TDJm2aE3GbFZXE7G6/vu51MQGmPnGqyXt9tkCODL4rJ9RUF0GHP2dFr/K6+pXk3thY1xiaYzGo3IbG2jpFAGCfqupI8GUyVWbhXwDceqMeDy8zzoKlEolHcs7hAKOIxYJ0QXpRfLzgQ+1uSJNN3UN5vfS0vMjaIrMd1Uowbke5aeMByf5dJUnrjlEtyInttBFDAuyigZpztHsaowOS3FVvrd2FN9ZtCxxq/48n9199d8AHD8/XrPvVrADOdA7S8q8rVeSIj5B8A+AqAPs75MwA+66qkcYTVrBKgLsR3ft10NolUG5zWZ/zoqU7DKFJyOebX0CcGfWi58UckZPIhqG8dSPpINHcOJ+Snc2JFzaAB0JDMdgHJCCdRTpGHVhzCU2ut1wcBwHMbK3QWwMT9C98sN2xf2hx1MTQHrtldg6qGHgRDEdy/tBjvluosG2btQJIS2r1I0+ofCmKe5l4XrD+O8tPdjhTJls4hPLIyHlVYynDuKCOqGntx0mGQj6W6CL9arxoxLwnx0A9vHTiN9RYTIiu2nER3vwOPCQ/QKmZePEFt7ldtgCwj3jlkn4Q5VreSs++D234zSQG0ODapH9f81HuZxI9xbrFxcqy63emnVB/R1g4jhSEQiuD/nj1oek5JVTsWrD8Rs7ipmInq1fjz6KnOpOA/kiRh15Ez+NmT+bFtdy08mJDP1AirPkF0zsEfjMSWjxyu7rC0qtop8lYu8+KWVvtjHU2EuHhwIlGsjdiwL3mce6KuC797rjBB0QaQ9jCtlfU98T4thaJ6A+6tlMldkvizGAoN4T3TL3V1blQwyb1K04A7a58RIgpgHpQJJeW3cwf9HON3zxWiuVMsl08C2R/nCTN/XZnhe5hyviwoIe5NXnLrj4SN6SeLOHUBzQQNBjmJMo7eBVSj4ButaxGP4hVn15EzmLu2DHUt/WjvGUmaiDC9hpR4nfJ6sYFcnWZyRF2zKhq1c1KeD306a2FUAl7eUY35LvJ9uSXV2Xifz5e03sVKuTeibyhgW2/hSBR/WlFiq/D0DQU9mdF3ixczvk5SFYkQ1q23FcatBdBBm9K7mmnbzvnnTjE8x9Lq7+DDYPasymq7EApH075+x6h6R/zW1o8z7SbjDVNRJdP7FL2/qoYeLHqzPCn4T1SSsHZPonfSwHAQ+xTXXbNybb/tgvQq7s/NNu7dqVgAzScJkq1EduWI9g2ZthoZYaQUipDluXZhStvL0B8YEKprSTJfvalPTN/t78XF0+JBzZw8y0jUmQJ/rNO7YEkiaSBWA9gP4CrG2DYAGz0rfRxQ7+EgOlN52ax4bqNxHhE94YjxJ9XxYnSTa5jN/dtdP/tdpBnJkulnazPNrBnTslq+pJ02UvBrgvzcvagAX/3HK5PPscHn8xke12SSYNns4xTVfbz7BfN+JVzDodZx8cxpmKwzsewuPRPLM5gpTli4Tonw6s5k1/KECK8C9XJCwHISiUi2Az2j8rWoblrbHLr7ihKJRoVzxlnhxK3TjtbukZgbsWShDBjhtE2riESdNCtD++zMFAXLZX4eWABLTrbjkgumJaS+MKKivht/f/XF4gV6gJm1yrzNG2yDhBN13ViwPj7RZDVw31YSd0PeooliLUnGAdlU91Sz+rWaIHCiQIimE0klCIwoUQHrumgxZuutxwLjRcGz48XK1bho2izcdMX1BnvN+yQ94WjyhI3Wkuck8E9tX739QRoikneBAkUsgLsA3A7gHgD3cc6f8qz0ccBfXz6Slut+8IqZnl5PtLkdqTZwp8swoUjUdJLZCzeRbDAWFVMvPnCpkVy+3s9fv66r3WQ9jQivm7jNWbl8pTrr6mTQK5cpJVm9Mq38AckBO5xi5AEhMoh3ipm7uBGmA09FllTX1Bq1zWF/CPPWliVZRACMqVGTk2k7t/2GSBTa+LE6C6CmPWrXxybilQuo+cGnzw7Y9uWGLukO6OgbRX3rgGmqFyOcvq/6yS2VrUUNCb8/aJHKRBtETbssxiwQ1qiSosZs4OzVt/2Rlw4rk6vWdZJSEBiTa+ujxEuSve1Z9BvjdcAqTxlDfVmq9Ph7DZuOXmmLSlEEI8YTe6f7G5K2aVuCk3HFouPLhY9V5fIKEQvgCs75jQDcrTIkYiR0jF6Pzb22LqbxhQ9bzODZuYk4saJm0qNiDHhvJBF2MCBLG7p2pF+voE89sHSTdf5BQHW7ceJiYTJLjtQVQKdW/UhUcjTwSxci0Wadoh3AeOXt4CRy4iaTEOORqIRAyDqpsa0coQjuNwhy9LdVpWgXiB6bTYws8Va4Vd4jUQkf/8AsVDYkB61JlsncBdT8HIt9Dm7Qyvqdl2edYxRAggXfTTtv6RzGX18+gvfNPh9/+elnANi7z5qVYlZ8NJp8jlHfGkvhY1D/ZjKZeRGpx5s9y0hUMlXuj3D7FBDa62wrbjRfK6pg92jqzvYjFI7iqstmJO3bfURsnZU8iWh9jFVgLi2ZjFo+lpl0gXhbcEswmvw90L/3844uRseIuFVWe3468wDuPWO+VtgpIgrgMGNsPuQ8gFEA4Jwv9UyCCcTwaHzQ5bn+5/H10tkVGUUBBYDJk/LQaeEGZRf+P5vUNKcvRLJbvLLCpITeBVSneOgVQKFLOrwtfZ4zlZ4BPyocBj5JlXBEQjA0BhTzNLBHE8SkwdSK44xgWFwBNFNqfzFvX8py7DVJmTDWlT9AjgLoJLdeZYO7dyISkXDh+fbpAAADC6BIDkuPgsBY5foUSTU0GozgF0/vw7dvuBpf/vT7xAvWobWi91tEALXGWNiSk21CURXrWwewvbgRX/yH9ybtM1Pkm0zWI1opk4C33lShcNReAdTUTb9B2pSXtlUDAB67I9kdsLBCTGlzOhlpRSAUiUViH29YRbB1yuT3iK3hT4XtDbuTtumVNifKn/78sbCeUwQRBVBNIvIe5d/xcWdjkC5NlDrP24fXBsA0aoCyC2hyAeFI1Na9xsmLlUmX0YUbvFuY6xWV9T3oHwrgAsFBWSbw66xfbtbdyZa71GXJZNAVlUg0imKbNDO5gJME4lZsKUzPuj2nGOb3Gkcs3yLuwKOP+ijKn148hI9eNcv+QCS7TgtNVlmuAfTmA5jn89l+Sk+d6UMgGMG6vbWYPk1kCGVNVJIwZ1Wpy3ONtzuxqK3Pr4sFs0qFmAKYgcHv/uNn8eVPWSvf2sBRSzZVmh738EuHTffZERWwAIqyqaDBmwulAauhlJw2wnuPkkyTqmtlTd//Z+894+Qorv3vX0/eMJuzNu9qa1e7K620q5yRBEISAoQEQkgiiCQQIIkgG7DB+NpgGRsHMA5wL9d/B3wfx2tf45wAY2NsnLBpDJhgwCCBIsraeV5M2O6eDtVppmfmfP0xmu2urj5dXV1Vp86pUxI36RxRkwxbL1EUP5AJQZzCy5q3NNKXmfUSPDj/1IKuq6YdfvH0a3hWZW8rIwTB3N49ViNa5Qtv7TmMO/77KXzs6tlZuX8MSOs5nKhTPOsuvIrd9UOFxhPP8M3EE95Abc9CNY4rLLtcLqA656SBTezgE4xdQMtLQqkAXw898qzte76jsU8bFw41hFb3HZWSXKeYKc8T6bYpRryu83xm3MyVxPuiXO2N+NGL0sobpMvrOKk7eFkPkWJ/+spjeMHrTQvpJq5Ou4E5XeEOHz2Bx3Q2XrYDT/Q/NZ4SdyEYcDZUer6TzUik7+w/mhbgxIkosvGq7uEPnSAAqqI6KPs/nsAzen2c1b3QlMTXAOqnccq9GQDeePtdHDhkfX2qU8rH7xwIDpdU/Lywr6mSQ0fsrQHWgmcNYL7zE85tl7wOKYB5wKgXAl9ocOTYmAVQOQNKAOGg39AVhSwCuYUy8iJvCG894mHtbWeTt+z8yh+yLQJB6KKMhPtbj7hF+4TMDt5u+cJvbV3vpXbwLy++jW/88gUsnDwu26KkYTZaMy+jsZjtiMq5gJ6VNNPr6N1i3zEHt3zLkdk/XQWQMdYG4CIA7QBeA/BfAPoBvCKK4h+1r9TMzwfgMwAmIb6h/KWiKD4vOX8ZgCsAnADwH6Iofo8xVoP4XoRFAF4HcLEoipr2fC9/jFLl5mieBoKwQ8Av4Kg7E3UFzQuv70NXk3a470xiJqy/Fl4a9HgRZahyIktQUD9NlJFw7UYBdYr9h47nVPviNVn/7wn1gDL5yk+e+hd+8OQrxglznK+qbXOTZ/ziX487lleuWAA19wFkjE0D8D0AbwB4GMCbAB4BsA2Acax2dc4CEBFFcSaA9wD4mOR+DQCuBTAbwGkA7mSMhQG8H8BXRFGcC+BpxBVETTwR+VCDQ0dOpFxdnLCESMmR+qaLnp85YR2rAQbcwJGZ2Bh51xFELrPzq0+bvsYJ93Ejnnt1b461Ld6T9rE/u7N0xIt8/zfeCFJFeItRBzdrdxM9C+AHASwXRTE5vfFDxlg3gMmiKFqN+ToHwA8AQBTF3zDGRiTnpgF4XBTFowCOMsaeBzAxcc2HE2keSfy+R+sGL73hnBnXad7ccxiX7vw5JrRXOhJ1S8ofnzcXstaLHDxM5j+3+KJDURnt8h2NvdrMUAgzrkTu88ivX8q2CHnFp//HtNORJR7OIWvHl370XLZFSMPL0SwJIhN8X2WbCS+iaQEEEJQof0lehD3HljIA+yR/n2SMBTTOHQBQrjiePKbJL5/mjwyVLf7GsVEuQTjJL55W38eMIAgiF/gtrf9OoxC2lCEIwh30FMASlWOfAhC0cb/9AKLS+4uieELjXBTAXsXx5DFNrLpCPrhjIT53w3xrF3Mw1F2Tduy966dYzm+wsxof32Ic3j8cjEfNbK0r1U1XGdXfK661vhQNVcX8AprArufnR66c6YgcoaDe5+Au5SUhwzS1FZEMSMJHaVEQ926d51r+5y8e71remaSqLIwv3LQAn7thPs6e12n6+uvPG3JEjqHuGoz01tnOZ+GUcVg6vdXStRcs6bF9/23nTuJOe8uGYXz+xgW276nGlWf2m77mnPnq7/8z2+fhjFntqueKwvpx2uodbJO3rpmE8c2ZXSts1KYF/Nlrk9UY6KiydN37LhwxTuQCU3W++dsvk2+AXl4q74Me3LEQW9dMRHtDFG7w4I6FOHdhN1faq84awL1b5zpy3w9umuZIPk6xcnZ7tkUAAPS0VGRbBABA7KT7kd4/Mf9Drt8jW0xvGMa9Cz+C2U3G9Vyvdf0BY+yuROCWZACXDwP4oQ3ZHgewLJHfDADS3bOfBDCXMRZhjJUD6APwV+k1AE4H8KjeDczoEt2Jzq6vrRKCILi2xUBpURAT2tM3yA0H/fD7rGk/fp+ASMhYXl/iDRulvWRZn+a5kkgAV501gKKwO+UTDNjr5J0aJERC2QuKGwz4sPmsAd00Ab8Pt27MzkBCyaeum2u4CXJvq/UOxWsDP6v4fQL8Ph+CAT+KOL5XJQG/M+tiQ0GfIwuFAz6f5XfjN/EsJZK6VVdZhJHeOhSHA9zBjDafNYCuceUI+H2Gk1u8rFnQlfptpc2qKlNXdiKhAMIadSNapD/fGgk61yb7/QJCDubHg1GbO9BRhfrKIkfuFQz4MKmr2lYeVsvHl6X17Xr1VDn2uGDx2ARNaVEQgiBgYlcN18TNxct6ueSRKpOCIHBPJgkCUBwJYs3CLuPEBoyr1Z8MV+K2gpatuqGEZzyZCcJB/nHY3HF8k/9t0RbUF9em/g748m4DhBQBnx+CIGBl1+mGafV6sQ8ibu17iTH2RwAvJ/6+w4Zs3wJwhDH2a8TX8W1jjG1njK0URfHfiFsYHwXwMwC3iKJ4BMB/AFjLGHscwEwA9+rewcS3lKzw0u0ZeDDb+R88fBw+FUUvbKOzHY3FuDqjZFAco7Rq8iX59NZ5qKssdi1ISywGhGwogU6J5dRg2wo+QdCdrQXi5dTZVJYhiexz3Rp+a40Sj/SJaUyfUI/FI83cFiCpzjV3kvnoeIdtbFIsxScIWQ+QZWag8+mt81Iz0pWl4bgFYNs8w0mHJNIokx/cNB0TbQ78AaCmYkwRsTJoC+oozloTgaXF+gqgltfCLRuG+QVLICDzg1GePtCpyaCl01pRHLHjwAScv8iaZ0K22jO9OR9BEGSeQaOSxGfP7Uj95in/Zg2lap6izVtrsfxSONSEXb5yAnfa1nq5BXTGhHpnhLDJhy6b7mh+ym/fqYkzs/B+KsN1k1AdSTesqOYpCLKccz3g4Hk9Z2meExLPWRpUc+KUo9mbJlwzr2eM7QBQC+AtURRPMsZCACyNSkRRHAVwpeLws5LzXwDwBcU1bwJYynsP3tfa31GFcMLiZ3ZT9kjIbzqKp5qCFQr6LU/Kj8ZiXJ11tCiEt48fMfyYeQyRyiSLh5sxzGrx7pETuPebf1G9hodYLIb7r1+AS3f+3NL1Tn3MVq2xVikrDmJ/YhNggePeyapyypRx+NkfvLmm7/QZrXjkN/Glw3bK06vN8/IZbWiuK0UsFsNnv/OMYXrp960c7IaDfpwcHdWNjNrbWoHKaBh7Dhy1JG/yWkFwJkKydONpAe7GIEzWAeU9QgFf2j5ySqTtc3EkgGoN65sZpBNEehNmWug1U1rfipoFsLQomAqYpRUm1I7gAAAgAElEQVQZs6nGuPNPl0/IuKIS5nC7N1PHggGfZt+8ZGoLvvqTeNCUmvII9hw4avqbqC63Vo+s1Be38fkE3HrhCC7/6C8AyNsHaZ/KI7vaWOS+bfNQFA6gKhrGt3WCf5lpR3jSdTeX4/l/7dNNM62vHs++vBe/+tPrsuPNtSX41653ZceU3+blK/vx9PO70/bGKw4HcOioOWMCYL0NtTuZoUT5nq8/bwiV0TB2fuVpvPzmAUfvpQ//t6JlybugdzW+/OzXZTk6/QXOapyGX7/xpMO58jGveRa+9ty31U+aaMR1W1/G2PWIb/nwFOKWwPcA+BBjzNmpBwfhVQYuPr03VU7Sma+ORm1/96QSZWVgq9ZA8nR+WsR0Oq73XzTmJrhpeR8WDDVhjYGvPU8jr7xjfVUxWGslpvTU4paN+jPOl67QdjEdHbXXQTr1Yft97rsdSt1o/X4fRljcLYHr8RP1dO2i8bjKwF00W0i/DVvv1EQjNmugwVTedtZNJb8Bfvm0v9OtayaizWB9TSQUwF1XzNBNI2X6hHqZ6+3eg3HFMVocwkkHwuiPb7bu1istMh6FTKuIeTwwlAqiE/sySS141hRA7WsmqqwRB+LKnpSSSAAz+scsEC0aa7utyOcTsuCOZnA/weTG7HrZSYtEENx1M1eu6bY7Sam0PDrhreITBFkZjMoUwLF0yjv5fQJOm9YiO1ajspYzuX71pEa+KTlM1NUejvanrsLYZdgnCLjo9HS31Ts2pQ9t1WQeVRl/3Xj+ZJQZWOydxOkJa2V2ghB/h2sXaY8dra6JdQq/MDaeiobG2sJZTdNSljAgObnlbHktbnUvBoIW5/Wche1TrtJN4zMxItbbB3ArAAZgWBTFcQAGAIwAmCSK4m+57+BR/H6faoW4WcN15sEdC9E1Lr7+xEpFUvtYjRb465Fsf9RcfaSKTE1FBBuX9qYNJJRIn+m966fgw5enDzr1OuIWA796vQGf3cGZIADXrBq0lQeQGRfQVQvHOnKfIKTeI5cCnkgb8PvQaGGGPxNIB5B2BpNmrlT7jsbplM+Zc9otucgB5uuqUWqe7IIBv2aQECWspQKnTRtbV5PMv7aiCKsXdKMyGkZH45gb8cLJ43DftnncwRiSExYATM+8SOuDqaqhKCSegXuFIqCFE96vUlcwK3Vb7xKtQWu0WP4cfp+AlbPH3PPqK9UnM6wMDgVByLilqq6iCAOdVdhwqv0AQYB+3ywIwtgEDgRX2/vTprVi0XAzgLjSLi1WMxM6SZhkUqd7XDlqOZQcNaSeQMk6sngkLme3JACQoKMBzp3UhPNOGY+tayamjpXoWKMqy+L37Ggskw3KlXLw0M0RpMjpYHVqdUpNAWxriOJjW2ZjSk+tYdA9KVbHP44rgIr8km2cXnvb18bngukW0rHuBb2rNdMJif8BwKRaZybP3XYjLQ6kf+Pzmmehq6Jd9zozcun1pKsBXCmK4gEAEEVxH4AQAPuLKTyAIIx1yNJvWcsKJMSnIgFwWmsUKAcMRWG/rQqUbDSSSqkUedvNd4+m6rFGMxTwo05l4b3+egL9/P06jYjdsZkgCI4oRHoyOsXZC7pTb2RyTw0WTh4HIO5aaITU/S6QRZcirfUegHMWhKl9ddwRCVUnNwzEiHJEXU2yabm29doIo75dbSChxvKZbShOKLp666ZiUO8AfEJigHL1bHQmFMDSoiA2nMZQFA5wB3iy02ZJL9WqJ7dsGMZHN8+S3Utpt+QRQRnx1O4k09VnD6JMUmec6AMAGK5NLCmST274fILhZB5gVQEEnv+XbpBtR5AO0H0+AdvPHcL8oXGa6c28Or2nFqS+hgIQsBl8TI9YLIbm2nifxForZd9NnYrSfs78Tl3FRVp31p/aY0JRlxeeNNBYMst1i3tw//b5mpMJyvX5yVvztgVzBhuxbvF4XHvOoOoLcnrSYZKGNV0NvYnCJGrSabkO+30+bFk1iMk9tarnkyxOTA7YwekJDGX7lPxTqgAqFT43lCDeMWsMMdQWxdvP+uJa+AS9PkzAvOZ4wJip9ZMBANumbLYlp13fs9lN+o6U1svWhAutzrmTiTV7Um6EYo2e1+B9dFll5+xhzLt/Se6X1sjZqzx6A0dpzryiSv3Jk+sLb94wjNZxFUg+uX4x6d/IzfV1UmXeDplQqsJBP76wYyFefG0/2hujCPh9+PyNC7isGtLyLy8NoawkhP3vHnNR2nTu2TJbFrEw4Bd017BZJRIK4L3rh/GhLz6FF17fr5s2asHtpq6iCDvWTYbPJ+DOL/1BN+242rFBggOehDJGFRlO7KrGn194Oy1dKOjHvdviLie/F9/Cfd/6a+pcY3Ux3nj7UEpAtW9B2mbFVKZcMmH50bMqAPFtCNQmtHTz0UA5mLH63i5Y0oP5Q01p36dWeW04jSHo9+E/v//3dJlUrjFyoVJGpuZtR630UQKE1JpkQL5G2UlkrobJf3XEdcJ9N34PQVbzpe39OfM78Y1fvujIfYD4dz13YhNKi0KY0F6J/Yf02+nlM9vxxDPae/pJ14i31ke5Z02lya4/b0h2QFoflVFopa+jrrIYZ8/rxLd+FS+fZN1SvrLqsjDe3p++Vjng92HxSNxldPe+I2nnz5nfhS//2JkN7e++apZmtF01tp83hOvvexyAdrRPK9+S4cSeA82t0ZKV8xeNx0v/3q9br2QiKZ8zaQGUTADYjdieZMlIC3781Ku6aQZrJuAvu/+meT4GYHxFFzYNrEd3RQdeP6i9T6hPEDB33EwM1w2hOBg3bnRXdGim16IyXIE9R/em8rTDso7FePx1550pzUil9zZHGWPKsIT7YDEATMYw8fTJF2i2e7Hy4pWdv14OpUVBzbDgSXTbF5mrFb+sZ8/rRFlxMDUr1j2uXBYyWdkRy8ZzRhZAyfM7vZ+dwD1npE+m3J98goDu5vLUoNLKWpRIKICPX228D2Q8f+eeq7w0LAvdfv5iueuW0xOCPPVXzR0qeVVfWyWuX6vYSy9RjVlrpcyqo3Z9/PfYX0WcUShTtzIYvCpP84115WXyIcl+XnELoOElaWQiAJKB/oe+No31PYoyUT5fa31p+jtWZmFRiVg03Cz7Plvr4+2hVlCtopBf00JntP5pyUhL2vm5Exs10wPOBuFxe/nf5PFxy0yHSiRjre9cqbTZwSeM1QNBcU89rwYrjMbi72qY1cZd1DkeQm9cofw+X9v9rmq6YMCHj109W7bkIll/+zuqZBNOevdTvo8zZrWnvAS0rrtj03TcvH4YU3pqccclGnuQqVy6yKI1bONpLO2YGeUPkH/HahG2Z/Y3WFLWlBN7UnpbK2R9itX5DWVbsGpep2wrB19iCyKrJK+U1j1lJGOrbQaPWBVhvsnAKXUTURaKwi/IM1Wb6Ewqf1a5atIl0rvbyssn+NBXpe36bnVUq5dnmgw65+4E8H3G2NmMsUmMsVUAvof4tgyehVfh8QlIvT+twcHikWacM78T2xWbD1up9GoLbLXYtLwPVQZRO/UaGOVid17OmNWOT1w7V3PLCDvraKSN1Q1rJ6umsboVRNwCaH/0khn1j58PbpqGz90wP+XOpnzlvAprJlxbkzjuEsKRXVtDFM21JYqBRPzCSMiP/na5lSWWlsqYO6+YgSvP7OcKMCC7l5ELqIXeX99aot5xGD1nJiY/jCbOlHUn+aeyI7fyfE7tgHHrxhF8fMtsbTdMQXvCRd01d+zY+YvHp63FVK5v9RmMnCpKQ7hvm7XgBEr57BTZilnpLu3rFvfgs9fPl/VtXPcwIYj+vKi8Y5QObHmareQ7v37tkG6wOEA/SJsWUnlKFBNNvJ9nLBZDZTQ8llcMuHfrXNy7dV7qfCpPnUzVymNBwk13oFPdal0UDqC7uRxbVg2iWWMNnNVBrVozOXuwMf2gDZT3WDGrHestrk3Vatc/dd1c3Hj+ZMcnW0qLglgxqx2f2T5fdtyO9Tz5vUjridQCqJxEMQOPEcUwheLZ/D5to4kzJgKkXE7jedpDgICrJ22ymQtw7dDlqd8fmPkeDNTwL1nR2wbiR4yx3QCuANCK+D6Al4uiqO8vlWV4X4ogjFUJrW9kncK6kexdnHEB1U9r1H9IXQyUbmNS+ZwckKvNqIzdR//a5PqliV3VmgvYr1k9ER97+I+m5RKE9Pc+vrkc/zAIBa2akYdorC6BzydIosVaa8zdHNsrs860/nfVORNRVhxKRW/76e//ZS5/PctD0logxINtaK2R0cPojSldhfS+sTHZdO6n0ZjJXUDTsRsBt6Mxin++YSJUuMpDpHkgJX8YWAARM64nPOUqZWpvHVbN60w7HvD7UFEalu0zKJMNguaEi9pR5bO01Otbooy+5XDQbxhc7PaLp+L2//qdoSx2WD6jHT/7/WuysPixxN610jdhNEAVYP7daealsEDzbL0j5drV8aAn/e1VeOmN/br13crEjvQTVPb/vF5HqdtKkoeCfoSC6fmaVQBXL+jC4pGWMauZhfriZB3zuxTE58bzJ+PF1/dh+cx2ANYG+lqv3+8ToNxuxYnafeuFI2nHfIJ6PTx/8XgMdFShMhrGo396A1/96T9UJUnKKK0mUnfQVfM7LStBuuNS3skOxd8+QbsPc2wcrDG2ZpXdEPc8byE7+3KVhsaWqNQUmYvKatTrPw3gqwC+BuDrAP7EGOtnjHWYlDFjaIXTVsNs4Y+tATR1GQBzbqM+QTCcQZR+2NetnigLaSy9k/S2Wj7u3OgFgTFaA5hY63bd6omaafrbq/C5GxaYFksZ4jcU9Fly6fHaVk3JR0qWrVUrhtPr1qzCE7xCidErOX2WelOk+7lJykMrndxYYKNiGBT+kqkKtz+Od6X2rQ12xmcmW+pKuTtQeVCW9PNmItmZXZenllprDUp6R69iqTK4/9JpraZcoYdZre52IdoTB+YsgONMtlOGrroc78HvE1SVROWlttoNIV1xSynGksM893Cq/YpP6iR/y8uyotR402sjrxwpDdXyuqN8hHu3zsXOzTPT5NO8gtsCqPg77TynC6ialV0QZC6TTllVpEzrU6460oZ3TMW7B2iyZPraKlPKnxbJbTmSlm7pOnHAeA2gmb2nJ7QbR9pU9UyR1Hcp5SUhNFaXIBIKyCx6yqQpC6CknKUuoMtntrsyZ56sV8afvTyFrgJos65eOGEtlrQuQFCy76A0z67ydtN5GslkdD4pi51n09sGoh7AkwAuR3w7iM0Afg/gYwDUHdA9wLT+Bu4Q72MWQHlF+uS1c3DXlTPT0ifTWXGXUnbcegMmrZkbKdLBtKDjziIPvW7vI9DadDieuf61AuKz50YyWFlk7BMcHLDbxKyLoB6p5/CYYiojbczutAnQXn5qn5FZi4IdCYyU9gVD8W0YuhLrTywsAQQAXHX2AG7dOBKPOGh0icpN1N5bv4arlxo8g3SjV5lmTdbKWyUfoya5tT6Kz9+4UD+RBKP9DvUMplrWVGW/cfOGYXRzBL3RyyPtPE9lFQTVCLtODugFxF3oAGDLqkHcsmE4pTxIvz9DFzWzIulOUspPS/tG6RYfdmiuLcW150zEtL563XTFkWBK6ewaV5aSL0lvqyLaoonIiHqM2nABdQLDNoDjxpcs68MyjqjZST561SxZ9FMlyX1TGzUmfNQkWjK1BQ/sWIiVsztwzvxO3HH5LNl5o+i+P3/6Nclf+u9s/anpax0jBjEigHhZG1qidYo7WT2k9UQZr8Bqf++EVV+Zg18nCuji1vma53iYUjcRZ3Uv45aFC5vf2M65t+PueXfoToRURfQnD/RG2x8D8B5RFNeJovheURTPAfAjAKOiKL5lReBMIAgCmji3BEhWXuXLixaHdAfyVt6bGXcTn0/QVbYWTh6HS5bJ/XxldcCl1vvNPYc1zxneUZFAuu+T1poCfuQWQKuWPLXG7OJl6ZvF6rFsJn/HxIuTb3PBZO1w607gdM2z7GJi4ZqLVTYGtiUEJ1oue9sUa4+TqIkTDvpTQQy4rGsK1AbiWoPOeZMaURIJ4CrZoIrHdVV9kkpTxtSfStckhQXQBQu3VpAXLRmksmhZAJUdtaryZ/AsTmyzIgC47IwJ6ccdrOeCAJw+vQ33b5+PKT21suiu0vfF8+qcjAKavLnanodm9srVev+RsB9D42tUotCmP0PA78P92+fjvevTJ6yLwwHZdi/K220/b5LqpuNGRSU9r1uX3FIAHch4zsRGrF7QZeqaHp0thbadO4S7rpypa/FXwycICPh9WD6zHXWKaye0V+ET18xJu0atyK3oaDz77/kEQdUSKavHMkHU02mtAXQbs0qiMghMktPbF6Ov2pl9RqUIgqBrdTTCzIbtaoT8IRQFIhB0ZLh52lbdPPQWC7SIovhTxbFa5MA+gEazpIOd1SgKB8aCDJjsXyytAVRco5dFcSSIvQe1w0ZvUIl+pfVRywLCGEppDrnOaWTOlrNwSjMWTmnGc6/uRZvN2VdBaQG0+KRqj8Ba9C0BSooN1t8Yce/WuQAE2fqGpFxW1pUA8na9pdblDeQ5ZhRNZeeCBqhVjNLNljmzMkRv8CrdkF3N3KXtnmpeIqM1J2rtjdZtasqL8OlEUIm1i8bj4Z/+A8OsznANoFmxU65AaeuhlCljGbf6aw2eR2MxzTWAToio9CRR1i+u4AqC+sbdTu3hmbgLgPTtBZTwrAF0EukSObORb7mSazyOVr+gWT7KPk3xbgY6qrH9vCHVtZyJyw2P+3yCZkx3rrrgwifn1les1z4EAz5HPXeSaEWYNo1C9CUjLThrbgfXpWoeKFrjtnQX0Pi/0rqgnNyy0qd3qURbtURa26f+LTkzaZaehwABO+fejqMnj8Iv+PHsO8/hn/tfMZWrndNJfDp2vCKVzeTl12qT1kOIongxvL4NBIxf+OnTWwFAogDyDaxT6wcsyHRAsQ+QXh6N1eaDTcg7C+lvNzVA+xn2tFSkdYKhoLlZFZ8gKBo1a7I4UTxTDDaANaI4EkRxJKDY7DtZUW1lHc/J4YGyciCpq3xnwTXXjPVAM6UNubVuf+FSJou2y7/ugUMcTnmlqab21aGzqdxwOwUlp05twQM3LdR0n5KLpW5BmTepKeUGJ08f/1drbYoW77lgiqEsbjE6GuNaA7hpOX+kNimGyw9UTl+ksGzzKqh2tgbRe0UyCyDPGkBTN9Y/XZTYwqYkEjC9lEPTciJBy2pRXhrG9nMnpa3707yXxKtl8UizannyeDop279eieVI7Rm2nzsJk7qqU1t2OI1R03TKcDPX5uxmydQWT0ZYmZxWtneTx9foBnpKei9UlIZMW8+l7b/qGkClBdBC35i27l2B1TelZQHUY9uUzWiNWtuGBBBQFIigIlyOaKgUN4xssZiPBpJXN1Q7gNXjV6oms6Pg6pXY84yx5dIDjLEVAMyHuskwRgHtxsqLf9Alz8DsBUBdpfEMU3NtKe7bNs/SvnDyj1R9JiSbTaCrM/SCYibLoqKkKqNJud3oaFIRva0+GEfQE6sM99TijMRaH738rbr5Wq03Wu7daemkX4UkcY/E8mvLAsh7PPWOjS821P/UjhlcVFYcwievXyDbMoO36HmiFgPas8XrT+3BLRvSo9ilMIiHEYNc1h4dq/0pU9x1gR6NxeDXKDjpYS3PAiO3JyOlTO2sdEuBcTUl2tYOhdxm3d91spIhXwNonJGZZk/ttkPdNViYcH1fvaALswcbcOmKCaYHTnbbzoHOatSU64wDFJO4ScV9/tA41efSHSdoCGs0thjorMZ1ayZpjiFktzBMYZ6KkhA+eOl0x/PNlv4X1thSS4pZF1CjenjLhmFsXMow2FmNaLG+FVI6pojFYqrtv9P7AMZdsXVTJAXSzSdtDaDONhBadFd0YH3fGtPXAU5YFvlefEt0HC4b3IiFLekuxYC9sbVea3A9gPcxxr7DGLuHMfa/AG4FsN3y3TIE74vx8dWzNMzO4syYUI/2BsUMt4qMAb96dDazlEtcD2Rug1mwwFjC9PswdnHjykelfDxRYpzKjBR5dC+pW6HA7T7Cg88n4GxJuHyt8lqgMYhxCyv3klrkZPsdaWR2/qLxHGWp8dasBfmzkDh5ifQD4atJZto5vskJ9fy0Jk2SgUp6FRvEp72PGH8nqBZQwUliMe3n0XPp48VogkktX+mrOXOOdn1VZt1Qbd0So1t3ZGsAXVjAqWDtou7UsomykhA2LZ+AmooiWxZAHROgIwyzOkztrcN/vueUuEUsV/puAwzrvcbpBpPr8/o74orMBUt6+O6rK5P1a5dMVViXLLXdSnH0M6kqi8T7W0FQXSvJO1ZKfsNSg4rSAmilZMxMYA7WTEB9sXpkWGXbobUez260TcAdA8aoQWORlKs0qN8O21uHqIEoirtEUZwB4C4AvwVwpyiKM0RR3G35bhnCcD1a8nxyYJ2FOPlOVyel0rtgqAnF4UDK5cWNe5rBzPdj9m0ot4Gw2gt7tY9NimWmmt6xaVrq98rZY4M+X1pZ8WPHJczvE1QrYLfO4nzA/jtRjQKqUY6V0TA2Le/DHZumcX0rS6a2YOXsDlx9tnaEOS3LGEdgS80P1rBDUzNkS1r6mF5Cg3y04Fmf6hPiFmNA/mhaE3ZLp7fi+vOGsGqefBCjtmG5mWpiZjsIs4zGtNcjcs3FGQWBsWABlGap906VctsqJc6LeawfdtqdOy+fgTqN/TuNvHLOmd8piyjO8z3YGUkkv+u2hmhaFEnz62e9ifHgXz2F1sbyWly3eiLuvmoWFg3HFTBn17eawfi+hpMgNkQvLQoimhYsiE8DHBsmj6VXuo+74tklyfLKiRfhkv51Ggn5FECjArRaN9zYBkVKY0k8knB9sb1lRXoYqo6iKD4hiuLDoig+4ZoUWcLswDpmZxEgT/42rlXW4Y1Le/HprXNlA4ZsKjhmbm1FH5fNajmoz3uhI92fWD9qZh896abl0kXXZuvAZSvGogXqDT6TbmZaLid+/1hzKc1l62r1SJdJkmm7mspw+8VTU8dXzGqT/Z12XdK6r/JV6XW4swcb0VxbqrleTY1hpr131eJhjfUFykqqNhml5QJq4h1uO3cSBjqqMGW8e50IwLsNhCBRPo3T+30+9HdUpc86q11rokw+cMk0nD23Q/vd2CA2GtNUWKRKsmULoAULirRORULaHibSrEMBezHq9K6VeTlzVBytNYtGeQPQjey4ZkG3bl7LZ7bLopdKB3xuDv7UAsZo3U3qfg8gbf2cXvHa7SYtlYBR9VU5bxRISI2A34cq6f5/dgyA1i9Nt96pJTLU/+RXmXZdVv4tGytpu2Mn7+P0GNL0ZAbHhBqgvw2Ebv4W37CaXHfNeb/hdVdOvAhLWhcYWvZWj1+JNT1n4ozOpbrpToxaD8uSuZiuHsRsRZRGEDN3I85jHC3yCFMfyKm6L3rKpGVKBXQ1dy2yN0uoz4qZ7aiMhlVDt/Mg3yLD3DNOlWzMq6cA3n7xNGxa3odOlWAeQHwwr7avYXHEwOVZ4v7aWh9NWW9Om9ZqsHcX33PyFIedWrFK4h4LxKNmAsDELutBFszIM9gZjxYYUlmLYjgbb6Ku8FgAeTzoeFB22LFYzFQn3lhdgjNmd7jj1hMDosVBTJ+Qvg+cNCS71VsbBf7QcgH9wCXTsHJ2O/p0NpWWXrl0equ9QbPOA1aUjk0SGVoABSDoksXWsO1RkcUIt5yJtMrz7Hmd2LpmbBJt+3nGQZwuOr0Xk7qqUetC5EsjrLQ50nVpVrFjRbaDKwqTTQWK1/kgeZlPZ/xgzXtO4BrhjY23+R7YShAYwPo4We2qaMjYUj1YM0F3T8EkRYEIFjTPRiSgvyVRdaQS0xqmYNPAesM8lRi2gIyxEVEUnzKds4cZG4eqzLqbuN6WDBavu/JMdXez3tZKFIcDOFNnTVI2FUJTLqCWLIAuPZsHdMK2hig+dvVsR/IyP/s29jug04lWl0cwe7ARb+45pJ6Pzj2uPLMfn/3OM+r5JmZxk7O5H908C7v3HVENY6+KWl0y7WNsMr30UkWBnzq1BUtGmjU7Zb6ldJmplOas9jwKoOCIu72qi6sHvlMgruQJgoArVvbjt397M+1cEiviLp/ZltpcXQvVecVYDC11pWgxcKNT1km3JsSWjLTgG798MS4bR3q/UVQ3T+GOBqj/KsbuabR/JRCPujtvUlN2ImNaqFOXrrAWMVdKwO/DzeuHURE1vzWDk5+BWl7ml7yYS5/2nqUWQN37GN/IkvpnmG2qNzR1dy0LoNH9eNbQqSuhzlSMG0e24KNP3ZueO+eLFgQBF05Ya+nePC3rDYyx3zDGtjDGzG2K5lGSLzPlJuZwm10SCWCNwUalTgcBKI4EcO+2eVgyoh1i1251nTPYaDMH95AWp5VtNJR5pI55QQM0ySXL5B2m9Ll8PuMn6pOFCh9LbXcWVavKT+tLt5YkOXdhF1bObseGU+OL+ctLwzK3LLP3AuTdRtJldWqvthun0zVA3VrPf70yEhtP/lK42zsTMo2OGqeRVx/rpeoVBbClrjQtuq2eJVS6HlTrHXXq7JF1ypRmw6iC0nzbGuIW8lqOCNTxa8cif87sb+C6BjDfL4SC/pRrJs+EgJk1m00W234epK9sco+6Bd/WWELnMc1W74x8DxZuYmwBTD+m57Zshu7mcv0orB4lfbJw7O+b1w9j+3n6yyiURIs0LPCKysvVzlqZrDd5XjkRlVT0lLe2ugbQahAYp4wO7WWt6vfMwNiTZw3gWgCnI17e/x9j7MuMsQVuC+YmqTVAguJvA3pb4wPj/o5q3XQXL+vT7cgN5fIoFy/rxedumJ/6263qmXSTM4P0Y9myaqKl+7ppIZ07sVF3zZpTzJvUiDkT5QMy2doVjmeUhdGXJK+UrqkwiSDw15drVg2mfhdHgjhrbqdhOGstjL6oYMCHL9y0AJvPklvWZcVkol4sn9lma8pHp4MAACAASURBVKsBnhagc1yZTEm3fBeDxzLzNfC5gDrzfSktWbFYdiZqbr94KrafK3e50ysG+RpA9TTFkSA2nqYeqZRn/kWa741rJ2PHusnoajKeMInnL2DuxCY8uGMh6quKud7X5rMG0NGo54qtda/4vzwKk5k1gBWlYce8JfSoryzGAzsW4tpzJqK51vl965RY/XbcHFFYkcjCElZP8eHLZ5hKn/7ezJsAlVlI/+5uLseAwXhUmv6i03tV91wFrE1eWKpfpl+y/IKgT31CwO6WUaavs3QVP5nQB3hb1noArQBqAOwGsJox9iXXpMoQYy6gfOlPndqCm9cP48w57Qb5yiuVUxUlaQWxjIogcwYbsWPdZL7LBYFrfyD1a/nTLrIQnEGaP48bjGoeBvnaYWJXtcGaNafQty7xPE5ynVFPc7ks/VVnaUe7TMGzv48OO9ZNxuQe+wFLUndS+biVe7AZuZiZqQKTumosbWScai+ki/I10voEARuXam9nwFtnjT1x+J9casnRcjUUhLHHs/NdnXfKeGxcyjDUHbfCxJCdyTO1gYOuBVC2BtBCAWis75MlkfwujgTAWs1PFKT20OToGPUs5zwY3UMQBEzq1h/gKtOXlfAHy1LD7xNUJ+uUHhA+QcDQ+BpskUxYuWQANE3SahLj2aDTS3jFl1sDs9tRaD3N7IExC7tR25WmQtooonmTmmRtj9r3t27xeEzrq1P1+FFbf20W430Ak3lL0kuoiMT7b7Vy6yrvwIJm+QSQscXRaoF6u67yYKgAMsZ+C+B+AH8GMEMUxetEUdwCwN2wcja5deMItp+rbhpPvvD2xMxlciBhhM8noLu53PSaBLW6nhrvmegyKqPWLTCAekU/a26HpUGCE/d2NH8nsndVxMw0FqrlIFUABcGwrFhLBXasm4zr1kySNb48QQOSyvesAbkLGc99k+kcQSObm9cP60YGNJGVjGSkypIie+5KvK2BnfVZI4lB+6lT1V1Pxu7Bn6d0nJncwFqJPIqidYrCASwYGodQULLXpUfGuaMqA+5IIophZenYxBSvi7IUt78fvWuN1g+aJaWgGKQTAJxmUE9l6QX7fc28oSbZZN0dl0zD1WcP8k1+ZqEeqtX95FKRky4qgNbmMIzc8fILLevdqvljS4QM2y4dF1AuGXTOxVR+Lx5pwZVnDqi/K8UhtfbOjjzx88rnHeMT8z+EsE/bG2j78Gas6TnTnDxWl2O5PFmRie3peEYr60VR/IfyoCiKp7kgj2N0NpXhnf1HdNNMn1CPitKwLEx+plAfp+tXqGzsV+gYLrfsTigObrqAupV1KOjDseNjC7CM6hXPOj5BgGxSYGZ/A9ob+KyXoaAfD9y0UDfAgABBc+LD6XJS3qUobMGCzSHThy+bgZf+fQCN1SV49uU95m+RmhDiQ6/zMWpHBjqqce/WeaajIOohbZs6GsvwiWvm4DfP/BsP/+z5MbkUYn3kypk4foJj8aAGK2d34IXX9uNiDYUzG6hZAG+7aCreOXBUtpeZ7hvSaOfVrklLaeP70Wsabr94KjZ95OfWM1dgZv29zyegMhrGngNHjfNN/cc5mutK9fehc7jRMtvPq6VOtvM8rtmZxHDwn+8aYALpt2bUDitzcNLbQWoh1qt392yZDb/fh7+++Lb8eksB+4wSpHJP/Dk20Rf0BzFQ04uXD7yK3kq+pULGG8FbDTLlXGVtK2vBniN7sf/YgdSxTHi1aNY8xtgTSLwBxlLuRgKAmCiKs1yXzAG0Zr+SBesTBJtrabQxdrtLP7rJINqV3ck8xxtXE/m53a478WzqLqDe7pFuXj+Mj3zlaRw+eiJ+wCDACI/xWvnMZrefUFP+IiE/nwXDZk3pba1AMODH4WPx8kjbK4hzPZHZrTOqyyOoLo8kL+YT1gC9ztVuBD8e5c9M3Q8oyrWsJJQevECQRAEV+CzKejTVlOCjV8W7on++sd9WXk6hNiNeW1mUZnXWK1ttCyDH5I1hCp1rFfnLN653th3kdjM1e1shu2G73Bq0mR1o+zNgAbSEwcvxen/rFNLnPHVqC/a9eww//8NrGomduafaPsKHj43tIRfR2W+xvFR9WY21CQaTFkxF8qXti9Bf3YuWKOdae4PbWd8IXp0542bgsdd+YyqvG4e34K1Du3DHb+9OHcvE3I3eKMBaXFEPwR0q3gUMZx0Up1fMakNzrb6bjV0LoOoMcob6B7fbdSe6fTdltBIUiIfW+iguPr0Xn/n2XwEYN62ZDv19xyXT8PeX96C1Psr3jmyKd9O6KQCAO7/0e9XzvJFMpakysYeUYtLTEF2RsjCGmtlfD/GVvVg8Ilm/q+L+ZHZvp1xD1R3PZNRXrTbZdRdqxaW8yszUvnr8vx89Z+lWTvc/2ahVsnu61J9GQn6UFgUxrLEPsJJkO2/FRc9NcvGrt+VWrZnn2O9IKIANpzJNBdCBkQ0AYFJX+lraQ0dOpH6fu7DbMKdkfIDkOnorCqD54pRf4BN8aCvTjnZvPneLCqDGgyxrX2JaARQEQaVgsusCukQUxQcYY3eqSHKzizI5hpMuTmZIf23ONHu23TnUgghkbNGCtTKorYhgoLNae3YsmbsTFkCXNMDP37ggzULiJIbrKqQWQB4rgoPFIHOfSrk5atc5x11AFd8MtzInSWYmCqHiUhMXJSwinN+joOtia49JXdX40wtvm5oZDQb8aZZi5dWy/Bx/z87mZxVpG33Owm4cP3ZCNZ2VQQfPFY7OVXCWaWlRPHLpF38ocmfNawE0q8DEx1H8hfDprXNT76KsJISDh4/rWkKMcCsIjM8n4JPXzuF+tkxYAJ0KZGTidM6htQbQTNm5aRU9fjLugl9XUaRp5ZPSWh/FHZdMQ11iaxlLLqDg6+fGJgvdxboFUP26aMhaVGBlfiG/tcjnZtAb2bya+PdZAKLi/4QRKnVDLYqi0UBAGrLXtv5n73LX81PjI1fOwjSOSHNp7ksW7qXuAgqMsxnm203lD5C7daptFG16DaATQunkq1fnnbIMKXMpSUwGWZkU8qYF0D2Zrl09EQ/ctND2YCzdBdRefrmAVAG8aEU/zprbqZpO3wKotT527KJwQklR7gmZLRe6eZOacOWZ/dzpede86imAm5anL5kwuydhSSSYahOuWTWIeZOasHxGm6k8pCXu5kSE5rvVCQLjpgWws6kM/R1VuGKlifdu83y+YOYzdfOTXjGzDf3tlbJItkY015UilNiP1FL9MnIDdrgWGHvjOXs/n+DD+Ar1dl9XDomclw1uRDTkbOAtNTRHpqIo/jDx88sAngPwTwAvATjmulQ5TnxfqnRGeuswoz++6TVvnZsxQRIu2AUNsDjsnpusNBKkU9/YvVvnOpORGhoyXmOicZQS8Au4ZcOwDYH4CEk2h1bbAkNmeOF6Ee70OJ2J/cj09s1yrC1WZHTXlTPxwUunW9pUOBMKoNki11UAHVDcfD7nnTQFQUB9ZXwtnNMbdjvpyaDcE9KI/o6xzeCLw3z1y8oaQCnvvWAKZvbXY+HkcdigsW+gWZQSmbF8+3wCppjYviX5+EZeLXoWLLVvYKCTf8sIJfVVxbjo9F4U21o6YqMeOjgQzUQU0IDfh+vPG8L0CfXc12Rv+jE7aG3ibqZ1FQBM67O33QqgXjOryiK4fu1k/SBHOhh9v+qBRPmePVPeaVry9FTqu8Q6vy567PdQrbk+yCo8vdW3AAQBjAPgB/A6gK+6KVRek4qBYL7y2NX/eiWRHXtaKrDh1B5X3GR7WyuwekE3OpvK8Ou//tvRvO11zvqoWgABlBZZM8VfsrwPXeP4NmG2Q19bJRYNN2N6n3FHnGkXUCnrT+3BQGcVBACf/+7fVNM4rWwlv5mSSNDUmmCpFH5/5gYlMZ2/pJjciSYrpLk/ATh7XgdqKiKYPWDOUuM2V501kFpHa3Zvu+vPG8Jbew/jF394DYtH+Nam6H5jGq9d+u221kdx2Rlxy8vCyePg9wl46JFn07ZfsUNlNIw1C7q42zBzVg0+E6CuAuMVXSHL7sz9HVXoa6vEKVPG1t/6BfctgJYwsv545Z06hNbjJLewqSjlGVsIuPyMfjz597cck8spjOrXeaeMx8M/lW8i4LV3LAjqnemlA+tx06O3a1+Xkz51cnhG/zWiKM5kjD0A4BoAP3ZZprzAsNm18K7trgFsqSvFPVtm45mX3sFQd41rytSVZw6grETesHnto//Y1bNx/X2Py46p73tjXvDkGqoWg6A+TuETBFywpEfzvOy5OB7HrXdVFA5gZn8DnnpWuyNzKkiNk4/g9n4/ANBWH8VfX3wHbfVRvPi6cURLO9tAZAqlFIIQD3iwhFNJMoWNprEo7JdZ8axQV1GEc08xDqKQRNcFVPMi7WvmTWrClJ5a1Uh/djjdhDukWasGYOzVcvKktW1Crj1noqo3hJex+tUGAz7ceP5k2bFMWACtYDY4Xs6T1gjG/wn4ffj4ltko4vAYEAR5v2h1GOhG0RrJElSZPBU4rsskWnWyJKjvpeK4BdDR3PjgUQAPJf4tEUXxMGPMQ6/Oq2gXkZ3Cc+KjKS8NY5bLs+92zP5J/D7B1c5L1VVSM7U5Oa5eNYg9B47aDnPvFNLnUr6bOy+fgWDAhxs+82tJenebIr120ykLYFNNCcRX96LRqquhREjTDb2FR1g5ux1NNSWYMr4WP39aP+ARoK8oe2YQpbINhFfJdHRc3bLQaOiNRLSr/Nleq2zBAiht4t934Qj2HTyGT33jz6ljVvuAofE1lq6zgrS99MrgaEZ/A57+x27MneQtS7tRE6Dse8I2AvI4hZ3vSq8vreAIuuIkbtRNI6OE3+9DU00JXt/97thBw0BAnAuEOTEOkic/v6p7BabUTdRMf0rLXDy/90VHZJNSHi5DyBfEzKapjuetBY8C+E3G2PsB/Ikx9hsAB12WyXWyOfuQnPG0MtzIlY3g1S1p5vK4d9u81CbRWo+9c/NMZ9+lhthmbxHw+zyj/AGQPZcAQfZ+lPuTAZnwvdeuDE5Z29Ys7MK42hLM7HfOJc5NggG/KVkzYZW0q0mqWQDdwm6NzUh5SrDgAWpwlXU+eOl0vL3vCJc1Qg8z0qXGeJIGvKMxfascr1mwDPGIuFN768CunYOyYvcjCTqKohJtXa09EM8UTTUluOyMCehyYCsnK1+w/WBc9q7Xg8crrbGqWKYAem0a0KeQqDU6DpWRCs3054w/wxU5Ar4APj7/PzI6UWrY4ouieF/yN2Ps/wD8Qye5JoyxIgBfAlAH4ACAC0VR3KVIcxuA5QBOANgqiuKTjLEhAJ8GcBLAUQAbRVF804oMmUL5TSycrLJhpYWX7JG+xRAH9D+Eg36EE8FNtJ67plyuZE3qqkZjjfWInVqKa47o3ZroWQCzgZ4MTlliIqGAbE2MWTxQTLp44T0aopDRVSXL5jea6TWVep28Vjm5VXz1lUUYZ6PdTOJGaPvkGiMvT35KH8UJKZ16Ui8qf4bWmMS/5y7sxi+efi0VOCzbWJ1IdGR7Kg/3Rkafpdp3y1smYxPRbnskead8My2LpgLIGPsvaLdFl1i412YAfxFF8XbG2FoAtwK4TnK/KQDmA5gOoAXANwBMBfBJANeIovhHxtgVAHYA2G7h/lmju3msEUt+D2Ze89pF8YW0gzYinLmBVmVVa7AyUbGvWzPJ1vXeaQYcRlL2PINwt8damXABtYsX+gS996D3PSXdm5N7NXkFL5SpFpm2AOoxZ2Ij/vLPt7FiZrvsuFsiZuPRk7c0MvCdyDELoB1F1UNVMGsky2Dp9FYsnd6aXWEcIN0LwvxL9nK94AkypEzBOwmQKdxQsL2stEvRswA+nPh3M4BfA3gccYVsmsV7zQGwM/H7EQDvUzn/I1EUYwBeYYwFGGO1ANaKoviGRN4jFu+fwgsflBkZTp3agiUjzZ6aqdDDbTE3LmXc4dbNoGW5DAZyIOSiHiYHJW4/r+4+gDlSx/XI9hNUlIbxH5dOz3oQDCcGP5nCS7IVhQPYfu5Q2nG3ZMzGYCX1KAZt02nTXAgY5HU8bO10Gy99h47gwPM4VSRuVKthVosf/e5VzfNqt8y8u725NYCFhOYoOrkPIGPselEUk4rb44wxwyigjLFNALYpDr8JYF/i9wEAStt+GYC3JX8fAFAuiuLziTxnAdgCYJ7R/Wtro7rnKyqKDdOY5ZaLp+FD//UkAKCsrAgVZRFVeUIJxcXv98WPJxaZFReHVWUqKVE/nkm07l9aGlE9V1sbTVtPYucZ3tg3pvPX1kaxZkmv4TVFBhFO1eQpUtnuobq6FOWlYdyzdT7+3yN/xx9E/VDMPM+Z6fdZWTnm3lVdXYLSXWP++EpZbr5oGhob3HW7KX9L+/51tVFUmFRc3CjPeVNaUmG3zeYfjap/+2YpLy/Svf7eGxeitCiI6vJ0S5/V+0qvWzC1FQ//9B+4YGkvvvyDZ03nG43K5aquLkWtyppTJ9h1cGx7Wl4Zi4rjbYQgCLJrnKxPWnlZuUddbdSVwUptbdSy67XV51t/+gTc+rlf44JlEzTTfvMjZ6Qmo3wqPrplZc7VezsIwbG+rre9yrIMocQepcFQIGPPkeny8kn2YdUaO9gJRpTNsZLavetrSg3T8OQbDIwFwzE7fk3u5xmJOF+vamuj+GpvA86/9fuq56OlEYQVY8HKqmLVsVaSpLzhhLzCu8dl9zNLaanxGHrt4Eo8/Jf/BQCUq5Sv2fsGdYIXZXs8L4XHjFLKGDsFwO8AzAIQMUgPURQfBPCg9Bhj7JsAkk8eBbBXcdl+yXlZGsbYeQBuAbBcuW5QjV27Duie37PnEHZx7b/Cz7EjY5V0//7D8I2Oha+WynM0ke7kydH48cQUyaFDR1XlPnjwiOHzuEltbVTz/lqyvf32wdT6vSR2nmHv3sOm8zkseR9qqOVzVOWad955F8cOH0N5xI8tZw/g5X8fwAce+p2pfKXoladbvPPOmMK1Z88hvPvu0dTfSln8sVHX5du/X/t97tnzLo4fOaa8RBO3ynNCSzk+unkWqsrCpvM/cGBswsKObO8eVG8TkhT7BYweO+HY8yvLMgTg8zcuQMDvSymAZu518KDcWWPPO+/Cd/KkI7IqCSUa0t7WCv424lD8e4/FYrJr3CpPKVbusXu3O/HXdu8+YFmxtPp8TZURPLhjIQRB0Ey7d89YuzU6mr4dxAFJO8J7XzfYc2CsPV09r9OyDMeOnQAAHHfwm9YjG32RtKzU7r179wH4bSzIzdZYSass+1vlk6lWv3upUrx37yHsKuL3gEpupXLkiDv16oTOVi0HDhzB0aMnZMf27T2Mw4e1+/ikW+nRhLyxWADDdZMwUNNnSX6jfhQA5tbOwcOIK4B79x7CLkGe3ux9jx/T7ucyXUf1FE6eWnQJgI8C6AHwDIALLcrxOIBlAJ4EcDqAR1XO72SM3Q2gGYBPFMXdjLH1AK4AsEAUxXcs3tt1pN2nnqU9taw1j83Oji/jypRLDMc78coaNTNIS88L1U43CIwXBExQXW441+UK77lgCp545t/oa6vMyv2l2JmNz2QU0PLSMO65Zg5KTQyMiDjZ6ovysQ+0G0m10LHqjtxaV4rls9qdFcYBbG+t4iiZdy2e3FOLv/5TPmwXOAPrJYPACIKASwYusC5EVhY550bbphcEZqsoip8QRfFZAGlxT5PnTdzrfgD/zRh7DMAxAOsS+ewE8PVExM9HATwBwAfgasaYH8CnALyC+HYUAPBLURRvM3HfjMD9vi0Egck1nO7YM9Vs8YidI9+1jFgshml9dXjy72+hpjzigcqnLUAuKthO09NSgZ4W7TDUOYPiVbo94C8vMefVsXR6K559ZQ/WLR7vkkSEU6xdNB6f/c4z2RaDcBuLTcTpM9owtbfOWVk8QrLZ7B5Xjudf24fqMnMTk26PWfTyV2uTjSd5aQyQKfSmq7YxxrS+KAHA+QC4FUBRFA8BWKNy/CbJ79sB3K5IUsV7j0xyz5bZ+OHvXsUPfvsKAAuDG6rj3CSDWVQ47LarhOeV5Oqs9RUr+3HpigmemJHU3wYic3K4Ra7WEadRzuZ7TbevjIbxgUusxjQjMsm0vnrPKoBOf+6FGwLG/LDolo3D+OUfX8cwq3VFnkyyYlYbvvfrl9OOJ9vRm9ZNxruHj6PM5ESX25i12ma6e/QhDwYVLqGnAL7f4FrPWeEySXlpGFUawSr0QkHnU+Ou9R077cbXWF2CG9cOYVxdqXFii3Q1lak2ZMp3matje0EQEPBnfwsIQL+Td2ofQCL7KL8VUowJQhv6PsyXQVdTObo8slegXfrbq9QVwESRBPw+lJdmN7KzE/C+Y7tjkW1TNuPHL/8CMxpHTF45duNTWubirUO77QniYfSigP53JgXJRfTWVsU0VL2kQmHSYzSncKMf62t3zxC87dxJ6Goqw7cf/aetfLw6C+m5qOI5sgaQcBZ6tUQ+4viSd4fz8xLUBmijua+yzUK7YAnDp7/xZ5w2zaV9Fc06vwna42ML2WnSXdGB7ooOW3mcMz5t9VteQSuWHcL84uX8bQlzbSZzsLM6/oNDbC0FZbCzGlefPeigVPlLvu8DmI8sGWlBabH+1ipKyALoDFes7MfufekRL/OV1vpSNFWXGCckiDzCreZxYlc1vnDTQncy5yCX9oN1ilx5QlIA7SAzAXJekgwCIySzyOf5Pm+zfGYb3tk/Fqpe1QVUeUDlPfe0VGDdEu8GkvBaHVNr/2vKI9i970j6iRykrDi+RiNqUmHyMudbCJSi/J4KoN93hekT6l3JtygcwGFFiHYvcPvFnOsyvVKfqGJz4zlvFA9hNQJqtpFK/YFLpuG2/3xSNz3/Kg+qLG5DCqBD2Lf/5ebH71V4SvOc+V3ya3iigKoce88FU7hkyhZe63R7Wiow0FGFeZOaUsc+fPkM3f2EcomJ3dU4f/F4DHXXZFsUT5GrA5x85Z4ts3HsRH58cwSR6+TqPILUotfCEafB2AKYowWRg5ACaAPZuFpSZ7024M4m92yZjcM6m2J6CdV2KaZMk/uNk95APBPWwoDfh+3nDaUd80KEUifwCQKWjLRkW4zsk+YCmh0xCHVCQT9CQX+2xch5HK/WNH4oSAolAJogQFbHW+tK8cpbB8fOJ/71mudSPpIfIy4PwBu8IhVVMg++dZ4Gq7w0jIaq4gxII8dK06Gm3CnzyaXXds2qQUyfUI+2hqhh2s1nDWCouwYdDWUZkIwoBJTfitcD/LQ1RE3vsUVkj6HuGrQ1RHHq1PyYbOkaF297O5sKrw3uaCy8Z1YSCRXGZIxynHXWvE7UVRSl/s62IlxIBhyyANpBUlOkg5sYYpqVKHmY2x3Kg5XxxrVD+PFT//L0xqs8Wx4oURufKvPJJQvg5J5aTO7hi0w6tbfO0++TyEFyzAL4/gtHvNjcEhpEQgHcdtFUPP/aPvzod69mTxCH6vUZszrQ3lCGwU5Pbn3sCOUlIdRXFaf1NbduHMZoIY28VSiJpK8Zv/PyGVmQxBm0PHqURwUAgUD86DCrxS53xcoIubLcoWAUwL62Svz95T2O5iltrkzPWuRG/VClr73K1W0ZnOCMWe14Z/9R/OE5/uZE+tE215ZgWl99WqNcUlQwnwxB2CI9CIy3Gz1BcL/bHuiowuFj3gu8QmSfYMCHKZwTdrmKzyeoKjWCIMDv8fbBbYoj6WOLYCB3nfRuWDukelzZD8QgGQ4X9hxAxsnd2mWSZTPbXM1fqgDyDCMKu6lzn2hxCFtWDaIyyr9xarJd8vsE3LFpOlbMak9LEwkFsPPKmVizoCvtXK7QXBsPsd7fXpllSYh8pq6ySPZ3gY/vAADbzxvCLRvMbkxMeBmq1oQTBPy+tD45l42iNeXq7vTxfQDTjwFJZZC+qExRMAqg2zMLUgOg3uJV3g+6KByfDYqEC8Mv3AukGiGDd1RTUZTTwRP6O6qwY91kXL2K9i0k3KOjsQw3SmaBSQEkCILQZtOKCdkWwXWUFkBB8t+YZPAVy2XtN0coHAXQBaT1k9cFNKkcJr+B9kSAjqoyuaXqlg3DWDq9FTP7G+wLSnCS3gjlI4IggLVWIhIid1bCXaSu4l53ASUIK1C9Jpwiz4ceADgmArP8ORVS9NGCGQG6/VJl/ut6t0qdi6e/dvUk/F58C3MnNsqSNdWU4NyF3Y7KSOgzFn6YIAinoWEyQRBEYaM2YcLrfUU4S8EogG7DbwGMk6zw5SUhnDKl2R2hCFOYmcjNdyshQTgNWUoIgiC0UY4r8tEapdwHEKDJwWxROApgDOhuLsfz/9qH2ooi4/QmMRsFlCq896ABKkEQhHXu2TIbfo3w7wRBmKO8hD+InddIjqeUKqxyP1jlsIuCwGSOvFcAt6waxC/++Bp62yrR21aJA4eOm4oMqYd0dka+D6COhSiPLUef3joXJ0/m7vNRs0MQBGGd8tLcHbBaheYNCTd4YMfCNGUpH1B9pJQLqCQITB5aP71G3iuAU3pqZXvrOKX8AUClpLMz7wKafx+22kam2caMq6YpF1ALshAEQRAEQRiRL8qf8inULHzJY/k6rgr6Ajg+6r39X8lXwwbT+upTv32KIDBaCl5KH8mPbzu/IA2QIAiCMAF15YRT5KOD2IpZ7bIN7eP7AOb3IsDk+L+jrBUfnv0+LOtYkmWJ1CEF0AZSq59PUZIUJCT3MNMG0dslCIIgCMIp8nHc2NYQxeduWKCbpqWuFADQWF0M5JE1MAagPBz17LrGvHcBzRRmzfXerA6FTZ54XBCEp7jjkmk4ePh4tsUgHGbxSGajVzdUF+Pfbx/K6D35oI6DIHhRG2edv2g8OpvKMLO/ATt/n3mZ3MarS75IAXQIqTVQb/FqPs7weBkzpe3Vj5QgcpnmxOwukT88uGNhxtvL+3cswpaP/gyv7XoXE9orM3pvgsgE+TQ61Goe0q1hAorCASwYNIz4JAAAGXZJREFUGic/nAdj5eSTzmqchmd2P4vTOxZnVR4lpAA6hOltIEjZ8BwlkfjnUFYSMk6cB40TQRCEFbLRfwX8PkRC/ozf1wjqygmnKE6MQcIerOdO4fMZKLp59EEln7M4WITrplyRVVnUIAXQIWTbQOjU7uS5/Kni+cPswUbs2nsEswcbsi0KQRAEocCra2kIwglKIkHcdtFUVJXl/nYqyi/15vXDeP3tdxEMyJXboF+e8pzuFbjvTw9iSdsCdwXUoJC2nyAFMMMUTtXKPQJ+H1Yv6OJKS++RIAgisxTS4IwoTNoaotkWwRW6m8vR3Vyedpy1yd25J1Qz/M9592PXrgOZEs01vN5ekQJok5s3DGPPgaOmr8sjKzdBEARBEARBcFNaFMyb/Q5zEVIAbdI9Ln1GAwCqyiIAgIaqYvkJWjtGEARBEKYJB+PuY6GAd9ZI0fiVIFTQ+zAKZBjsdZd1UgBdoraiCLdfPBW1FUWy4wVS772DSwVOejxBEERmuXBpL772s+exdlF36lh1YrJ1XG1JtsQiCCLH6anownN7X0BdUa1jeZILaAGSVA5a69N9uZPVgaKA5jZ9Cb/1RVMyuxcWQRBEoVJbUYQtqwZlxyqjYXz48hmojGYncIbXZ/kJgjBmy9Cl2H/sACojFbbzqiuuwd/fec5RZdINSAHMNN6eECA4aWuI4lPXzU1tHUEQBEFkh7SlFgRBeB4v2UH8Pr8jyh8ArOxciupIFWY2TnUkP7eg0WuW8FLFJ6xRWhTMtggEQRBENqG+nCBMke92kEgggkWt87IthiG+bAuQj+j5/XrdJzjfoNImCIIgCIIgiDFIAXSQgY4qAEBzbal2ItoIniAIgiDyAurLCUIFipTnecgF1EGuOWcQb+45rKsApj4J8gElCIIgCIIgChAaBWeXjCmAjLEiAF8CUAfgAIALRVHcpUhzG4DlAE4A2CqK4pOSc+sAXCOK4sxMyWyWYMCvb/3D2KQIVXyCIAiCyG1oLpcgzEGfjDfIpAvoZgB/EUVxLoAvArhVepIxNgXAfADTAawFcJ/k3GQAm5AH9WawM+4mOqG9KsuSEARBEARBEISz6DmArpjVjvHN5bh29aSMyUOkk0kX0DkAdiZ+PwLgfSrnfySKYgzAK4yxAGOsFsAogA8D2ArgC5kS1i3OmN2Owc5qdDSWZVsUgiAIgjDFzeuHsy2Cp6ClTgRhjspoGO+ldiTruKIAMsY2AdimOPwmgH2J3wcAlCvOlwF4W/L3AQBVAD4CYDuAw7z3r61N34DdSzTUKx/d23i9PPXwafjnZPOZcrk8vQiVp3NQWTpLPpZnZVVx1p7Li+V59PjJ1G8vyqdHrsnrZbTK8rzFPfjaT57TTZMvSJ+vuroUVWURR/Ii3MEVBVAUxQcBPCg9xhj7JoDkG40C2Ku4bL/kfDJNOYDxAO4HEAEwgTH2CVEUt+rdf9euA9aFJ2TU1kZzujxHR0dVj2frmXK9PL0GladzUFk6S76W5949h7CrOPN7oHq1PI+fGFMAvSifFl4tz1xEryxPG2lOKYD5Xt7S59u9+yBOHj1uKR+qm86hp0hn0gX0cQDLADwJ4HQAj6qc38kYuxtAMwBfIghMPwAwxtoBPGyk/BEEQRAE4Q7k8SiHXEAJgshFMqkA3g/gvxljjwE4BmAdADDGdgL4uiiKTzLGHgXwBOLBaa7OoGwEQRAEQRgQI41HBkUBJQgiF8mYAiiK4iEAa1SO3yT5fTuA2zWufwnADHekIwqBqrIw3tl/NNtiEARBEHlCMODHgqEmdDbl1tp+giAKG9oInigYAv5M7npCEASRf5ABMJ2NS3uzLQJBEIQpSAEk8pqY5h8EQRAEQRDusmCoCQePnMi2GAQhgxRAgiAIgiC4oDWABGGOQrQQUzvhfcgnjigYYmQCJAiCIAiCIAocUgCJgoEmpAiCIKwxe7ABANBUU5JlSQiCIAi7kAsokdeQ0kcQBGGfTcsnYONpvQgGaN6YIIgxNp7GEApSu5BrkAJIEARBEIQhpPwRBKFkweRx2RaBsAC15kTBQNZAgiAIgiAIotAhBZAoIEgDJAiCIAiCcJNQ0J9tEQgDyAWUIAiCIAiCIAhb3HXlTPz77UMoLQpmWxTCALIAEnnN5SsnZFsEgiAIgiCIvKeuoggTu6qzLQbBASmARF4z0DHWEJEDKEEQBEEQBFHokAJIFAwUBIYgCIIgCIIodEgBJAiCIAiCIAiCKBBIASQKhhiZAAmCIAiCIIgChxRAomCorSgCAHQ3l2dZEoIgCIIgCILIDrQNBFEwzOxvwPyhJkzsqsm2KARBEARBEASRFUgBJAoGQQBmDTRmWwyCIAiCIAiCyBrkAkoQBEEQBEEQBFEgkAJIFAwUAoYgCIIgCIIodEgBJAiCIAiCIAiCcImjR4/iu9/9Nlfa73//u3jssV+6Kg8pgETBIGRbAIIgCIIgCKLgeOedt7kVwGXLzsCcOfNdlYeCwBAFA7mAEgRBEARBFDb/87Pn8btn33I0z6m9dTj3lG7N81/84n/ipZf+iblzp2JkZBoOHz6M97znffjBD/4Pzz77N+zfvw/d3T24+ebb8OCDn0N1dTVaW9vx5S9/EcFgAK+//hoWLToVF164yRF5SQEkCIIgCIIgCIJwiY0bL8ELLzyP6dNn4sCBA9i69Qa8++5BRKNRfOITn8Ho6Cg2bDgXu3bJFdM333wDDz30VRw/fhxnnbWUFECCIAiCIAiCIAgznHtKt661zm1aW9sAAOFwBHv27MFtt92M4uJiHD58GCdOnJCl7ezsRiAQQCAQQDgccUwGUgAJgiAIgiAIgiBcQhB8iMVGAQA+XzwqxW9+8zjeeutN3HHHndizZw9+9aufIxaLKa5zRx5SAInCgRYBEgRBEARBEBmmsrISx4+fwNGjR1PH+vr68dBDD+Lqqy+DIAhoahqH3bt3ZUQeUgAJgiAIgiAIgiBcIhwO46GHviI7Vl1dgwce+GJa2okTh1K/p0wZSf3+3//9oWPy0DYQBEEQBEEQBEEQBQIpgARBEARBEARBEAUCKYBEwUBLAAmCIAiCIIhChxRAgiAIgiAIgiCIAiFjQWAYY0UAvgSgDsABABeKorhLkeY2AMsBnACwVRTFJxljdQC+AKASgB/ARlEUX8iU3ARBEARBEARBEPlCJi2AmwH8RRTFuQC+COBW6UnG2BQA8wFMB7AWwH2JUzsBfFkUxXmJa3ozJjFBEARBEARBEEQekUkFcA6AHyR+PwJgscr5H4miGBNF8RUAAcZYLYDZAJoZYz8BcAGAX2RIXiLfiNEqQIIgCIIgCMKbbNlyOV5++SV8//vfxWOP/TLt/MqVpzlyH1dcQBljmwBsUxx+E8C+xO8DAMoV58sAvC35O5mmHcAeURQXM8beD2AHgPfr3b+2NmpNcEKVfCnP0mjEE8/iBRnyCSpP56CydBYqT2eh8nQWKk/noLJ0lkIuz1AogMrKYlx44TrV8z6f4Ej5uKIAiqL4IIAHpccYY98EkJQ4CmCv4rL9kvPSNG8D+N/Ese8C+JDR/XftOmBeaEKV2tpo3pTnwYNHs/4s+VSeXoDK0zmoLJ2FytNZqDydhcrTOagsnSUT5fnN57+Hp9/6i6N5Tq4bxKruFZrnb775RqxZsxaTJw/j2Wf/hvvu+yQqKipx8OAB7N69C6tWnYuzz16NY8dOYM+eQ7jrrrtRXV2NM844Gzt3fgj//OeLGDeuGUeO8I9l9RTFTLqAPg5gWeL36QAeVTl/GmPMxxhrBeATRXE3gMck180D8EwmhCXyEHIBJQiCIAiCIDLMGWechUce+R4A4P/+77uYMmUEixefinvuuQ/33HMfvva1L6te96tf/RzHjh3D5z//EK64YguOHj3iiDwZiwIK4H4A/80YewzAMQDrAIAxthPA1xMRPx8F8ATiiunVieuuB/AAY2wz4i6k6jZRgiAIgiAIgiAIHVZ1r9C11rnB9Okz8ZnPfBL79+/Dn//8NO6++1P47GfvxS9/+XMUF5fgxIkTqte9+uor6OvrBwA0NDSgrq7eEXkypgCKongIwBqV4zdJft8O4HbF+ZcBLHFZPCKPWTLSgh8/9SpYa2W2RSEIgiAIgiAKDJ/Ph4ULF+Puu+/C3LkL8PDDX8LAwEScffZq/OEPT+GJJx5Tva69vRM//ekPAZyP3bt3YdeuXarpzJJJCyBBZIXzF4/H2fM6EAlRdScIgiAIgiAyz/LlK3HuuWfi4Ye/hTfeeB333LMTP/3pj1BaWgq/349jx46lXTN37nz87ne/xWWXXYiGhkZUVFQ4IosQy791UTFajOsctLjZWag8nYXK0zmoLJ2FytNZqDydhcrTOagsnYXK0zlqa6OC1rlMBoEhCIIgCIIgCIIgsggpgARBEARBEARBEAUCKYAEQRAEQRAEQRAFAimABEEQBEEQBEEQBQIpgARBEARBEARBEAUCKYAEQRAEQRAEQRAFAimABEEQBEEQBEEQBQIpgARBEARBEARBEAUCKYAEQRAEQRAEQRAFAimABEEQBEEQBEEQBYIQi8WyLQNBEARBEARBEASRAcgCSBAEQRAEQRAEUSCQAkgQBEEQBEEQBFEgkAJIEARBEARBEARRIJACSBAEQRAEQRAEUSCQAkgQBEEQBEEQBFEgkAJIEARBEARBEARRIJACSBAEQRAEUcAwxoRsy0AQROYgBZAgsgx1vIRXYYwFsi1DPsAYq8q2DAShBmPMxxjrFUWRNoUmiAIibzv3xKD6fAB/BbBXFMVXGGMCNXLWYIx9HMBvRFH8n2zLkuswxnwAbgDwFoC/iKL4+yyLlNMkyvNCAM8AeE0UxdeyLFJOk2g77xJFcYcoiicYY35RFE9mW65cJFE37wfwHQDfpz7IHony/CCA3wN4VBTFXVkWKadJfOtfB/AOgEsZYz5RFEezLFZOkijLjQD+DOBNURRfz7JIOU2iPM8D8A/Ex/AvUPvpLHlpAUxUnK8COB3AWgD3MMZmiKIYI2uLZeoB3MIYW5ttQXKZxADmiwA6AJQDeD9jrCK7UuUuie/5KwAWAbgAwJmKc4R5qgFsZox9DQCSyh+VpzkS3/r/IK6sPMIYCwGoSZyjsjRJosy+DOAEgP0AYsm2k8rTPIn6+SUAdQBKAICUP2skyvIrAE4DsAlxxSV5juqmSRLl+TCAhQAWA/g4Y2yExvDOkpcKIOKDQb8oihsAfBjAtwB8JFmBsita7sEY6wLgR3zm9SrG2Losi5TLnAYAoihuBvCfiA9mBIA6CossARAURXE9gB8DWMgYm8UYm0rfumUOIj6B5mOM/YQxNsgY66LyNM0GAH0Avo34YObTAL7FGFtEZWmJEQBHAHwMwHUAbgXwdcbYAipPS3wWwLOiKM4B8CZjrDHbAuUwywCERFFch/i3Pp0x1s8YG6K6aYnliPfrVyD+vT8D4KNUns6SrwrgLgBvMcaCoigeRHxm5iEAGxhjJTTQNoYxJjDG5iX+fBlxN5FvA7gDcSWQLIHWEAC8mnC1OQAgAiCYOFeWPbFyluMA/p74PQJgCMB8AF9ijHVmTaochTEWRHyyJyKK4hoAxwD8GkDD/9/e/cd6VddxHH8ShUiSOVy/RtgW+NasZqPlj6BhbdacASkYQvJDphEGkm3OZa0lhK02cpUZDI1FY8qUTGxtTYkWokxNzWy8cLQbjOGCBDG4inBvf3zOF798d+/l+71+5Nxz7+uxOeF74bv3Xvscznl/Pud8TvHzwSWWVzX3F/+tAzYA80kXM0t8sd0rL5MawBnAA6Tb6FcAP4yID5VZWNUUz/Y+JGlxsdoymtTEWO/sIzXRk0irfx8FvgKsjYgxpVZWTa+SVvmRdAR4EtgCTI2IU30Nn0e/aQAbGpatwEjgx3DstoZHgCHAa55BaMp5pNuWJhUH4DpJRyQ9AtwKfDcippRbYjU0jM1HgXskdUTE6cAoYF9EXEW6kBlSWqEV0ZDnY8BPi1/fDYSk20mr/m+UUV/V1Ocp6Q1JB4GtETGBdI54HFhW/NzPAvagYWy2k2auNwMPSjoq6XekC5lDZdVYJQ157gSGk5733VOcj9aSnrnysd6EIs9LiuwehmPXR4uB8Z40a17D2HwS+DNwMTAeuEjSEtJ5yNebTWjIcwswKiLujohvkyZ7/gp0SGr3NXwe/aYB5M2GZbKk14HpwKcjYllEfIR0YJ4LnFFijVUyEtgF/DIiZtWeDShWrv4CzAOeKrPACjlubEp6sZjBOgw8Txqr1wE/l3S4zEIron5y4rCk/wJI2gm8MyKuBsYBblaaU8vz8rrPzic9q3qbpEuB5yLirFKqq5ZalhOLfzPXkW713h8R74+IqaRsTymzyAo57lgHrietBE6OiPERMQP4DP14Q7vMzgMerh3rxUX3YOBfQBvwqWJF0E6scWzeD/wJ2FZsnnU16W6U9jKLrJD666R24HLSBO8e4FvAfuAT3jMhn/50oNcaljsjYm4xi/1F0i1280knjnmS9pZYY5UMAqYCE4ClEXENpNnCYiemTZLaSqyvSurH5kyAYgarExhLemj8Bkkqr8RKqZ+cmFn7MCKuI61UzQfmehe2ptXyXB4Rc4rPvgNcJWkzgKTrJf27rAIrpJblXRExp7h7QsANpLG5APi6pP+UWGOV1B/rcyUdAqaQbg+7gnS73QxJu0ussUrqj/VZkjqLlek9wDbgSjw50az6sTm7OKc/DrwvIu4AvgHM8q7UTWu8hj8k6R7Sv58XknZTvlXS/jKL7E/606xZrWE5BGyMiA5Jv46IBZKORsTw4pkr60KxIrWIdBLYLumPEfFBSbsj4grgvog4RdJKL7+3rHFsdkpaTdoAZguwVNK2MgusmO7yXEW6zftdPkm0pDHPo5J+A2wvVgM6fcw3rTHLI5JWR8QvgA5gmKQDpVZYLd2d128uzuvvLiZ7rTld5bkaQNKaiPhDsfpiJ9aY5aBibF5KaqIH+zzUksY8Dxdj82nS7d/3euIsr0GdndU8r3fRsGyta1guAO4Dbpe0vNRCK6DIcj2wnfQsxVDgH5J+VfdnxgHLgYuAV31B2L1Wxmb4vTYn5GM9rxPkeSFpF7slklaWWmgFeGzm1WSeSyWtKLXQimgyTx/rTfCxnpeP9fJVsgF0w5JXRIwCfiBpTkS8h/SMyjXAY5JWFc/9dUTEUEmvlVtt39bi2LyYNDb97qVu+FjPy3nm4yzzcp55Oc98nGVevbhOOuA886vqM4AfJu0CdiPptQRrgbERMRuObVSyCRgryQOnGxHxjuJh8CnAsGL25QDwLOmVD+dG2qmylt/rJZVaJa2MzVfc/J2Qj/W8nGc+zjIv55mX88zHWebV6nWS83wbVOoZwOJ5lMuAs3mzYdkdEc+Stof+XNGw1J6xcMPSjWIG5kFgBxDAF4DRkXav2xURG4BrgTMlvQLHNi6xLnhs5uU883Ke+TjLvJxnXs4zH2eZl/PsWypzC2jRsPye4xuWZ4Baw3Iq8FvgZknby6u0GiJiEfBJSddG2gb6J6RNSSaR3rM0BpgNfE3eYa1HHpt5Oc+8nGc+zjIv55mX88zHWeblPPueKq0A3gjslfTNhoZlQ0TUGpb34hfsNqsNGFEcdCOA8yV9vpiJmUh6QflCN39N8djMy3nm5TzzcZZ5Oc+8nGc+zjIv59nHVKkBbMMNS06bgKcltUfEEWBY8flB0rtYvifJL9JuThsemzm14TxzasN55tKGs8ypDeeZUxvOM5c2nGVObTjPPqVKm8BsAlYovaOmq4ZllqQXyiquaiTtlbSz+O1B4ImImAgsBDa6+WuJx2ZezjMv55mPs8zLeeblPPNxlnk5zz6mMs8A1ouI4cBiYANpWXmhB07vRcRI0n3ZW4CZkl4suaTK8tjMy3nm5TzzcZZ5Oc+8nGc+zjIv59k3VOkW0Hqnk1aqLsANSw4vk17+/H1n+ZZ5bOblPPNynvk4y7ycZ17OMx9nmZfz7AOqdAtovVrD4oGTgaRDwGxnmYXHZl7OMy/nmY+zzMt55uU883GWeTnPPqCSt4ACRMQQSYfLrsOskcdmXs4zL+eZj7PMy3nm5TzzcZZ5Oc/yVbYBNDMzMzMzs9ZU9RZQMzMzMzMza5EbQDMzMzMzswHCDaCZmZmZmdkAUdXXQJiZmZ00ETEbuA1YCTwjaX1E3AEsk7SjF993GvAwcI6kD2Qt1szMrAduAM3MzJqzBtgBfBZYL2lRb79I0v+ACRHxUq7izMzMmuEG0MzMrDmDgVuAYRGxGbgJmAdMA0YDZwIjgDuBK4GzgVmSnoiIBcB0oBO4V9LPSqjfzMzMzwCamZk16SjwI2CNpIcaftYu6UvAA8Blkr5c/NlpEfEx4KvAOGA8MDki4iTWbWZmdoxXAM3MzN66vxX/3w/8s/j1PmAo8HHgLODR4vMzgDGATmaBZmZm4BVAMzOzVnTQ9bmzs4e/I+AF4BJJE4BVwN+zV2ZmZtYEN4BmZmbNex6YFBHTmv0Lkp4jrf5tioinSKt/u96m+szMzHo0qLOzp0lLMzMzK14DcY6kWzJ/70t+DYSZmZ1MXgE0MzNrzvSIuCnHF0XEaRGxMcd3mZmZtcIrgGZmZmZmZgOEVwDNzMzMzMwGCDeAZmZmZmZmA4QbQDMzMzMzswHCDaCZmZmZmdkA4QbQzMzMzMxsgHADaGZmZmZmNkD8H3ic2AIXGpYQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot daily training market returns\n",
    "ax.plot(stock_data_return.index[:split_row,], train_stock_data_return['QQQ'], c='C0', label='train')\n",
    "\n",
    "# plot daily validation market returns\n",
    "ax.plot(stock_data_return.index[split_row:,], valid_stock_data_return['QQQ'], c='C1', label='valid')\n",
    "\n",
    "# rotate x-labels 45 degree angle\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels and limits\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2010'), pd.to_datetime('31-12-2019')])\n",
    "ax.set_ylabel('[daily QQQ returns]', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# set plot title\n",
    "plt.title('Invesco QQQ Trust Series 1 - Daily Historical Adjusted Returns Train-Validation Split', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine count (shape) of daily return train sequences $r^{i}_{train}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3283, 29)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_data_return.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine count (shape) of daily return train sequences $r^{i}_{valid}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 29)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_data_return.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Time-Series to Sequence Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we determine the number of return time-steps $n$ each individual sequence $s^{i}$ should be comprised of. Each sequence is thereby determined by the number of predictor (return) time-steps $t$ and the prediction (return) horizon $h = t+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px\" src=\"timesteps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will set the number of predictor (return) time-steps to $t$=4. This indicates that the input sequence of each sample is a vector of 4 sequential daily stock returns (pls. note, the choice of $t$=4 is arbitrary and should be selected through experimentation). Furthermore, we set the return horizon to be predicted to 1, which specifies that we aim to forecast a single future time-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 4 # number of predictor timesteps\n",
    "horizon = 1 # number of timesteps to be predicted\n",
    "sequence_length = time_steps + horizon # determine sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract sequences $s^i$ of **sequence length: 5 time-steps**. Thereby, we will step-wise iterate (\"rolling window\") over the entire sequence of daily stock returns $r_i$. In each iteration step, we extract an individual sequence of stock returns consisting of $n$ time-steps. The extracted individual sequences of daily closing prices are then collected in a single data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"sequences.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 In-Sample Target Log-Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will determine the number of available **rolling training (\"in-sample\") target sequences**. Remember that each sequence exhibits consists of **a sequence length of 5 time-steps** as defined by the variable `sequence_length` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train_sequences = train_stock_data_return.shape[0] - sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print the number of available rolling training sequences comprised of 5 daily adjusted log-return time-steps each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3278"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_train_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a 2D tensor (a 2D array) that contains the target variable we aim to learn to predict as part of the training procedure. The training target corresponds to a **2D tensor of size** `no_train_sequences`$\\times$`sequence_length`. The tensor contains the daily adjusted log-returns of the QQQ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the train sequence target daily returns data 2D matrix\n",
    "train_stock_sequence_target_data = np.zeros([no_train_sequences, sequence_length])\n",
    "\n",
    "# init the train sequence target daily dates data 2D matrix\n",
    "train_stock_sequence_target_date = np.empty([no_train_sequences, sequence_length], dtype='datetime64[s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fill the created 2D tensor with the rolling sequences of the QQQ's daily adjusted log-returns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the sequence count\n",
    "j = 0\n",
    "\n",
    "# iterate over the distinct daily returns of the training dataset\n",
    "while j < (train_stock_data_return.shape[0] - sequence_length):\n",
    "\n",
    "    # determine current training sequence returns\n",
    "    return_sequence = np.array(train_stock_data_return['QQQ'][j:j + sequence_length].T)\n",
    "    \n",
    "    # determine current training sequence dates\n",
    "    date_sequence = np.array(train_stock_data_return.index[j:j + sequence_length].T)\n",
    "    \n",
    "    # fill 2D matrix of train stock target sequences with current sequence\n",
    "    train_stock_sequence_target_data[j, :] = return_sequence\n",
    "    \n",
    "    # fill 2D matrix of train stock target sequences with current sequence\n",
    "    train_stock_sequence_target_date[j, :] = date_sequence\n",
    "    \n",
    "    # increase the sequence count\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, let's inspect the final shape of the filled 2D-array of daily adjusted log-return **target training** sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3278, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_sequence_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, inspect the **first 10 sequences** of the initialized and filled 2D-array of daily adjusted log-return target training sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.00604943,  0.00064991,  0.00819669,  0.        ],\n",
       "       [-0.00604943,  0.00064991,  0.00819669,  0.        ,  0.        ],\n",
       "       [ 0.00064991,  0.00819669,  0.        ,  0.        , -0.00409013],\n",
       "       [ 0.00819669,  0.        ,  0.        , -0.00409013, -0.01258953],\n",
       "       [ 0.        ,  0.        , -0.00409013, -0.01258953,  0.0123734 ],\n",
       "       [ 0.        , -0.00409013, -0.01258953,  0.0123734 ,  0.00086305],\n",
       "       [-0.00409013, -0.01258953,  0.0123734 ,  0.00086305, -0.01170886],\n",
       "       [-0.01258953,  0.0123734 ,  0.00086305, -0.01170886,  0.        ],\n",
       "       [ 0.0123734 ,  0.00086305, -0.01170886,  0.        ,  0.        ],\n",
       "       [ 0.00086305, -0.01170886,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_sequence_target_data[0:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the **last 10 sequences** of the initialized and filled 2D-array of daily adjusted log-return target training sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02292021,  0.00626867, -0.02486142, -0.01460171, -0.0314837 ],\n",
       "       [ 0.00626867, -0.02486142, -0.01460171, -0.0314837 ,  0.        ],\n",
       "       [-0.02486142, -0.01460171, -0.0314837 ,  0.        ,  0.        ],\n",
       "       [-0.01460171, -0.0314837 ,  0.        ,  0.        , -0.02511091],\n",
       "       [-0.0314837 ,  0.        ,  0.        , -0.02511091,  0.        ],\n",
       "       [ 0.        ,  0.        , -0.02511091,  0.        ,  0.06056744],\n",
       "       [ 0.        , -0.02511091,  0.        ,  0.06056744,  0.00386229],\n",
       "       [-0.02511091,  0.        ,  0.06056744,  0.00386229, -0.00052278],\n",
       "       [ 0.        ,  0.06056744,  0.00386229, -0.00052278,  0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_sequence_target_data[-10:-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 In-Sample Input Log-Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will determine the number of available **rolling training (\"in-sample\") input sequences**. Remember that each sequence exhibits consists of a a total number of 5 time-steps as defined by the variable `sequence_length` above. However, we will use several NASDAQ constituents as an input. This will add a third dimension to the input data, defined by the variable `no_input_stocks`, we need to consider when preparing the rolling input training sequences. \n",
    "\n",
    "Initialize a 3D tensor (a 3D array) that contains the input sequences we aim learn from as part of the training procedure. The training input corresponds to a **3D tensor of size:** `no_train_sequences` $\\times$ `sequence_length`$\\times$`no_input_stocks`. The tensor contains the daily adjusted log-returns of the distinct NASDAQ constituents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the train sequence input data 3D matrix\n",
    "train_stock_sequence_input_data = np.zeros([no_train_sequences, sequence_length, no_input_stocks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fill the created 3D tensor with the rolling sequences of the distinct NASDAQ constituents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the distinct ticker symbols\n",
    "for i in range(0, len(symbols)):\n",
    "\n",
    "    # init sequence count\n",
    "    j = 0\n",
    "\n",
    "    # iterate over the distinct daily returns of the training dataset\n",
    "    while j < (train_stock_data_return.shape[0] - sequence_length):\n",
    "\n",
    "        # determine current training sequence\n",
    "        sequence = np.array(train_stock_data_return[symbols[i]][j:j + sequence_length].T)\n",
    "\n",
    "        # fill 3D matrix of train stock input sequences with current sequence\n",
    "        train_stock_sequence_input_data[j, :, i] = sequence\n",
    "\n",
    "        # increase sequence count\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the final shape of the filled 3D-array of daily adjusted log-return **input training** sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3278, 5, 28)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_sequence_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's inspect the first 10 sequences of the **first constituent** of the filled 3D-array of daily adjusted log-return input training sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0290665 ,  0.0044663 , -0.03445491,  0.02370213,  0.        ],\n",
       "       [ 0.0044663 , -0.03445491,  0.02370213,  0.        ,  0.        ],\n",
       "       [-0.03445491,  0.02370213,  0.        ,  0.        , -0.01818239],\n",
       "       [ 0.02370213,  0.        ,  0.        , -0.01818239, -0.06050989],\n",
       "       [ 0.        ,  0.        , -0.01818239, -0.06050989,  0.0193056 ],\n",
       "       [ 0.        , -0.01818239, -0.06050989,  0.0193056 ,  0.02548505],\n",
       "       [-0.01818239, -0.06050989,  0.0193056 ,  0.02548505, -0.05754218],\n",
       "       [-0.06050989,  0.0193056 ,  0.02548505, -0.05754218,  0.        ],\n",
       "       [ 0.0193056 ,  0.02548505, -0.05754218,  0.        ,  0.        ],\n",
       "       [ 0.02548505, -0.05754218,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_sequence_input_data[0:10, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's inspect the first 10 sequences of the **last constituent** of the filled 3D-array of daily adjusted log-return input training sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00032302, -0.00615596, -0.01045419,  0.00687323,  0.        ],\n",
       "       [-0.00615596, -0.01045419,  0.00687323,  0.        ,  0.        ],\n",
       "       [-0.01045419,  0.00687323,  0.        ,  0.        , -0.01280167],\n",
       "       [ 0.00687323,  0.        ,  0.        , -0.01280167, -0.00662944],\n",
       "       [ 0.        ,  0.        , -0.01280167, -0.00662944,  0.00926865],\n",
       "       [ 0.        , -0.01280167, -0.00662944,  0.00926865,  0.01989937],\n",
       "       [-0.01280167, -0.00662944,  0.00926865,  0.01989937, -0.00323519],\n",
       "       [-0.00662944,  0.00926865,  0.01989937, -0.00323519,  0.        ],\n",
       "       [ 0.00926865,  0.01989937, -0.00323519,  0.        ,  0.        ],\n",
       "       [ 0.01989937, -0.00323519,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_sequence_input_data[0:10, :, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Out-of-Sample Output Log-Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will determine the number of available **rolling validation (\"out-of-sample\") target sequences**. Remember again that each sequence exhibits consists of a a total number of 5 time-steps as defined by the variable `sequence_length` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine max valid index\n",
    "no_valid_sequences = valid_stock_data_return.shape[0] - sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print the number of available validation training comprised of 5 daily adjusted log-return time-steps each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_valid_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a 2D tensor (a 2D array) that contains the target variable we aimed to learn to predict as part of the training procedure. The validation target corresponds to a **2D tensor of size** `no_valid_sequences`$\\times$`sequence_length`. The tensor contains the daily adjusted log-returns of the QQQ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the valid sequence target daily returns data 2D matrix\n",
    "valid_stock_sequence_target_data = np.zeros([no_valid_sequences, sequence_length])\n",
    "\n",
    "# init the valid sequence target daily dates data 2D matrix\n",
    "valid_stock_sequence_target_date = np.empty([no_valid_sequences, sequence_length], dtype='datetime64[s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now again fill the created 2D tensor with the rolling sequences of the QQQ's daily adjusted log-returns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init sequence count\n",
    "j = 0\n",
    "\n",
    "# iterate over the distinct daily returns of the validation dataset\n",
    "while j < (valid_stock_data_return.shape[0] - sequence_length):\n",
    "\n",
    "    # determine current validation sequence returns\n",
    "    return_sequence = np.array(valid_stock_data_return['QQQ'][j:j + sequence_length].T)\n",
    "    \n",
    "    # determine current validation sequence dates\n",
    "    date_sequence = np.array(valid_stock_data_return.index[j:j + sequence_length].T)\n",
    "\n",
    "    # fill 2D matrix of valid stock target sequences with current sequence\n",
    "    valid_stock_sequence_target_data[j, :] = return_sequence\n",
    "    \n",
    "    # fill 2D matrix of valid stock target sequences with current sequence\n",
    "    valid_stock_sequence_target_date[j, :] = date_sequence\n",
    "\n",
    "    # increase sequence count\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, let's inspect the final shape of the filled 2D-array of daily adjusted log-return **target validation** sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_sequence_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, inspect the **first 10 sequences** of the initialized and filled 2D-array of daily adjusted log-return target validation sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.00401123, -0.03321587,  0.04189455,  0.        ],\n",
       "       [ 0.00401123, -0.03321587,  0.04189455,  0.        ,  0.        ],\n",
       "       [-0.03321587,  0.04189455,  0.        ,  0.        ,  0.01183519],\n",
       "       [ 0.04189455,  0.        ,  0.        ,  0.01183519,  0.00900488],\n",
       "       [ 0.        ,  0.        ,  0.01183519,  0.00900488,  0.00811651],\n",
       "       [ 0.        ,  0.01183519,  0.00900488,  0.00811651,  0.00285608],\n",
       "       [ 0.01183519,  0.00900488,  0.00811651,  0.00285608, -0.00366503],\n",
       "       [ 0.00900488,  0.00811651,  0.00285608, -0.00366503,  0.        ],\n",
       "       [ 0.00811651,  0.00285608, -0.00366503,  0.        ,  0.        ],\n",
       "       [ 0.00285608, -0.00366503,  0.        ,  0.        , -0.00887598]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_sequence_target_data[0:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the **last 10 sequences** of the initialized and filled 2D-array of daily adjusted log-return target validation sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00057327,  0.00076392,  0.006232  ,  0.00402305,  0.        ],\n",
       "       [ 0.00076392,  0.006232  ,  0.00402305,  0.        ,  0.        ],\n",
       "       [ 0.006232  ,  0.00402305,  0.        ,  0.        ,  0.00263776],\n",
       "       [ 0.00402305,  0.        ,  0.        ,  0.00263776,  0.00051926],\n",
       "       [ 0.        ,  0.        ,  0.00263776,  0.00051926,  0.        ],\n",
       "       [ 0.        ,  0.00263776,  0.00051926,  0.        ,  0.00878534],\n",
       "       [ 0.00263776,  0.00051926,  0.        ,  0.00878534, -0.00084225],\n",
       "       [ 0.00051926,  0.        ,  0.00878534, -0.00084225,  0.        ],\n",
       "       [ 0.        ,  0.00878534, -0.00084225,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_sequence_target_data[-10:-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Out-of-Sample Input Log-Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we will determine the number of available **rolling training (\"out-of-sample\") input sequences**. Remember that each sequence exhibits consists of a a total number of 5 time-steps as defined by the variable `sequence_length` above. However, we will use several NASDAQ constituents as an input. Again, this will add a third dimension to the input data, defined by the variable `no_input_stocks`, we need to consider when preparing the rolling input validation sequences. \n",
    "\n",
    "Initialize a 3D tensor (a 3D array) that contains the input sequences we aim to use in the validation procedure. The validation input corresponds to a **3D tensor of size** `no_valid_sequences` $\\times$ `sequence_length`$\\times$`no_input_stocks`. The tensor contains the daily adjusted log-returns of the distinct NASDAQ constituents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the valid sequence input data 3D matrix\n",
    "valid_stock_sequence_input_data = np.zeros([no_valid_sequences, sequence_length, no_input_stocks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fill the created 3D tensor with the rolling sequences of the distinct NASDAQ constituents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over distinct ticker symbols\n",
    "for i in range(0, len(symbols)):\n",
    "\n",
    "    # init sequence count\n",
    "    j = 0\n",
    "\n",
    "    # iterate over the distinct daily returns of the training dataset\n",
    "    while j < (valid_stock_data_return.shape[0] - sequence_length):\n",
    "\n",
    "        # determine current training sequence\n",
    "        sequence = np.array(valid_stock_data_return[symbols[i]][j:j + sequence_length].T)\n",
    "\n",
    "        # fill 3D matrix of train stock input sequences with current sequence\n",
    "        valid_stock_sequence_input_data[j, :, i] = sequence\n",
    "\n",
    "        # increase sequence count\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the final shape of the filled 3D-array of daily adjusted log-return **input validation** sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 5, 28)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_sequence_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's inspect the first 10 sequences of the **first constituent** of the filled 3D-array of daily adjusted log-return input validation sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.03164037, -0.05491576,  0.0533879 ,  0.        ],\n",
       "       [ 0.03164037, -0.05491576,  0.0533879 ,  0.        ,  0.        ],\n",
       "       [-0.05491576,  0.0533879 ,  0.        ,  0.        ,  0.03898542],\n",
       "       [ 0.0533879 ,  0.        ,  0.        ,  0.03898542, -0.0076764 ],\n",
       "       [ 0.        ,  0.        ,  0.03898542, -0.0076764 ,  0.04915696],\n",
       "       [ 0.        ,  0.03898542, -0.0076764 ,  0.04915696,  0.01317472],\n",
       "       [ 0.03898542, -0.0076764 ,  0.04915696,  0.01317472,  0.00278083],\n",
       "       [-0.0076764 ,  0.04915696,  0.01317472,  0.00278083,  0.        ],\n",
       "       [ 0.04915696,  0.01317472,  0.00278083,  0.        ,  0.        ],\n",
       "       [ 0.01317472,  0.00278083,  0.        ,  0.        , -0.03792192]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_sequence_input_data[0:10, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's inspect the first 10 sequences of the **last constituent** of the filled 3D-array of daily adjusted log-return input validation sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.00444031, -0.03748175,  0.04546014,  0.        ],\n",
       "       [-0.00444031, -0.03748175,  0.04546014,  0.        ,  0.        ],\n",
       "       [-0.03748175,  0.04546014,  0.        ,  0.        ,  0.00127464],\n",
       "       [ 0.04546014,  0.        ,  0.        ,  0.00127464,  0.00722439],\n",
       "       [ 0.        ,  0.        ,  0.00127464,  0.00722439,  0.01419825],\n",
       "       [ 0.        ,  0.00127464,  0.00722439,  0.01419825, -0.00644617],\n",
       "       [ 0.00127464,  0.00722439,  0.01419825, -0.00644617, -0.00775208],\n",
       "       [ 0.00722439,  0.01419825, -0.00644617, -0.00775208,  0.        ],\n",
       "       [ 0.01419825, -0.00644617, -0.00775208,  0.        ,  0.        ],\n",
       "       [-0.00644617, -0.00775208,  0.        ,  0.        , -0.00732245]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_stock_sequence_input_data[0:10, :, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finalize the data preparation, let's briefly revisit how RNN's or, more specifically, LSTM based NN's can be trained to predict the next element of an input sequence. The cartoon below is derived from the \"Next Word Predictor\" Example that we also discussed in the course. For each **input return** $r_{i}$ of the input return training sequence $s^i$ the LSTM is supposed to learn to **predict the return** of the next time-step $\\hat{r}_{i+1}$. In order to make such a future return $\\hat{r}_{i+1}$ prediction the LSTM uses it's learned hidden state information $h_{i}$ as well as the current return $r_{i}$ as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Input-Target Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time-step the predicted return $\\hat{r}_{i+1}$ is then compared to the **target return** $r_{i+1}$. The discrepancy between both is collected as a loss $\\mathcal{L}$ for the distinct timesteps. The accumulation of the individual time-step losses is accumulated as the total loss of a sequence $\\mathcal{L}_{All}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish the LSTM training setup outlined above each input training sequence $s^{i}_{train}$ is sliced to result in an offset sequence of input-train-sequence return time-steps denoted by $s^{i}_{train, input}=\\{r_{1}, ..., r_{t-1}\\}$. Furthermore, each target training sequence $s^{i}_{train}$ is sliced to result in an offset sequence of target-train-sequence return time-steps denoted by $s^{i}_{train, target}=\\{r_{2}, ..., r_{t}\\}$. An example of the applied sequence offset given a sequence length of five time-steps is shown hereafter: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"sequencesplit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now determine the introduced offset of the target training sequences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_target = train_stock_sequence_target_data[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the derived dimensionality of the **target-train-sequences** tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3278, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's determine the introduced offset of the input training sequences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_input = train_stock_sequence_input_data[:, :-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the derived dimensionality of the **input-train-sequences** tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3278, 4, 28)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step we will likewise process each input validation sequence $s^{i}_{valid}$. The input validation sequences are sliced to also result in offset sequences of input-validation-sequence return time-steps denoted by $s^{i}_{valid, input}=\\{r_{1}, ..., r_{t-1}\\}$. Furthermore, each target validation sequence $s^{i}_{valid}$ is sliced to result in an offset sequence of target-validation-sequence return time-steps denoted by $s^{i}_{valid, target}=\\{r_{2}, ..., r_{t}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the introduced offset of the target validation sequences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sequences_target = valid_stock_sequence_target_data[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the derived dimensionality of the **target-validation-sequences** tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sequences_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sequences_input = valid_stock_sequence_input_data[:, :-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the derived dimensionality of the **input-validation-sequences** tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 4, 28)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sequences_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Dataset Class Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process data using arbitrary neural network implemented in `PyTorch` it is necessary to convert the data into the `PyTorch` tensor format. This allows the library to store and update gradient information corresponding to a particular dataset when throughout the network training. Therefore, in the following cell, we convert the training and validation sequences into a `PyTorch` tensor format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data to torch tensor\n",
    "train_sequences_target = torch.from_numpy(train_sequences_target).float()\n",
    "train_sequences_input = torch.from_numpy(train_sequences_input).float()\n",
    "\n",
    "# convert validation data to torch tensor\n",
    "valid_sequences_target = torch.from_numpy(valid_sequences_target).float()\n",
    "valid_sequences_input = torch.from_numpy(valid_sequences_input).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recall from previous labs that the training of neural network models is conducted in a stochastic (mini-)batch wise procedure. To retrieve the prepared training and validation data in such (mini-)batches we tailor the `dataset` class provided by the `PyTorch` library accordingly.\n",
    "\n",
    "Therefore, we inherit and overwrite the individual functions of the `dataset` class. So that our dataset will supply the neural network with the individual batches of training sequences $s^{i}_{train, input}$ and corresponding batches of training targets $s^{i}_{train, target}$ throughout the model training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define daily returns dataset\n",
    "class DailyReturnsDataset(data.Dataset):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, inputs, targets):\n",
    "\n",
    "        # init innput sequences and corresponding target sequences\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    # define the length method \n",
    "    def __len__(self):\n",
    "\n",
    "        # returns the number of samples\n",
    "        return len(self.targets)\n",
    "\n",
    "    # define get item method\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # determine single input sequence and target sequences batch\n",
    "        input_batch = self.inputs[index, :, :]\n",
    "        target_batch = self.targets[index, :]\n",
    "\n",
    "        # return input and target sequences batch\n",
    "        return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have overwritten the dataset class we instantiate it using the prepared `PyTorch` tensor of input return sequences and the `PyTorch` tensor of target return sequences. This will allow us to retrieve both batches of input return sequences and corresponding batches of target return sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DailyReturnsDataset(inputs=train_sequences_input, targets=train_sequences_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works by getting the 42th sequence and its corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = train_dataset.__getitem__(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now likewise instantiate a daily closing dataset using the prepared validation input sequences $s^{i}_{valid, input}$ and corresponding targets $s^{i}_{valid, target}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = DailyReturnsDataset(inputs=valid_sequences_input, targets=valid_sequences_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works by getting the 42th sequence and its corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input, valid_target = valid_dataset.__getitem__(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement the LSTM architecture of the to be learned time series model. Upon successful implementation we will specify the distinct neural network training parameters, such us the loss-function, learning-rate and optimization technique used throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. LSTM Architecture Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement the architecture of the LSTM neural network utilized to predict future returns of financial time series data, e.g. as in this example, the future returns of the QQQ and the underlying NASDAQ Top 28 constituents. The neural network, which we name **\"LSTMNet_Many2One\"** consists in total of two `LSTMCell` layers (to extract the temporal features) and three non-linear fully-connected layers (the predict the returns):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 400px\" src=\"lstmnet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the LSTM cell structure as well as the formal definition of its individual gate functions are shown in the following illustration that we already discussed in the lecture (not considering the bias of each LSTM cell layer):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"lstmcell.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Source: https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the two `LSTMCell` layers consists of an LSTM cell exhibiting a hidden state of 50 dimensions. The following three non-linear fully-connected layers consequently reduce the 50 hidden state dimensions of the second LSTM cell into a single output dimension. \n",
    "\n",
    "The single dimensional output corresponds to the predicted return of the next time-step. Please note, that the choice of the implemented architecture and selected network training parameters are arbitrary and should in a real-world scenario be selected through experimental evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the LSTMNet Many2One network architecture\n",
    "class LSTMNet_Many2One(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(LSTMNet_Many2One, self).__init__()\n",
    "\n",
    "        # define lstm network RNN architecture (feature extraction)\n",
    "        self.lstm1 = nn.LSTMCell(no_input_stocks, 50)  # first lstm layer\n",
    "        self.lstm2 = nn.LSTMCell(50, 50)  # second lstm layer\n",
    "        \n",
    "        # define lstm network FC architecture (feature prediction)\n",
    "        self.linear1 = nn.Linear(50, 20)  # first linear layer\n",
    "        self.linear2 = nn.Linear(20, 10)  # second linear layer\n",
    "        self.linear3 = nn.Linear(10, 1)  # third linear layer\n",
    "        \n",
    "        # define non-linar activation function\n",
    "        self.tanh = nn.Tanh() # the non-linarity\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, input):\n",
    "        \n",
    "        # init predictions\n",
    "        predictions = []\n",
    "\n",
    "        # init the lstm hidden states\n",
    "        h_t1 = torch.zeros(input.size(0), 50, dtype=torch.float).to(device)\n",
    "        h_t2 = torch.zeros(input.size(0), 50, dtype=torch.float).to(device)\n",
    "\n",
    "        # init the lstm cell states\n",
    "        c_t1 = torch.zeros(input.size(0), 50, dtype=torch.float).to(device)\n",
    "        c_t2 = torch.zeros(input.size(0), 50, dtype=torch.float).to(device)\n",
    "        \n",
    "        # iterate over distinct time steps\n",
    "        for input_t in input.chunk(input.size(1), dim=1):\n",
    "            \n",
    "            # squeeze the input to remove the second-dimension\n",
    "            input_t_squeezed = input_t.squeeze(1)\n",
    "\n",
    "            # propagate through time step data (feature extraction)\n",
    "            h_t1, c_t1 = self.lstm1(input_t_squeezed, (h_t1, c_t1))\n",
    "            h_t2, c_t2 = self.lstm2(h_t1, (h_t2, c_t2))\n",
    "            \n",
    "            # propagate through feture data (feature prediction)\n",
    "            l1 = self.tanh(self.linear1(h_t2))\n",
    "            l2 = self.tanh(self.linear2(l1))\n",
    "            prediction = self.linear3(l2)\n",
    "            \n",
    "            # collect predictions\n",
    "            predictions += [prediction]\n",
    "\n",
    "        # stack predictions\n",
    "        predictions = torch.stack(predictions, 1).squeeze(2)\n",
    "\n",
    "        # return predictions\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have implemented the `LSTMNet_Many2One` neural network we are ready to instantiate a model to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMNet_Many2One().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized, we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] LSTMNet Many2One architecture:\n",
      "\n",
      "LSTMNet_Many2One(\n",
      "  (lstm1): LSTMCell(28, 50)\n",
      "  (lstm2): LSTMCell(50, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] LSTMNet Many2One architecture:\\n\\n{}\\n'.format(lstm_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Great! Finally, let's also determine the number of model parameters that we aim to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Number of to be trained LSTMNet Many2One model parameters: 37641.\n"
     ]
    }
   ],
   "source": [
    "# init the number of model parameters\n",
    "num_params = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in lstm_model.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of to be trained LSTMNet Many2One model parameters: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our \"simple\" `LSTMNet_Many2One` model already encompasses an impressive number **37'641 model parameters** to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Training Parameter Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now good to train the network. However, prior to starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the prediction error of the true return $r_{t+1}$ and the by the model predicted return $\\hat{r}_{t+1}$ at a given time-step $t+1$ of sequence $s^{i}$. In other words, for a given sequence of historic returns we aim to learn a function $f_\\theta$ that is capable to predicts the return of the next timestep as faithfully as possible, as expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\hat{r}_{t+1} = f_\\theta(r_{t}, r_{t-1}, ..., r_{t-n})$. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby, the training objective is to learn a set of optimal model parameters $\\theta^*$ that optimize $\\min_{\\theta} \\|r_{t+1} - f_\\theta(r_{t}, r_{t-1}, ..., r_{t-n})\\|$ over all time-steps $t$ contained in the set of training sequences $s_{train}$. To achieve this optimization objective, one typically minimizes a loss function $\\mathcal{L_{\\theta}}$ while training the neural network. In this lab we use the **\"Mean Squared Error (MSE)\"** loss, as denoted by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L}^{MSE}_{\\theta} (r_{t+1}, \\hat{r}_{t+1}) = \\frac{1}{N} \\sum_{i=1}^N \\| r_{t+1} - \\hat{r}_{t+1}\\|^{2}$, </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the training process, the PyTorch library will automatically calculate the loss magnitude, compute the gradient, and update the parameters $\\theta$ of the LSTM neural network. We will use the **\"Adaptive Moment Estimation Optimization\" (ADAM)** technique to optimize the network parameters. Furthermore, we specify a constant learning rate of $l = 1e-06$. For each training step, the optimizer will update the model parameters $\\theta$ values according to the degree of prediction error (the MSE loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-06 # set constant learning rate\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate) # define optimization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's start to learn a model by training the NN for **1000 epochs** in mini-batches of the size of **128  sequences** per batch. This implies that the whole dataset will be fed to the network **1000 times** in chunks of 128 sequences yielding to **26 mini-batches** (3,278 training sequences / 128 sequences per mini-batch) per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "no_epochs = 1001 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify and instantiate a `PyTorch` `dataloader` using the training dataset specified above (the `training_dataset` variable). The specified `dataloader` will feed the **train-input-return** sequence tensors as well as the **train-target-return** to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataloader.DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will specify a periodic **model evaluation interval**. The interval defines the number of completed training epochs upon which an in-sample **backtest** of the current model will be conducted. Furthermore, a model **checkpoint** (a snapshot of the current model parameters) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model evaluation and checkpoint interval\n",
    "train_checkpoint_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the `train_checkpoint_epoch` variable to a value of **100 epochs** denotes, that we aim to **evaluate our model** (and save a corresponding model checkpoint) upon the successful completion of 100 training epochs respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing the Model Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will define a set of reusable `Python` functions that we will repeatedly use to **backtest** the trained model at different stages of the training process. In order to run a backtest simulation we will use the python `bt` library. The Python `bt` library is a flexible, backtest framework that can be used to test quantitative trading strategies. In general, backtesting is the process of testing a strategy over a given data set (more details about the `bt` library can be found via: https://pmorissette.github.io/bt/).\n",
    "\n",
    "Let's now start to implement the required functions to backtest and evaluate our learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Derive Model Return Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a function `compute_model_predictions` that, given a trained model (`lstm_model`), determines and returns a list of return predictions $\\hat{r}_{i+1}$ based on a set of given **in-sample** input training sequences (`train_sequences_input`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_predictions(lstm_model, train_sequences_input):\n",
    "\n",
    "    # set model in evaluation mode\n",
    "    lstm_model.eval()\n",
    "\n",
    "    # determine model return predictions\n",
    "    train_predictions = lstm_model(train_sequences_input.to(device))\n",
    "\n",
    "    # convert model return predictions to list\n",
    "    train_predictions_list = train_predictions.cpu().detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # return model return predictions\n",
    "    return train_predictions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Derive Model Trade Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function `compute_model_signals` that converts a list of **future return predictions $\\hat{r}_{t+1}$** (`train_predictions_list`) into **trade signals** $\\phi(\\hat{r}_{t+1})$. Thereby, we will interpret any positive future return prediction greater or equal a predefined threshold $t$ (`signal_threshold`) as a **\"long\" (buy)** signal, formally denoted by $r_{t+1} \\geq t$. Correspondingly, we will interpret any negative future return prediction lower or equal a predefined threshold $-t$, formally denoted by $r_{t+1} \\leq -t$ as a **\"short\" (sell)** signal. Formally, the trading signal is derived according to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "$\n",
    "\\\\\n",
    "\\phi(\\hat{r}_{t+1})=\n",
    "\\begin{cases}\n",
    "1.0 & \\textrm{(\"long signal\")}, & for & \\hat{r}_{t+1} > t\\\\\n",
    "-1.0 & \\textrm{(\"short signal\")}, & for & \\hat{r}_{t+1} < -t\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{r}_{t+1}$ denotes a by the model predicted future return at time $t+1$. In the following we will define the function accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_signals(train_predictions_list, train_stock_sequence_target_date, signal_threshold):\n",
    "\n",
    "    # init and prepare the model signal data \n",
    "    signal_data = pd.DataFrame(train_predictions_list, columns=['PREDICTIONS'], index=train_stock_sequence_target_date[:, -1])\n",
    "\n",
    "    # derive trading signals from model predictions\n",
    "    signal_data['SIGNAL'] = np.where(signal_data['PREDICTIONS'] >= signal_threshold, 1.0, 0.0)\n",
    "    signal_data['SIGNAL'] = np.where(signal_data['PREDICTIONS'] <= -signal_threshold, -1.0, signal_data['SIGNAL'])\n",
    "    \n",
    "    # offset the signal data\n",
    "    signal_data = signal_data.set_index(signal_data['SIGNAL'].index - pd.DateOffset(1))\n",
    "    \n",
    "    # return the prepared signal dta \n",
    "    return signal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's specify the trading signal threshold $t$ as introduced above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_threshold = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, setting such a threshold will avoid to generate trades caused by fluctuating (\"tiny\") return predictions of the trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Prepare Backtest Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function `compute_market_data` that, given a set of trading signals (`signal_data`), determines and **returns the QQQ market price data** (`stock_data_price`) of the trading signals time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_market_data(stock_data_price, signal_data):\n",
    "\n",
    "    # determine QQQ market price data \n",
    "    market_data = pd.DataFrame(stock_data_price['QQQ'])\n",
    "    market_data = market_data.rename(columns={'QQQ': 'PRICE'})\n",
    "    market_data = market_data.set_index(pd.to_datetime(stock_data_price.index))\n",
    "    \n",
    "    # filter market data according to trading signals time interval \n",
    "    market_data = market_data[market_data.index >= signal_data.index[0]]\n",
    "    market_data = market_data[market_data.index <= signal_data.index[-1]]\n",
    "    \n",
    "    # returns the filtered market data\n",
    "    return market_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Prepare Model Backest Strategy and Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trading signals as well as the market data let's implement the LSTM based trading strategy which we name `LSTMStrategy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStrategy(bt.Algo):\n",
    "    \n",
    "    def __init__(self, signals):\n",
    "        \n",
    "        # set class signals\n",
    "        self.signals = signals\n",
    "        \n",
    "    def __call__(self, target):\n",
    "        \n",
    "        if target.now in self.signals.index[1:]:\n",
    "            \n",
    "            # get actual signal\n",
    "            signal = self.signals[target.now]\n",
    "            \n",
    "            # set target weights according to signal\n",
    "            target.temp['weights'] = dict(PRICE=signal)\n",
    "            \n",
    "        # return True since we want to move on to the next timestep\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function `model_backtest` that, given a set of (i) trading signals (`signal_data`) and (ii) market price data (`stock_market_data`), instantiates and **runs a backtest**. Upon successful backtest completion the function collects and returns (i) a set of backtest performance measures (`backtest_result_stats`) as well as (ii) detailed backtest simulation results (`backtest_result_details`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backtest(signal_data, stock_market_data):\n",
    "    \n",
    "    # init the lstm backtest strategy\n",
    "    lstm_strategy = bt.Strategy('lstm', [bt.algos.SelectAll(), LSTMStrategy(signal_data['SIGNAL']), bt.algos.Rebalance()])\n",
    "    \n",
    "    # init the lstm backtest\n",
    "    backtest_lstm = bt.Backtest(strategy=lstm_strategy, data=stock_market_data, name='stock_lstm_backtest')\n",
    "    \n",
    "    # run the backtest\n",
    "    backtest_results = bt.run(backtest_lstm)\n",
    "    \n",
    "    # collect aggregated backtest result statistics\n",
    "    backtest_result_stats = {}\n",
    "    backtest_result_stats['total_return'] = backtest_lstm.stats.total_return # total return\n",
    "    backtest_result_stats['daily_sharpe'] = backtest_lstm.stats.daily_sharpe # daily sharpe\n",
    "    backtest_result_stats['monthly_sharpe'] = backtest_lstm.stats.monthly_sharpe # monthly sharpe\n",
    "    \n",
    "    # determine number of signal changes\n",
    "    backtest_result_stats['trade_signals'] = len(list(itertools.groupby(signal_data['SIGNAL'], lambda x: x > 0)))\n",
    "    \n",
    "    # collect details backtest result statistics\n",
    "    backtest_result_details = backtest_lstm.strategy.prices.to_frame(name='Rel. EQUITY')\n",
    "    backtest_result_details['Abs. EQUITY'] = backtest_lstm.strategy.values # equity per timestep\n",
    "    backtest_result_details['CASH'] = backtest_lstm.strategy.cash # cash per timestep\n",
    "    \n",
    "    # return backtest lstm details\n",
    "    return backtest_result_stats, backtest_result_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will define a function `plot_model_predictions` that, that creates a plot of (i) the actual **\"ground-true\" returns** (`train_sequences_target`) and (ii) the **predicted returns** of the trained model (`train_predictions_list`). Both returns will be visualized within the in-sample time interval and allow for a visual evaluation of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_predictions(train_stock_sequence_target_date, train_sequences_target, train_predictions_list, epoch, filename):\n",
    "    \n",
    "    # plot the prediction results\n",
    "    plt.style.use('seaborn')\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "    # init plot figure\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # convert train sequences target to list\n",
    "    train_targets_list = (train_sequences_target).numpy()[:, -1].tolist()\n",
    "    \n",
    "    ax.plot(train_stock_sequence_target_date[:, -1], train_targets_list, color='C1', label='groundtruth (green)')\n",
    "    ax.plot(train_stock_sequence_target_date[:, -1], train_predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "    # set x-axis limits\n",
    "    ax.set_xlim(train_stock_sequence_target_date[:, -1].min(), train_stock_sequence_target_date[:, -1].max())\n",
    "\n",
    "    # set plot legend\n",
    "    plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "    # set plot title\n",
    "    plt.title('Epoch {} - LSTM-NN Many2One - In-Sample Prediction vs. Ground-Truth Market Prices'.format(str(epoch).zfill(6)), fontsize=10)\n",
    "\n",
    "    # set axis labels\n",
    "    plt.xlabel('[time]', fontsize=8)\n",
    "    plt.ylabel('[daily returns]', fontsize=8)\n",
    "\n",
    "    # set axis ticks fontsize\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    \n",
    "    # save plot to plotting directory\n",
    "    plt.savefig(os.path.join(visuals_directory, filename), dpi=300)\n",
    "\n",
    "    # close plot\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training and Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train, validate and backtest the LSTM neural network model (as implemented in the section above) using the prepared dataset of daily return sequences. In the following, we will have a detailed look into the distinct training steps and monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Preparation of the Model Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the training we aim to monitor the model performance. Thereby, we will collect the models training loss and backtest performance at the distinct **model evaluation intervals** defined above. To achieve this objective will initialize a `Pandas` data frame (`training_statistics`) that will be populated with the logged information with progressing training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the training statistics\n",
    "summary_cols = ['timestamp', 'no_epochs', 'batch_size', 'learning_rate', 'train_epoch_loss', 'signal_threshold ', 'signals', 'total_return', 'daily_sharpe', 'monthly_sharpe']\n",
    "training_statistics = pd.DataFrame(np.zeros((int(no_epochs/train_checkpoint_epoch), len(summary_cols))), columns=summary_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Running the Model Training and Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the model. The training procedure of each mini-batch is performed as follows: \n",
    "\n",
    ">1. do a forward pass through the LSTMNet network, \n",
    ">2. compute the mean-squared prediction error $\\mathcal{L}^{MSE}_{\\theta} (r_{t+1}, \\hat{r}_{t+1}) = \\frac{1}{N} \\sum_{i=1}^N \\| r_{t+1} - \\hat{r}_{t+1}\\|^{2}$, \n",
    ">3. do a backward pass through the LSTMNet network, and \n",
    ">4. update the parameters of the network $f_\\theta(\\cdot)$.\n",
    "\n",
    "To ensure learning while training the LSTM model we will monitor whether the loss decreases with progressing training. Therefore, we obtain and evaluate the mean prediction performance over all mini-batches in each training epoch. Based on this evaluation we can conclude on the training progress and whether the loss is converging (indicating that the model might not improve any further).\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    " \n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\n",
    ">- `optimizer.step()` updates the network parameters based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-12:58:07] training :: epoch: 0 train-loss: 0.06653685122728348\n",
      "[LOG 20210201-12:58:07] training :: epoch: 1 train-loss: 0.06638859269710687\n",
      "[LOG 20210201-12:58:07] training :: epoch: 2 train-loss: 0.06620499749596302\n",
      "[LOG 20210201-12:58:08] training :: epoch: 3 train-loss: 0.0660095621760075\n",
      "[LOG 20210201-12:58:08] training :: epoch: 4 train-loss: 0.06581166988381973\n",
      "[LOG 20210201-12:58:08] training :: epoch: 5 train-loss: 0.065602124023896\n",
      "[LOG 20210201-12:58:09] training :: epoch: 6 train-loss: 0.06539910515913597\n",
      "[LOG 20210201-12:58:09] training :: epoch: 7 train-loss: 0.06519625679804729\n",
      "[LOG 20210201-12:58:09] training :: epoch: 8 train-loss: 0.06499105605941552\n",
      "[LOG 20210201-12:58:10] training :: epoch: 9 train-loss: 0.0647843494438208\n",
      "[LOG 20210201-12:58:10] training :: epoch: 10 train-loss: 0.06457766775901501\n",
      "[LOG 20210201-12:58:10] training :: epoch: 11 train-loss: 0.06436957361606452\n",
      "[LOG 20210201-12:58:11] training :: epoch: 12 train-loss: 0.06416690378234936\n",
      "[LOG 20210201-12:58:11] training :: epoch: 13 train-loss: 0.06395396647545007\n",
      "[LOG 20210201-12:58:11] training :: epoch: 14 train-loss: 0.06375549074548942\n",
      "[LOG 20210201-12:58:12] training :: epoch: 15 train-loss: 0.06354398223069999\n",
      "[LOG 20210201-12:58:12] training :: epoch: 16 train-loss: 0.06334571196482731\n",
      "[LOG 20210201-12:58:12] training :: epoch: 17 train-loss: 0.06314666483264703\n",
      "[LOG 20210201-12:58:13] training :: epoch: 18 train-loss: 0.06294786070401852\n",
      "[LOG 20210201-12:58:13] training :: epoch: 19 train-loss: 0.06273778341710567\n",
      "[LOG 20210201-12:58:13] training :: epoch: 20 train-loss: 0.06253192702738139\n",
      "[LOG 20210201-12:58:14] training :: epoch: 21 train-loss: 0.0623329897912649\n",
      "[LOG 20210201-12:58:14] training :: epoch: 22 train-loss: 0.06213596678124024\n",
      "[LOG 20210201-12:58:15] training :: epoch: 23 train-loss: 0.06193803680630831\n",
      "[LOG 20210201-12:58:15] training :: epoch: 24 train-loss: 0.06174935013628923\n",
      "[LOG 20210201-12:58:15] training :: epoch: 25 train-loss: 0.0615423063819225\n",
      "[LOG 20210201-12:58:16] training :: epoch: 26 train-loss: 0.06134811086723438\n",
      "[LOG 20210201-12:58:16] training :: epoch: 27 train-loss: 0.0611502665739793\n",
      "[LOG 20210201-12:58:16] training :: epoch: 28 train-loss: 0.0609606527365171\n",
      "[LOG 20210201-12:58:17] training :: epoch: 29 train-loss: 0.06076597264752938\n",
      "[LOG 20210201-12:58:17] training :: epoch: 30 train-loss: 0.06056665887053196\n",
      "[LOG 20210201-12:58:18] training :: epoch: 31 train-loss: 0.06037345227713768\n",
      "[LOG 20210201-12:58:18] training :: epoch: 32 train-loss: 0.060183796601799816\n",
      "[LOG 20210201-12:58:18] training :: epoch: 33 train-loss: 0.059994102957156986\n",
      "[LOG 20210201-12:58:18] training :: epoch: 34 train-loss: 0.05979976975000822\n",
      "[LOG 20210201-12:58:19] training :: epoch: 35 train-loss: 0.05962019098492769\n",
      "[LOG 20210201-12:58:19] training :: epoch: 36 train-loss: 0.0594189535253323\n",
      "[LOG 20210201-12:58:20] training :: epoch: 37 train-loss: 0.059229362182892285\n",
      "[LOG 20210201-12:58:20] training :: epoch: 38 train-loss: 0.05904442114898792\n",
      "[LOG 20210201-12:58:20] training :: epoch: 39 train-loss: 0.05885396969433014\n",
      "[LOG 20210201-12:58:21] training :: epoch: 40 train-loss: 0.05866985882704075\n",
      "[LOG 20210201-12:58:21] training :: epoch: 41 train-loss: 0.058479697801745854\n",
      "[LOG 20210201-12:58:21] training :: epoch: 42 train-loss: 0.05830146515598664\n",
      "[LOG 20210201-12:58:22] training :: epoch: 43 train-loss: 0.058113575411530644\n",
      "[LOG 20210201-12:58:22] training :: epoch: 44 train-loss: 0.057932012929366186\n",
      "[LOG 20210201-12:58:22] training :: epoch: 45 train-loss: 0.05774802041168396\n",
      "[LOG 20210201-12:58:23] training :: epoch: 46 train-loss: 0.057560317791425265\n",
      "[LOG 20210201-12:58:23] training :: epoch: 47 train-loss: 0.057381997171502844\n",
      "[LOG 20210201-12:58:23] training :: epoch: 48 train-loss: 0.05719656139039076\n",
      "[LOG 20210201-12:58:24] training :: epoch: 49 train-loss: 0.05702223628759384\n",
      "[LOG 20210201-12:58:24] training :: epoch: 50 train-loss: 0.05683592692590677\n",
      "[LOG 20210201-12:58:24] training :: epoch: 51 train-loss: 0.056656347014583074\n",
      "[LOG 20210201-12:58:25] training :: epoch: 52 train-loss: 0.056479718965979725\n",
      "[LOG 20210201-12:58:25] training :: epoch: 53 train-loss: 0.056300860041609176\n",
      "[LOG 20210201-12:58:26] training :: epoch: 54 train-loss: 0.056121051884614505\n",
      "[LOG 20210201-12:58:26] training :: epoch: 55 train-loss: 0.05594962992920326\n",
      "[LOG 20210201-12:58:26] training :: epoch: 56 train-loss: 0.055769736950214095\n",
      "[LOG 20210201-12:58:27] training :: epoch: 57 train-loss: 0.05559543806772966\n",
      "[LOG 20210201-12:58:27] training :: epoch: 58 train-loss: 0.05542272916780068\n",
      "[LOG 20210201-12:58:27] training :: epoch: 59 train-loss: 0.05524362824284113\n",
      "[LOG 20210201-12:58:28] training :: epoch: 60 train-loss: 0.05506796977267815\n",
      "[LOG 20210201-12:58:28] training :: epoch: 61 train-loss: 0.0548913858544368\n",
      "[LOG 20210201-12:58:29] training :: epoch: 62 train-loss: 0.05472373962402344\n",
      "[LOG 20210201-12:58:29] training :: epoch: 63 train-loss: 0.054558723591841184\n",
      "[LOG 20210201-12:58:30] training :: epoch: 64 train-loss: 0.05437755025923252\n",
      "[LOG 20210201-12:58:30] training :: epoch: 65 train-loss: 0.05419957064665281\n",
      "[LOG 20210201-12:58:30] training :: epoch: 66 train-loss: 0.054032222009622134\n",
      "[LOG 20210201-12:58:31] training :: epoch: 67 train-loss: 0.05386229676122849\n",
      "[LOG 20210201-12:58:31] training :: epoch: 68 train-loss: 0.05369248269842221\n",
      "[LOG 20210201-12:58:32] training :: epoch: 69 train-loss: 0.05352612670797568\n",
      "[LOG 20210201-12:58:32] training :: epoch: 70 train-loss: 0.053359782323241234\n",
      "[LOG 20210201-12:58:32] training :: epoch: 71 train-loss: 0.05319297027129393\n",
      "[LOG 20210201-12:58:33] training :: epoch: 72 train-loss: 0.05302041238890244\n",
      "[LOG 20210201-12:58:33] training :: epoch: 73 train-loss: 0.052853471671159453\n",
      "[LOG 20210201-12:58:33] training :: epoch: 74 train-loss: 0.052685813835034005\n",
      "[LOG 20210201-12:58:34] training :: epoch: 75 train-loss: 0.052519154376708545\n",
      "[LOG 20210201-12:58:34] training :: epoch: 76 train-loss: 0.052362728434113354\n",
      "[LOG 20210201-12:58:35] training :: epoch: 77 train-loss: 0.052190170695002265\n",
      "[LOG 20210201-12:58:35] training :: epoch: 78 train-loss: 0.05202435659101376\n",
      "[LOG 20210201-12:58:35] training :: epoch: 79 train-loss: 0.051861995114729956\n",
      "[LOG 20210201-12:58:36] training :: epoch: 80 train-loss: 0.051704929711726993\n",
      "[LOG 20210201-12:58:36] training :: epoch: 81 train-loss: 0.05153421871364117\n",
      "[LOG 20210201-12:58:36] training :: epoch: 82 train-loss: 0.05137113940257292\n",
      "[LOG 20210201-12:58:36] training :: epoch: 83 train-loss: 0.051207392261578485\n",
      "[LOG 20210201-12:58:37] training :: epoch: 84 train-loss: 0.05105747941594858\n",
      "[LOG 20210201-12:58:37] training :: epoch: 85 train-loss: 0.050889961708050505\n",
      "[LOG 20210201-12:58:37] training :: epoch: 86 train-loss: 0.05072884261608124\n",
      "[LOG 20210201-12:58:38] training :: epoch: 87 train-loss: 0.050564996181772306\n",
      "[LOG 20210201-12:58:38] training :: epoch: 88 train-loss: 0.05040414545398492\n",
      "[LOG 20210201-12:58:38] training :: epoch: 89 train-loss: 0.05024678747241314\n",
      "[LOG 20210201-12:58:39] training :: epoch: 90 train-loss: 0.05008662391740542\n",
      "[LOG 20210201-12:58:39] training :: epoch: 91 train-loss: 0.049930949623768144\n",
      "[LOG 20210201-12:58:39] training :: epoch: 92 train-loss: 0.04976968381267328\n",
      "[LOG 20210201-12:58:40] training :: epoch: 93 train-loss: 0.04961431814500919\n",
      "[LOG 20210201-12:58:40] training :: epoch: 94 train-loss: 0.049453830489745505\n",
      "[LOG 20210201-12:58:40] training :: epoch: 95 train-loss: 0.049301412959511466\n",
      "[LOG 20210201-12:58:41] training :: epoch: 96 train-loss: 0.04914768226444721\n",
      "[LOG 20210201-12:58:41] training :: epoch: 97 train-loss: 0.048987445779717885\n",
      "[LOG 20210201-12:58:41] training :: epoch: 98 train-loss: 0.04883536839714417\n",
      "[LOG 20210201-12:58:41] training :: epoch: 99 train-loss: 0.04867186311345834\n",
      "[LOG 20210201-12:58:42] training :: epoch: 100 train-loss: 0.04852120993802181\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-12:58:46] backtest :: total return: -0.7894, monthly-sharpe: -1.2253, daily-sharpe: -0.7574.\n",
      "[LOG 20210201-12:58:48] training :: epoch: 101 train-loss: 0.04836494189042311\n",
      "[LOG 20210201-12:58:48] training :: epoch: 102 train-loss: 0.0482095441279503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-12:58:49] training :: epoch: 103 train-loss: 0.04805904526550036\n",
      "[LOG 20210201-12:58:49] training :: epoch: 104 train-loss: 0.04791136229267487\n",
      "[LOG 20210201-12:58:49] training :: epoch: 105 train-loss: 0.04775949667852659\n",
      "[LOG 20210201-12:58:50] training :: epoch: 106 train-loss: 0.04760295725785769\n",
      "[LOG 20210201-12:58:50] training :: epoch: 107 train-loss: 0.04745295385901745\n",
      "[LOG 20210201-12:58:50] training :: epoch: 108 train-loss: 0.0472992853476451\n",
      "[LOG 20210201-12:58:51] training :: epoch: 109 train-loss: 0.04714517452969001\n",
      "[LOG 20210201-12:58:51] training :: epoch: 110 train-loss: 0.04699978991769827\n",
      "[LOG 20210201-12:58:51] training :: epoch: 111 train-loss: 0.04684715818327207\n",
      "[LOG 20210201-12:58:51] training :: epoch: 112 train-loss: 0.046695588872982904\n",
      "[LOG 20210201-12:58:52] training :: epoch: 113 train-loss: 0.046547538959063016\n",
      "[LOG 20210201-12:58:52] training :: epoch: 114 train-loss: 0.04639452208693211\n",
      "[LOG 20210201-12:58:52] training :: epoch: 115 train-loss: 0.046245767520024225\n",
      "[LOG 20210201-12:58:53] training :: epoch: 116 train-loss: 0.04610206731236898\n",
      "[LOG 20210201-12:58:53] training :: epoch: 117 train-loss: 0.04595005942078737\n",
      "[LOG 20210201-12:58:53] training :: epoch: 118 train-loss: 0.04580589813681749\n",
      "[LOG 20210201-12:58:54] training :: epoch: 119 train-loss: 0.045655822238096826\n",
      "[LOG 20210201-12:58:54] training :: epoch: 120 train-loss: 0.045507910303198375\n",
      "[LOG 20210201-12:58:54] training :: epoch: 121 train-loss: 0.04535816624187506\n",
      "[LOG 20210201-12:58:54] training :: epoch: 122 train-loss: 0.04521721200301097\n",
      "[LOG 20210201-12:58:55] training :: epoch: 123 train-loss: 0.04506906867027283\n",
      "[LOG 20210201-12:58:55] training :: epoch: 124 train-loss: 0.044922091210117705\n",
      "[LOG 20210201-12:58:55] training :: epoch: 125 train-loss: 0.04477541148662567\n",
      "[LOG 20210201-12:58:56] training :: epoch: 126 train-loss: 0.04462750943807455\n",
      "[LOG 20210201-12:58:56] training :: epoch: 127 train-loss: 0.044485485181212425\n",
      "[LOG 20210201-12:58:56] training :: epoch: 128 train-loss: 0.04434583321786844\n",
      "[LOG 20210201-12:58:57] training :: epoch: 129 train-loss: 0.04419563237864237\n",
      "[LOG 20210201-12:58:57] training :: epoch: 130 train-loss: 0.04405134156919443\n",
      "[LOG 20210201-12:58:57] training :: epoch: 131 train-loss: 0.04391307269151394\n",
      "[LOG 20210201-12:58:57] training :: epoch: 132 train-loss: 0.043772972833651766\n",
      "[LOG 20210201-12:58:58] training :: epoch: 133 train-loss: 0.04362096551519174\n",
      "[LOG 20210201-12:58:58] training :: epoch: 134 train-loss: 0.043483271048619196\n",
      "[LOG 20210201-12:58:58] training :: epoch: 135 train-loss: 0.043340520933270454\n",
      "[LOG 20210201-12:58:59] training :: epoch: 136 train-loss: 0.04319705713826876\n",
      "[LOG 20210201-12:58:59] training :: epoch: 137 train-loss: 0.04305452108383179\n",
      "[LOG 20210201-12:58:59] training :: epoch: 138 train-loss: 0.04291655767995577\n",
      "[LOG 20210201-12:58:59] training :: epoch: 139 train-loss: 0.042774253023358494\n",
      "[LOG 20210201-12:59:00] training :: epoch: 140 train-loss: 0.04263498748724277\n",
      "[LOG 20210201-12:59:00] training :: epoch: 141 train-loss: 0.04248816161774672\n",
      "[LOG 20210201-12:59:00] training :: epoch: 142 train-loss: 0.04235607915772842\n",
      "[LOG 20210201-12:59:01] training :: epoch: 143 train-loss: 0.042212132507791884\n",
      "[LOG 20210201-12:59:01] training :: epoch: 144 train-loss: 0.042075270357040256\n",
      "[LOG 20210201-12:59:01] training :: epoch: 145 train-loss: 0.04193672795708363\n",
      "[LOG 20210201-12:59:01] training :: epoch: 146 train-loss: 0.04179571783886506\n",
      "[LOG 20210201-12:59:02] training :: epoch: 147 train-loss: 0.04165446443053392\n",
      "[LOG 20210201-12:59:02] training :: epoch: 148 train-loss: 0.04152033936518889\n",
      "[LOG 20210201-12:59:02] training :: epoch: 149 train-loss: 0.041371812184269614\n",
      "[LOG 20210201-12:59:03] training :: epoch: 150 train-loss: 0.04123899541222132\n",
      "[LOG 20210201-12:59:03] training :: epoch: 151 train-loss: 0.041100114154127926\n",
      "[LOG 20210201-12:59:03] training :: epoch: 152 train-loss: 0.04096530764721907\n",
      "[LOG 20210201-12:59:03] training :: epoch: 153 train-loss: 0.040830620349599764\n",
      "[LOG 20210201-12:59:04] training :: epoch: 154 train-loss: 0.040691216547901816\n",
      "[LOG 20210201-12:59:04] training :: epoch: 155 train-loss: 0.040556686285596624\n",
      "[LOG 20210201-12:59:04] training :: epoch: 156 train-loss: 0.04042301785487395\n",
      "[LOG 20210201-12:59:05] training :: epoch: 157 train-loss: 0.04028212336393503\n",
      "[LOG 20210201-12:59:05] training :: epoch: 158 train-loss: 0.04014567113839663\n",
      "[LOG 20210201-12:59:05] training :: epoch: 159 train-loss: 0.040016575501515314\n",
      "[LOG 20210201-12:59:06] training :: epoch: 160 train-loss: 0.03987313792682611\n",
      "[LOG 20210201-12:59:06] training :: epoch: 161 train-loss: 0.039740814469181575\n",
      "[LOG 20210201-12:59:06] training :: epoch: 162 train-loss: 0.03960861752812679\n",
      "[LOG 20210201-12:59:06] training :: epoch: 163 train-loss: 0.039472779688926846\n",
      "[LOG 20210201-12:59:07] training :: epoch: 164 train-loss: 0.03934475765205347\n",
      "[LOG 20210201-12:59:07] training :: epoch: 165 train-loss: 0.03920823521912098\n",
      "[LOG 20210201-12:59:07] training :: epoch: 166 train-loss: 0.039069575329239555\n",
      "[LOG 20210201-12:59:08] training :: epoch: 167 train-loss: 0.03893610133001438\n",
      "[LOG 20210201-12:59:08] training :: epoch: 168 train-loss: 0.03880303897536718\n",
      "[LOG 20210201-12:59:08] training :: epoch: 169 train-loss: 0.03866871088170088\n",
      "[LOG 20210201-12:59:09] training :: epoch: 170 train-loss: 0.0385394196670789\n",
      "[LOG 20210201-12:59:09] training :: epoch: 171 train-loss: 0.03840917363189734\n",
      "[LOG 20210201-12:59:09] training :: epoch: 172 train-loss: 0.03827629524927873\n",
      "[LOG 20210201-12:59:09] training :: epoch: 173 train-loss: 0.03814058922804319\n",
      "[LOG 20210201-12:59:10] training :: epoch: 174 train-loss: 0.038009643697967894\n",
      "[LOG 20210201-12:59:10] training :: epoch: 175 train-loss: 0.037882295078956164\n",
      "[LOG 20210201-12:59:10] training :: epoch: 176 train-loss: 0.03775048098311974\n",
      "[LOG 20210201-12:59:11] training :: epoch: 177 train-loss: 0.037618836674552694\n",
      "[LOG 20210201-12:59:11] training :: epoch: 178 train-loss: 0.03749302903620096\n",
      "[LOG 20210201-12:59:11] training :: epoch: 179 train-loss: 0.03736139518710283\n",
      "[LOG 20210201-12:59:12] training :: epoch: 180 train-loss: 0.03723180408661182\n",
      "[LOG 20210201-12:59:12] training :: epoch: 181 train-loss: 0.03710026351305155\n",
      "[LOG 20210201-12:59:12] training :: epoch: 182 train-loss: 0.036966642508139975\n",
      "[LOG 20210201-12:59:12] training :: epoch: 183 train-loss: 0.0368421976096355\n",
      "[LOG 20210201-12:59:13] training :: epoch: 184 train-loss: 0.036710143519135624\n",
      "[LOG 20210201-12:59:13] training :: epoch: 185 train-loss: 0.03658184538093897\n",
      "[LOG 20210201-12:59:13] training :: epoch: 186 train-loss: 0.036449855766617335\n",
      "[LOG 20210201-12:59:14] training :: epoch: 187 train-loss: 0.03631695417257456\n",
      "[LOG 20210201-12:59:14] training :: epoch: 188 train-loss: 0.036197343411353916\n",
      "[LOG 20210201-12:59:14] training :: epoch: 189 train-loss: 0.03606735112575384\n",
      "[LOG 20210201-12:59:14] training :: epoch: 190 train-loss: 0.03593982498233135\n",
      "[LOG 20210201-12:59:15] training :: epoch: 191 train-loss: 0.03581143844013031\n",
      "[LOG 20210201-12:59:15] training :: epoch: 192 train-loss: 0.035686443058344036\n",
      "[LOG 20210201-12:59:15] training :: epoch: 193 train-loss: 0.03555765146246323\n",
      "[LOG 20210201-12:59:16] training :: epoch: 194 train-loss: 0.035429730581549496\n",
      "[LOG 20210201-12:59:16] training :: epoch: 195 train-loss: 0.035297670329992585\n",
      "[LOG 20210201-12:59:16] training :: epoch: 196 train-loss: 0.035178895466602765\n",
      "[LOG 20210201-12:59:16] training :: epoch: 197 train-loss: 0.035049485185971625\n",
      "[LOG 20210201-12:59:17] training :: epoch: 198 train-loss: 0.03492167291159813\n",
      "[LOG 20210201-12:59:17] training :: epoch: 199 train-loss: 0.03480141409314596\n",
      "[LOG 20210201-12:59:17] training :: epoch: 200 train-loss: 0.0346752879424737\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-12:59:22] backtest :: total return: -0.7894, monthly-sharpe: -1.2253, daily-sharpe: -0.7574.\n",
      "[LOG 20210201-12:59:23] training :: epoch: 201 train-loss: 0.03455041549526728\n",
      "[LOG 20210201-12:59:24] training :: epoch: 202 train-loss: 0.034423908505302206\n",
      "[LOG 20210201-12:59:24] training :: epoch: 203 train-loss: 0.034299215874992885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-12:59:24] training :: epoch: 204 train-loss: 0.03417361441713113\n",
      "[LOG 20210201-12:59:24] training :: epoch: 205 train-loss: 0.03405289323284076\n",
      "[LOG 20210201-12:59:25] training :: epoch: 206 train-loss: 0.03392901959327551\n",
      "[LOG 20210201-12:59:25] training :: epoch: 207 train-loss: 0.0338048583899553\n",
      "[LOG 20210201-12:59:25] training :: epoch: 208 train-loss: 0.03367311034638148\n",
      "[LOG 20210201-12:59:26] training :: epoch: 209 train-loss: 0.033556956224716626\n",
      "[LOG 20210201-12:59:26] training :: epoch: 210 train-loss: 0.03343295377607529\n",
      "[LOG 20210201-12:59:26] training :: epoch: 211 train-loss: 0.033309960164702855\n",
      "[LOG 20210201-12:59:26] training :: epoch: 212 train-loss: 0.033185071526811674\n",
      "[LOG 20210201-12:59:27] training :: epoch: 213 train-loss: 0.03306651788835342\n",
      "[LOG 20210201-12:59:27] training :: epoch: 214 train-loss: 0.032938105842241876\n",
      "[LOG 20210201-12:59:27] training :: epoch: 215 train-loss: 0.0328198903455184\n",
      "[LOG 20210201-12:59:28] training :: epoch: 216 train-loss: 0.03270277294975061\n",
      "[LOG 20210201-12:59:28] training :: epoch: 217 train-loss: 0.03257615678012371\n",
      "[LOG 20210201-12:59:28] training :: epoch: 218 train-loss: 0.03245015356403131\n",
      "[LOG 20210201-12:59:29] training :: epoch: 219 train-loss: 0.03233540416336977\n",
      "[LOG 20210201-12:59:29] training :: epoch: 220 train-loss: 0.03220972418785095\n",
      "[LOG 20210201-12:59:29] training :: epoch: 221 train-loss: 0.03209817868012648\n",
      "[LOG 20210201-12:59:30] training :: epoch: 222 train-loss: 0.03197201226766293\n",
      "[LOG 20210201-12:59:30] training :: epoch: 223 train-loss: 0.03185092313931538\n",
      "[LOG 20210201-12:59:30] training :: epoch: 224 train-loss: 0.03173044151984728\n",
      "[LOG 20210201-12:59:31] training :: epoch: 225 train-loss: 0.03160906812319389\n",
      "[LOG 20210201-12:59:31] training :: epoch: 226 train-loss: 0.03149354515167383\n",
      "[LOG 20210201-12:59:31] training :: epoch: 227 train-loss: 0.03137032336627062\n",
      "[LOG 20210201-12:59:32] training :: epoch: 228 train-loss: 0.031251412171583906\n",
      "[LOG 20210201-12:59:32] training :: epoch: 229 train-loss: 0.031134861019941475\n",
      "[LOG 20210201-12:59:32] training :: epoch: 230 train-loss: 0.031017190991685942\n",
      "[LOG 20210201-12:59:33] training :: epoch: 231 train-loss: 0.03089023195207119\n",
      "[LOG 20210201-12:59:33] training :: epoch: 232 train-loss: 0.03077688619780999\n",
      "[LOG 20210201-12:59:33] training :: epoch: 233 train-loss: 0.030657956663232584\n",
      "[LOG 20210201-12:59:33] training :: epoch: 234 train-loss: 0.030535174820285577\n",
      "[LOG 20210201-12:59:34] training :: epoch: 235 train-loss: 0.03042029947615587\n",
      "[LOG 20210201-12:59:34] training :: epoch: 236 train-loss: 0.030304530563835915\n",
      "[LOG 20210201-12:59:35] training :: epoch: 237 train-loss: 0.03018276235805108\n",
      "[LOG 20210201-12:59:35] training :: epoch: 238 train-loss: 0.030070071968321618\n",
      "[LOG 20210201-12:59:36] training :: epoch: 239 train-loss: 0.029952390405994195\n",
      "[LOG 20210201-12:59:36] training :: epoch: 240 train-loss: 0.029834779552542247\n",
      "[LOG 20210201-12:59:36] training :: epoch: 241 train-loss: 0.029712807315473374\n",
      "[LOG 20210201-12:59:37] training :: epoch: 242 train-loss: 0.02960293159748499\n",
      "[LOG 20210201-12:59:37] training :: epoch: 243 train-loss: 0.029484997001978066\n",
      "[LOG 20210201-12:59:37] training :: epoch: 244 train-loss: 0.0293682967670835\n",
      "[LOG 20210201-12:59:37] training :: epoch: 245 train-loss: 0.02925627836241172\n",
      "[LOG 20210201-12:59:38] training :: epoch: 246 train-loss: 0.029140724872167293\n",
      "[LOG 20210201-12:59:38] training :: epoch: 247 train-loss: 0.0290254161048394\n",
      "[LOG 20210201-12:59:38] training :: epoch: 248 train-loss: 0.028906681431600682\n",
      "[LOG 20210201-12:59:39] training :: epoch: 249 train-loss: 0.028791335984491385\n",
      "[LOG 20210201-12:59:39] training :: epoch: 250 train-loss: 0.028677978911078893\n",
      "[LOG 20210201-12:59:39] training :: epoch: 251 train-loss: 0.028563797044066284\n",
      "[LOG 20210201-12:59:39] training :: epoch: 252 train-loss: 0.028447078397640817\n",
      "[LOG 20210201-12:59:40] training :: epoch: 253 train-loss: 0.028337784612981174\n",
      "[LOG 20210201-12:59:40] training :: epoch: 254 train-loss: 0.028221661511522073\n",
      "[LOG 20210201-12:59:40] training :: epoch: 255 train-loss: 0.0281154653773858\n",
      "[LOG 20210201-12:59:41] training :: epoch: 256 train-loss: 0.02799314998376828\n",
      "[LOG 20210201-12:59:41] training :: epoch: 257 train-loss: 0.027885554931484736\n",
      "[LOG 20210201-12:59:41] training :: epoch: 258 train-loss: 0.02776960154565481\n",
      "[LOG 20210201-12:59:42] training :: epoch: 259 train-loss: 0.02765135058703331\n",
      "[LOG 20210201-12:59:42] training :: epoch: 260 train-loss: 0.027542281322754346\n",
      "[LOG 20210201-12:59:42] training :: epoch: 261 train-loss: 0.027433701504308444\n",
      "[LOG 20210201-12:59:42] training :: epoch: 262 train-loss: 0.027315476121237643\n",
      "[LOG 20210201-12:59:43] training :: epoch: 263 train-loss: 0.027201468483186685\n",
      "[LOG 20210201-12:59:43] training :: epoch: 264 train-loss: 0.02709357476291748\n",
      "[LOG 20210201-12:59:43] training :: epoch: 265 train-loss: 0.026977356809836168\n",
      "[LOG 20210201-12:59:44] training :: epoch: 266 train-loss: 0.02687034311775978\n",
      "[LOG 20210201-12:59:44] training :: epoch: 267 train-loss: 0.0267578293211185\n",
      "[LOG 20210201-12:59:44] training :: epoch: 268 train-loss: 0.02664518213042846\n",
      "[LOG 20210201-12:59:45] training :: epoch: 269 train-loss: 0.026536950913186256\n",
      "[LOG 20210201-12:59:45] training :: epoch: 270 train-loss: 0.026426428857331093\n",
      "[LOG 20210201-12:59:45] training :: epoch: 271 train-loss: 0.026312648318707943\n",
      "[LOG 20210201-12:59:46] training :: epoch: 272 train-loss: 0.02619899895328742\n",
      "[LOG 20210201-12:59:46] training :: epoch: 273 train-loss: 0.02608919530533827\n",
      "[LOG 20210201-12:59:46] training :: epoch: 274 train-loss: 0.025982035538898066\n",
      "[LOG 20210201-12:59:47] training :: epoch: 275 train-loss: 0.025873627083805893\n",
      "[LOG 20210201-12:59:47] training :: epoch: 276 train-loss: 0.025766286998987198\n",
      "[LOG 20210201-12:59:48] training :: epoch: 277 train-loss: 0.025655176132344283\n",
      "[LOG 20210201-12:59:48] training :: epoch: 278 train-loss: 0.025545291244410552\n",
      "[LOG 20210201-12:59:48] training :: epoch: 279 train-loss: 0.025432176028306667\n",
      "[LOG 20210201-12:59:49] training :: epoch: 280 train-loss: 0.025324202357576445\n",
      "[LOG 20210201-12:59:49] training :: epoch: 281 train-loss: 0.025219246816749755\n",
      "[LOG 20210201-12:59:49] training :: epoch: 282 train-loss: 0.02511159860743926\n",
      "[LOG 20210201-12:59:50] training :: epoch: 283 train-loss: 0.025000949605153158\n",
      "[LOG 20210201-12:59:50] training :: epoch: 284 train-loss: 0.024894558251477204\n",
      "[LOG 20210201-12:59:51] training :: epoch: 285 train-loss: 0.024790193813924607\n",
      "[LOG 20210201-12:59:51] training :: epoch: 286 train-loss: 0.02467972018684332\n",
      "[LOG 20210201-12:59:51] training :: epoch: 287 train-loss: 0.02456974510390025\n",
      "[LOG 20210201-12:59:52] training :: epoch: 288 train-loss: 0.02446691942616151\n",
      "[LOG 20210201-12:59:52] training :: epoch: 289 train-loss: 0.02436014271986026\n",
      "[LOG 20210201-12:59:52] training :: epoch: 290 train-loss: 0.02424971078737424\n",
      "[LOG 20210201-12:59:53] training :: epoch: 291 train-loss: 0.024144198673848923\n",
      "[LOG 20210201-12:59:53] training :: epoch: 292 train-loss: 0.024035688274754927\n",
      "[LOG 20210201-12:59:53] training :: epoch: 293 train-loss: 0.023931343036775406\n",
      "[LOG 20210201-12:59:54] training :: epoch: 294 train-loss: 0.023823536932468414\n",
      "[LOG 20210201-12:59:54] training :: epoch: 295 train-loss: 0.023718110357339565\n",
      "[LOG 20210201-12:59:54] training :: epoch: 296 train-loss: 0.023612917974018134\n",
      "[LOG 20210201-12:59:55] training :: epoch: 297 train-loss: 0.023506346803445082\n",
      "[LOG 20210201-12:59:55] training :: epoch: 298 train-loss: 0.023405330470548227\n",
      "[LOG 20210201-12:59:55] training :: epoch: 299 train-loss: 0.023298983533795063\n",
      "[LOG 20210201-12:59:56] training :: epoch: 300 train-loss: 0.023192976529781636\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:00:01] backtest :: total return: -0.7894, monthly-sharpe: -1.2253, daily-sharpe: -0.7574.\n",
      "[LOG 20210201-13:00:03] training :: epoch: 301 train-loss: 0.023093819474944703\n",
      "[LOG 20210201-13:00:04] training :: epoch: 302 train-loss: 0.022986772040335033\n",
      "[LOG 20210201-13:00:04] training :: epoch: 303 train-loss: 0.02288192524932898\n",
      "[LOG 20210201-13:00:04] training :: epoch: 304 train-loss: 0.022778634555064715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:00:05] training :: epoch: 305 train-loss: 0.02267413306981325\n",
      "[LOG 20210201-13:00:05] training :: epoch: 306 train-loss: 0.022568517507841952\n",
      "[LOG 20210201-13:00:05] training :: epoch: 307 train-loss: 0.022468385859750785\n",
      "[LOG 20210201-13:00:06] training :: epoch: 308 train-loss: 0.022362288660728015\n",
      "[LOG 20210201-13:00:06] training :: epoch: 309 train-loss: 0.02226072234603075\n",
      "[LOG 20210201-13:00:06] training :: epoch: 310 train-loss: 0.022157620566968735\n",
      "[LOG 20210201-13:00:06] training :: epoch: 311 train-loss: 0.02205456427943248\n",
      "[LOG 20210201-13:00:07] training :: epoch: 312 train-loss: 0.021951032874102775\n",
      "[LOG 20210201-13:00:07] training :: epoch: 313 train-loss: 0.021854639411545716\n",
      "[LOG 20210201-13:00:07] training :: epoch: 314 train-loss: 0.021750188432633877\n",
      "[LOG 20210201-13:00:08] training :: epoch: 315 train-loss: 0.021645741250652533\n",
      "[LOG 20210201-13:00:08] training :: epoch: 316 train-loss: 0.021545862062619284\n",
      "[LOG 20210201-13:00:09] training :: epoch: 317 train-loss: 0.021447667064001925\n",
      "[LOG 20210201-13:00:09] training :: epoch: 318 train-loss: 0.02134411194576667\n",
      "[LOG 20210201-13:00:09] training :: epoch: 319 train-loss: 0.02124519431247161\n",
      "[LOG 20210201-13:00:10] training :: epoch: 320 train-loss: 0.02114215034704942\n",
      "[LOG 20210201-13:00:10] training :: epoch: 321 train-loss: 0.021039741257062324\n",
      "[LOG 20210201-13:00:10] training :: epoch: 322 train-loss: 0.020937891175540593\n",
      "[LOG 20210201-13:00:11] training :: epoch: 323 train-loss: 0.0208369203341695\n",
      "[LOG 20210201-13:00:11] training :: epoch: 324 train-loss: 0.02073935276040664\n",
      "[LOG 20210201-13:00:11] training :: epoch: 325 train-loss: 0.02063761379283208\n",
      "[LOG 20210201-13:00:12] training :: epoch: 326 train-loss: 0.020544136587816935\n",
      "[LOG 20210201-13:00:12] training :: epoch: 327 train-loss: 0.020443494216753885\n",
      "[LOG 20210201-13:00:12] training :: epoch: 328 train-loss: 0.02034104658434024\n",
      "[LOG 20210201-13:00:13] training :: epoch: 329 train-loss: 0.02024109396510399\n",
      "[LOG 20210201-13:00:13] training :: epoch: 330 train-loss: 0.02014742662700323\n",
      "[LOG 20210201-13:00:14] training :: epoch: 331 train-loss: 0.02004342733954008\n",
      "[LOG 20210201-13:00:14] training :: epoch: 332 train-loss: 0.019945066350583848\n",
      "[LOG 20210201-13:00:14] training :: epoch: 333 train-loss: 0.01984679362235161\n",
      "[LOG 20210201-13:00:14] training :: epoch: 334 train-loss: 0.019752265169070318\n",
      "[LOG 20210201-13:00:15] training :: epoch: 335 train-loss: 0.019649738589158423\n",
      "[LOG 20210201-13:00:15] training :: epoch: 336 train-loss: 0.019552081250227414\n",
      "[LOG 20210201-13:00:16] training :: epoch: 337 train-loss: 0.019455264823941085\n",
      "[LOG 20210201-13:00:16] training :: epoch: 338 train-loss: 0.01936024957551406\n",
      "[LOG 20210201-13:00:16] training :: epoch: 339 train-loss: 0.019262739672110632\n",
      "[LOG 20210201-13:00:16] training :: epoch: 340 train-loss: 0.01916630695072504\n",
      "[LOG 20210201-13:00:17] training :: epoch: 341 train-loss: 0.019066746848133895\n",
      "[LOG 20210201-13:00:17] training :: epoch: 342 train-loss: 0.01897295521428952\n",
      "[LOG 20210201-13:00:17] training :: epoch: 343 train-loss: 0.018877110372369107\n",
      "[LOG 20210201-13:00:17] training :: epoch: 344 train-loss: 0.01878281503629226\n",
      "[LOG 20210201-13:00:18] training :: epoch: 345 train-loss: 0.01868082991299721\n",
      "[LOG 20210201-13:00:18] training :: epoch: 346 train-loss: 0.018588573671877384\n",
      "[LOG 20210201-13:00:18] training :: epoch: 347 train-loss: 0.018491626144028626\n",
      "[LOG 20210201-13:00:19] training :: epoch: 348 train-loss: 0.018399069515558388\n",
      "[LOG 20210201-13:00:19] training :: epoch: 349 train-loss: 0.018303732101160746\n",
      "[LOG 20210201-13:00:19] training :: epoch: 350 train-loss: 0.01820726914761158\n",
      "[LOG 20210201-13:00:20] training :: epoch: 351 train-loss: 0.018112594333405677\n",
      "[LOG 20210201-13:00:20] training :: epoch: 352 train-loss: 0.018018464342905924\n",
      "[LOG 20210201-13:00:20] training :: epoch: 353 train-loss: 0.01792274659069685\n",
      "[LOG 20210201-13:00:21] training :: epoch: 354 train-loss: 0.017829508592302982\n",
      "[LOG 20210201-13:00:21] training :: epoch: 355 train-loss: 0.017737846463345565\n",
      "[LOG 20210201-13:00:21] training :: epoch: 356 train-loss: 0.01764233961987954\n",
      "[LOG 20210201-13:00:22] training :: epoch: 357 train-loss: 0.017548420753043432\n",
      "[LOG 20210201-13:00:22] training :: epoch: 358 train-loss: 0.01745429806984388\n",
      "[LOG 20210201-13:00:22] training :: epoch: 359 train-loss: 0.017362351672580607\n",
      "[LOG 20210201-13:00:23] training :: epoch: 360 train-loss: 0.01727011914436634\n",
      "[LOG 20210201-13:00:23] training :: epoch: 361 train-loss: 0.017177381409475438\n",
      "[LOG 20210201-13:00:23] training :: epoch: 362 train-loss: 0.01708575810950536\n",
      "[LOG 20210201-13:00:24] training :: epoch: 363 train-loss: 0.01699223340703891\n",
      "[LOG 20210201-13:00:24] training :: epoch: 364 train-loss: 0.016902395953925755\n",
      "[LOG 20210201-13:00:24] training :: epoch: 365 train-loss: 0.016810063630915605\n",
      "[LOG 20210201-13:00:25] training :: epoch: 366 train-loss: 0.01671832284102073\n",
      "[LOG 20210201-13:00:25] training :: epoch: 367 train-loss: 0.0166257549650394\n",
      "[LOG 20210201-13:00:25] training :: epoch: 368 train-loss: 0.016533462330698967\n",
      "[LOG 20210201-13:00:26] training :: epoch: 369 train-loss: 0.016445115853387576\n",
      "[LOG 20210201-13:00:26] training :: epoch: 370 train-loss: 0.016354850493371487\n",
      "[LOG 20210201-13:00:26] training :: epoch: 371 train-loss: 0.0162606263676515\n",
      "[LOG 20210201-13:00:26] training :: epoch: 372 train-loss: 0.016171183580389388\n",
      "[LOG 20210201-13:00:27] training :: epoch: 373 train-loss: 0.01608274719462945\n",
      "[LOG 20210201-13:00:27] training :: epoch: 374 train-loss: 0.01599131474414697\n",
      "[LOG 20210201-13:00:27] training :: epoch: 375 train-loss: 0.015901831480172966\n",
      "[LOG 20210201-13:00:28] training :: epoch: 376 train-loss: 0.015813364969709746\n",
      "[LOG 20210201-13:00:28] training :: epoch: 377 train-loss: 0.015723935722445067\n",
      "[LOG 20210201-13:00:28] training :: epoch: 378 train-loss: 0.015631630873450868\n",
      "[LOG 20210201-13:00:29] training :: epoch: 379 train-loss: 0.015544264183308069\n",
      "[LOG 20210201-13:00:29] training :: epoch: 380 train-loss: 0.015455604459230717\n",
      "[LOG 20210201-13:00:29] training :: epoch: 381 train-loss: 0.015366811269464401\n",
      "[LOG 20210201-13:00:30] training :: epoch: 382 train-loss: 0.01527863611968664\n",
      "[LOG 20210201-13:00:30] training :: epoch: 383 train-loss: 0.015191023345463552\n",
      "[LOG 20210201-13:00:30] training :: epoch: 384 train-loss: 0.015103683842775913\n",
      "[LOG 20210201-13:00:31] training :: epoch: 385 train-loss: 0.015014656282101687\n",
      "[LOG 20210201-13:00:31] training :: epoch: 386 train-loss: 0.014928113275135938\n",
      "[LOG 20210201-13:00:31] training :: epoch: 387 train-loss: 0.014839984691486908\n",
      "[LOG 20210201-13:00:32] training :: epoch: 388 train-loss: 0.014754393711113013\n",
      "[LOG 20210201-13:00:32] training :: epoch: 389 train-loss: 0.014668147151286785\n",
      "[LOG 20210201-13:00:32] training :: epoch: 390 train-loss: 0.014580272532139834\n",
      "[LOG 20210201-13:00:33] training :: epoch: 391 train-loss: 0.014496405752232442\n",
      "[LOG 20210201-13:00:33] training :: epoch: 392 train-loss: 0.014407001364116486\n",
      "[LOG 20210201-13:00:33] training :: epoch: 393 train-loss: 0.01432462864053937\n",
      "[LOG 20210201-13:00:34] training :: epoch: 394 train-loss: 0.014234831413397422\n",
      "[LOG 20210201-13:00:34] training :: epoch: 395 train-loss: 0.014151773594606381\n",
      "[LOG 20210201-13:00:34] training :: epoch: 396 train-loss: 0.014068726593485245\n",
      "[LOG 20210201-13:00:35] training :: epoch: 397 train-loss: 0.013982115814892145\n",
      "[LOG 20210201-13:00:35] training :: epoch: 398 train-loss: 0.01389784781405559\n",
      "[LOG 20210201-13:00:35] training :: epoch: 399 train-loss: 0.01381317196557155\n",
      "[LOG 20210201-13:00:36] training :: epoch: 400 train-loss: 0.013730150325080523\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:00:41] backtest :: total return: -0.7894, monthly-sharpe: -1.2253, daily-sharpe: -0.7574.\n",
      "[LOG 20210201-13:00:42] training :: epoch: 401 train-loss: 0.013641873410401436\n",
      "[LOG 20210201-13:00:43] training :: epoch: 402 train-loss: 0.013564140070229769\n",
      "[LOG 20210201-13:00:43] training :: epoch: 403 train-loss: 0.013473798914884146\n",
      "[LOG 20210201-13:00:43] training :: epoch: 404 train-loss: 0.013394074908529337\n",
      "[LOG 20210201-13:00:44] training :: epoch: 405 train-loss: 0.013311125767918734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:00:44] training :: epoch: 406 train-loss: 0.01322631466274078\n",
      "[LOG 20210201-13:00:44] training :: epoch: 407 train-loss: 0.013148760064863242\n",
      "[LOG 20210201-13:00:45] training :: epoch: 408 train-loss: 0.013064001543590656\n",
      "[LOG 20210201-13:00:45] training :: epoch: 409 train-loss: 0.012982435345363159\n",
      "[LOG 20210201-13:00:45] training :: epoch: 410 train-loss: 0.012901643828417245\n",
      "[LOG 20210201-13:00:46] training :: epoch: 411 train-loss: 0.012822373483616572\n",
      "[LOG 20210201-13:00:46] training :: epoch: 412 train-loss: 0.012739789851296406\n",
      "[LOG 20210201-13:00:46] training :: epoch: 413 train-loss: 0.012658174579533247\n",
      "[LOG 20210201-13:00:46] training :: epoch: 414 train-loss: 0.012579697327545056\n",
      "[LOG 20210201-13:00:47] training :: epoch: 415 train-loss: 0.012498962800376691\n",
      "[LOG 20210201-13:00:47] training :: epoch: 416 train-loss: 0.012414298485964537\n",
      "[LOG 20210201-13:00:47] training :: epoch: 417 train-loss: 0.012336791564638797\n",
      "[LOG 20210201-13:00:47] training :: epoch: 418 train-loss: 0.012256743171467232\n",
      "[LOG 20210201-13:00:48] training :: epoch: 419 train-loss: 0.012175272541263929\n",
      "[LOG 20210201-13:00:48] training :: epoch: 420 train-loss: 0.012093179501019992\n",
      "[LOG 20210201-13:00:48] training :: epoch: 421 train-loss: 0.012019437594482532\n",
      "[LOG 20210201-13:00:49] training :: epoch: 422 train-loss: 0.011943429636840638\n",
      "[LOG 20210201-13:00:49] training :: epoch: 423 train-loss: 0.011863367811131936\n",
      "[LOG 20210201-13:00:49] training :: epoch: 424 train-loss: 0.01178690418601036\n",
      "[LOG 20210201-13:00:50] training :: epoch: 425 train-loss: 0.011707105279828493\n",
      "[LOG 20210201-13:00:50] training :: epoch: 426 train-loss: 0.01162982166100007\n",
      "[LOG 20210201-13:00:50] training :: epoch: 427 train-loss: 0.011553064060325805\n",
      "[LOG 20210201-13:00:51] training :: epoch: 428 train-loss: 0.011474788117294129\n",
      "[LOG 20210201-13:00:51] training :: epoch: 429 train-loss: 0.011398088903381275\n",
      "[LOG 20210201-13:00:52] training :: epoch: 430 train-loss: 0.011322352139709087\n",
      "[LOG 20210201-13:00:52] training :: epoch: 431 train-loss: 0.011244555577062644\n",
      "[LOG 20210201-13:00:52] training :: epoch: 432 train-loss: 0.011168663640721487\n",
      "[LOG 20210201-13:00:53] training :: epoch: 433 train-loss: 0.011092717974231793\n",
      "[LOG 20210201-13:00:53] training :: epoch: 434 train-loss: 0.011019787500397516\n",
      "[LOG 20210201-13:00:53] training :: epoch: 435 train-loss: 0.010941696353256702\n",
      "[LOG 20210201-13:00:54] training :: epoch: 436 train-loss: 0.01086783197780068\n",
      "[LOG 20210201-13:00:54] training :: epoch: 437 train-loss: 0.010793796919572812\n",
      "[LOG 20210201-13:00:54] training :: epoch: 438 train-loss: 0.010718206660105633\n",
      "[LOG 20210201-13:00:55] training :: epoch: 439 train-loss: 0.010642734535325032\n",
      "[LOG 20210201-13:00:55] training :: epoch: 440 train-loss: 0.010568574214210877\n",
      "[LOG 20210201-13:00:55] training :: epoch: 441 train-loss: 0.010497550444247631\n",
      "[LOG 20210201-13:00:56] training :: epoch: 442 train-loss: 0.010425014278063407\n",
      "[LOG 20210201-13:00:56] training :: epoch: 443 train-loss: 0.010352329888309423\n",
      "[LOG 20210201-13:00:56] training :: epoch: 444 train-loss: 0.010278468091900531\n",
      "[LOG 20210201-13:00:56] training :: epoch: 445 train-loss: 0.010206459305034233\n",
      "[LOG 20210201-13:00:57] training :: epoch: 446 train-loss: 0.01013526119864904\n",
      "[LOG 20210201-13:00:57] training :: epoch: 447 train-loss: 0.01006281988408703\n",
      "[LOG 20210201-13:00:57] training :: epoch: 448 train-loss: 0.009991109227904907\n",
      "[LOG 20210201-13:00:58] training :: epoch: 449 train-loss: 0.009921220417779226\n",
      "[LOG 20210201-13:00:58] training :: epoch: 450 train-loss: 0.009849149339760725\n",
      "[LOG 20210201-13:00:59] training :: epoch: 451 train-loss: 0.00977697610281981\n",
      "[LOG 20210201-13:00:59] training :: epoch: 452 train-loss: 0.009707662563484449\n",
      "[LOG 20210201-13:00:59] training :: epoch: 453 train-loss: 0.009638937512555948\n",
      "[LOG 20210201-13:01:00] training :: epoch: 454 train-loss: 0.009569521993398666\n",
      "[LOG 20210201-13:01:00] training :: epoch: 455 train-loss: 0.009501080279453443\n",
      "[LOG 20210201-13:01:00] training :: epoch: 456 train-loss: 0.009429370124752704\n",
      "[LOG 20210201-13:01:01] training :: epoch: 457 train-loss: 0.009363169996784283\n",
      "[LOG 20210201-13:01:01] training :: epoch: 458 train-loss: 0.009290869276110943\n",
      "[LOG 20210201-13:01:01] training :: epoch: 459 train-loss: 0.009220793664168853\n",
      "[LOG 20210201-13:01:02] training :: epoch: 460 train-loss: 0.00915539307663074\n",
      "[LOG 20210201-13:01:02] training :: epoch: 461 train-loss: 0.009086813168743482\n",
      "[LOG 20210201-13:01:02] training :: epoch: 462 train-loss: 0.009018991715632953\n",
      "[LOG 20210201-13:01:02] training :: epoch: 463 train-loss: 0.008955734280439524\n",
      "[LOG 20210201-13:01:03] training :: epoch: 464 train-loss: 0.008885435043619229\n",
      "[LOG 20210201-13:01:03] training :: epoch: 465 train-loss: 0.008819857361511542\n",
      "[LOG 20210201-13:01:03] training :: epoch: 466 train-loss: 0.008756299431507405\n",
      "[LOG 20210201-13:01:04] training :: epoch: 467 train-loss: 0.008688899115301095\n",
      "[LOG 20210201-13:01:04] training :: epoch: 468 train-loss: 0.008623761948771201\n",
      "[LOG 20210201-13:01:04] training :: epoch: 469 train-loss: 0.008554338907393126\n",
      "[LOG 20210201-13:01:04] training :: epoch: 470 train-loss: 0.008491931733890222\n",
      "[LOG 20210201-13:01:05] training :: epoch: 471 train-loss: 0.0084266525048476\n",
      "[LOG 20210201-13:01:05] training :: epoch: 472 train-loss: 0.008362216910777183\n",
      "[LOG 20210201-13:01:06] training :: epoch: 473 train-loss: 0.008295938336791901\n",
      "[LOG 20210201-13:01:06] training :: epoch: 474 train-loss: 0.008236095846558992\n",
      "[LOG 20210201-13:01:06] training :: epoch: 475 train-loss: 0.008170817333918352\n",
      "[LOG 20210201-13:01:06] training :: epoch: 476 train-loss: 0.008106764692526598\n",
      "[LOG 20210201-13:01:07] training :: epoch: 477 train-loss: 0.008044218465399284\n",
      "[LOG 20210201-13:01:07] training :: epoch: 478 train-loss: 0.007980264436740141\n",
      "[LOG 20210201-13:01:07] training :: epoch: 479 train-loss: 0.007918595246827373\n",
      "[LOG 20210201-13:01:08] training :: epoch: 480 train-loss: 0.00785759469279303\n",
      "[LOG 20210201-13:01:08] training :: epoch: 481 train-loss: 0.007794945232140331\n",
      "[LOG 20210201-13:01:08] training :: epoch: 482 train-loss: 0.007734338000703316\n",
      "[LOG 20210201-13:01:09] training :: epoch: 483 train-loss: 0.007673517460576617\n",
      "[LOG 20210201-13:01:09] training :: epoch: 484 train-loss: 0.00761136647242193\n",
      "[LOG 20210201-13:01:09] training :: epoch: 485 train-loss: 0.007553995712302052\n",
      "[LOG 20210201-13:01:10] training :: epoch: 486 train-loss: 0.00749054800074261\n",
      "[LOG 20210201-13:01:10] training :: epoch: 487 train-loss: 0.00743196655709583\n",
      "[LOG 20210201-13:01:10] training :: epoch: 488 train-loss: 0.007373504949590335\n",
      "[LOG 20210201-13:01:11] training :: epoch: 489 train-loss: 0.0073125575884030415\n",
      "[LOG 20210201-13:01:11] training :: epoch: 490 train-loss: 0.007253593801019283\n",
      "[LOG 20210201-13:01:11] training :: epoch: 491 train-loss: 0.00719500039345943\n",
      "[LOG 20210201-13:01:12] training :: epoch: 492 train-loss: 0.007137300321259177\n",
      "[LOG 20210201-13:01:12] training :: epoch: 493 train-loss: 0.0070775369397149636\n",
      "[LOG 20210201-13:01:12] training :: epoch: 494 train-loss: 0.007020613375621347\n",
      "[LOG 20210201-13:01:13] training :: epoch: 495 train-loss: 0.006965601386932226\n",
      "[LOG 20210201-13:01:13] training :: epoch: 496 train-loss: 0.006908361101523042\n",
      "[LOG 20210201-13:01:13] training :: epoch: 497 train-loss: 0.006850764621049166\n",
      "[LOG 20210201-13:01:14] training :: epoch: 498 train-loss: 0.006795473235587661\n",
      "[LOG 20210201-13:01:14] training :: epoch: 499 train-loss: 0.0067383290734142065\n",
      "[LOG 20210201-13:01:14] training :: epoch: 500 train-loss: 0.00668318123699954\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:01:19] backtest :: total return: -0.7894, monthly-sharpe: -1.2253, daily-sharpe: -0.7574.\n",
      "[LOG 20210201-13:01:21] training :: epoch: 501 train-loss: 0.006625487397496517\n",
      "[LOG 20210201-13:01:21] training :: epoch: 502 train-loss: 0.006573366538549845\n",
      "[LOG 20210201-13:01:22] training :: epoch: 503 train-loss: 0.006519213480015214\n",
      "[LOG 20210201-13:01:22] training :: epoch: 504 train-loss: 0.006466336584148498\n",
      "[LOG 20210201-13:01:22] training :: epoch: 505 train-loss: 0.006409783059587846\n",
      "[LOG 20210201-13:01:23] training :: epoch: 506 train-loss: 0.006355247370755443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:01:23] training :: epoch: 507 train-loss: 0.006302219493171344\n",
      "[LOG 20210201-13:01:23] training :: epoch: 508 train-loss: 0.006250436274478069\n",
      "[LOG 20210201-13:01:23] training :: epoch: 509 train-loss: 0.006196783694366996\n",
      "[LOG 20210201-13:01:24] training :: epoch: 510 train-loss: 0.006143343151332094\n",
      "[LOG 20210201-13:01:24] training :: epoch: 511 train-loss: 0.006093316711485386\n",
      "[LOG 20210201-13:01:24] training :: epoch: 512 train-loss: 0.006041662086947606\n",
      "[LOG 20210201-13:01:25] training :: epoch: 513 train-loss: 0.005990779313903589\n",
      "[LOG 20210201-13:01:25] training :: epoch: 514 train-loss: 0.005939602529486785\n",
      "[LOG 20210201-13:01:25] training :: epoch: 515 train-loss: 0.005888224901774755\n",
      "[LOG 20210201-13:01:26] training :: epoch: 516 train-loss: 0.005840605214381447\n",
      "[LOG 20210201-13:01:26] training :: epoch: 517 train-loss: 0.005790190663761818\n",
      "[LOG 20210201-13:01:26] training :: epoch: 518 train-loss: 0.0057420005592016075\n",
      "[LOG 20210201-13:01:27] training :: epoch: 519 train-loss: 0.005690031899855687\n",
      "[LOG 20210201-13:01:27] training :: epoch: 520 train-loss: 0.005643360674954378\n",
      "[LOG 20210201-13:01:27] training :: epoch: 521 train-loss: 0.005594263318926096\n",
      "[LOG 20210201-13:01:28] training :: epoch: 522 train-loss: 0.005548061373142095\n",
      "[LOG 20210201-13:01:28] training :: epoch: 523 train-loss: 0.005499549675732851\n",
      "[LOG 20210201-13:01:28] training :: epoch: 524 train-loss: 0.005452908288974028\n",
      "[LOG 20210201-13:01:29] training :: epoch: 525 train-loss: 0.005405262470818483\n",
      "[LOG 20210201-13:01:29] training :: epoch: 526 train-loss: 0.005358990926582079\n",
      "[LOG 20210201-13:01:29] training :: epoch: 527 train-loss: 0.005311877484648273\n",
      "[LOG 20210201-13:01:29] training :: epoch: 528 train-loss: 0.00526779954536603\n",
      "[LOG 20210201-13:01:30] training :: epoch: 529 train-loss: 0.005220062027756984\n",
      "[LOG 20210201-13:01:30] training :: epoch: 530 train-loss: 0.0051770548228747566\n",
      "[LOG 20210201-13:01:30] training :: epoch: 531 train-loss: 0.005131827649445488\n",
      "[LOG 20210201-13:01:31] training :: epoch: 532 train-loss: 0.0050859481203728\n",
      "[LOG 20210201-13:01:31] training :: epoch: 533 train-loss: 0.005042699935774391\n",
      "[LOG 20210201-13:01:31] training :: epoch: 534 train-loss: 0.004999254651081104\n",
      "[LOG 20210201-13:01:31] training :: epoch: 535 train-loss: 0.00495646745324708\n",
      "[LOG 20210201-13:01:32] training :: epoch: 536 train-loss: 0.004915550960084567\n",
      "[LOG 20210201-13:01:32] training :: epoch: 537 train-loss: 0.004872847276811416\n",
      "[LOG 20210201-13:01:32] training :: epoch: 538 train-loss: 0.004829913306121643\n",
      "[LOG 20210201-13:01:32] training :: epoch: 539 train-loss: 0.004788870994861309\n",
      "[LOG 20210201-13:01:33] training :: epoch: 540 train-loss: 0.004748680198994966\n",
      "[LOG 20210201-13:01:33] training :: epoch: 541 train-loss: 0.004706844347170913\n",
      "[LOG 20210201-13:01:33] training :: epoch: 542 train-loss: 0.004666287631082993\n",
      "[LOG 20210201-13:01:34] training :: epoch: 543 train-loss: 0.004624880599574401\n",
      "[LOG 20210201-13:01:34] training :: epoch: 544 train-loss: 0.004585816650293195\n",
      "[LOG 20210201-13:01:34] training :: epoch: 545 train-loss: 0.004548032535240054\n",
      "[LOG 20210201-13:01:34] training :: epoch: 546 train-loss: 0.004507429378393751\n",
      "[LOG 20210201-13:01:35] training :: epoch: 547 train-loss: 0.0044699647965339515\n",
      "[LOG 20210201-13:01:35] training :: epoch: 548 train-loss: 0.0044301979494496034\n",
      "[LOG 20210201-13:01:35] training :: epoch: 549 train-loss: 0.0043949654791504145\n",
      "[LOG 20210201-13:01:36] training :: epoch: 550 train-loss: 0.004356000733633454\n",
      "[LOG 20210201-13:01:36] training :: epoch: 551 train-loss: 0.004319068707095889\n",
      "[LOG 20210201-13:01:36] training :: epoch: 552 train-loss: 0.004284139883776124\n",
      "[LOG 20210201-13:01:37] training :: epoch: 553 train-loss: 0.0042486109126072666\n",
      "[LOG 20210201-13:01:37] training :: epoch: 554 train-loss: 0.004212292269445383\n",
      "[LOG 20210201-13:01:37] training :: epoch: 555 train-loss: 0.004176660745333021\n",
      "[LOG 20210201-13:01:38] training :: epoch: 556 train-loss: 0.0041415286966814445\n",
      "[LOG 20210201-13:01:38] training :: epoch: 557 train-loss: 0.004107104900937814\n",
      "[LOG 20210201-13:01:38] training :: epoch: 558 train-loss: 0.004073409023336493\n",
      "[LOG 20210201-13:01:39] training :: epoch: 559 train-loss: 0.004038461860126028\n",
      "[LOG 20210201-13:01:39] training :: epoch: 560 train-loss: 0.004005508515267418\n",
      "[LOG 20210201-13:01:39] training :: epoch: 561 train-loss: 0.003972912860962634\n",
      "[LOG 20210201-13:01:40] training :: epoch: 562 train-loss: 0.003939993258637304\n",
      "[LOG 20210201-13:01:40] training :: epoch: 563 train-loss: 0.003908036813999598\n",
      "[LOG 20210201-13:01:40] training :: epoch: 564 train-loss: 0.0038756182005342385\n",
      "[LOG 20210201-13:01:40] training :: epoch: 565 train-loss: 0.003844614292924794\n",
      "[LOG 20210201-13:01:41] training :: epoch: 566 train-loss: 0.0038154854152638177\n",
      "[LOG 20210201-13:01:41] training :: epoch: 567 train-loss: 0.0037840620184747074\n",
      "[LOG 20210201-13:01:41] training :: epoch: 568 train-loss: 0.0037529322838124176\n",
      "[LOG 20210201-13:01:41] training :: epoch: 569 train-loss: 0.0037258423399180174\n",
      "[LOG 20210201-13:01:42] training :: epoch: 570 train-loss: 0.0036948132776440335\n",
      "[LOG 20210201-13:01:42] training :: epoch: 571 train-loss: 0.003666107847283666\n",
      "[LOG 20210201-13:01:42] training :: epoch: 572 train-loss: 0.0036372033109028754\n",
      "[LOG 20210201-13:01:43] training :: epoch: 573 train-loss: 0.0036094234539912297\n",
      "[LOG 20210201-13:01:43] training :: epoch: 574 train-loss: 0.003583024264886402\n",
      "[LOG 20210201-13:01:43] training :: epoch: 575 train-loss: 0.0035568470171151254\n",
      "[LOG 20210201-13:01:43] training :: epoch: 576 train-loss: 0.0035286236088722944\n",
      "[LOG 20210201-13:01:44] training :: epoch: 577 train-loss: 0.0035019991919398308\n",
      "[LOG 20210201-13:01:44] training :: epoch: 578 train-loss: 0.003477137589540619\n",
      "[LOG 20210201-13:01:45] training :: epoch: 579 train-loss: 0.0034527879638167527\n",
      "[LOG 20210201-13:01:45] training :: epoch: 580 train-loss: 0.0034278199757234408\n",
      "[LOG 20210201-13:01:45] training :: epoch: 581 train-loss: 0.0034041864475092064\n",
      "[LOG 20210201-13:01:46] training :: epoch: 582 train-loss: 0.003378134767095057\n",
      "[LOG 20210201-13:01:46] training :: epoch: 583 train-loss: 0.003355899163020345\n",
      "[LOG 20210201-13:01:46] training :: epoch: 584 train-loss: 0.0033326633364105453\n",
      "[LOG 20210201-13:01:46] training :: epoch: 585 train-loss: 0.003309385313724096\n",
      "[LOG 20210201-13:01:47] training :: epoch: 586 train-loss: 0.003285507895410634\n",
      "[LOG 20210201-13:01:47] training :: epoch: 587 train-loss: 0.0032654695141200838\n",
      "[LOG 20210201-13:01:47] training :: epoch: 588 train-loss: 0.0032427556400832077\n",
      "[LOG 20210201-13:01:48] training :: epoch: 589 train-loss: 0.003222334529989614\n",
      "[LOG 20210201-13:01:48] training :: epoch: 590 train-loss: 0.003202843372351848\n",
      "[LOG 20210201-13:01:48] training :: epoch: 591 train-loss: 0.0031808002475792398\n",
      "[LOG 20210201-13:01:49] training :: epoch: 592 train-loss: 0.003163065850877991\n",
      "[LOG 20210201-13:01:49] training :: epoch: 593 train-loss: 0.003144051951284592\n",
      "[LOG 20210201-13:01:49] training :: epoch: 594 train-loss: 0.003125064280958703\n",
      "[LOG 20210201-13:01:49] training :: epoch: 595 train-loss: 0.003105528598937851\n",
      "[LOG 20210201-13:01:50] training :: epoch: 596 train-loss: 0.0030888077140284274\n",
      "[LOG 20210201-13:01:50] training :: epoch: 597 train-loss: 0.003071499737696006\n",
      "[LOG 20210201-13:01:50] training :: epoch: 598 train-loss: 0.003053261790997707\n",
      "[LOG 20210201-13:01:51] training :: epoch: 599 train-loss: 0.003035935412089412\n",
      "[LOG 20210201-13:01:51] training :: epoch: 600 train-loss: 0.0030217141611501575\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:01:55] backtest :: total return: 2.6189, monthly-sharpe: 1.1133, daily-sharpe: 0.7575.\n",
      "[LOG 20210201-13:01:57] training :: epoch: 601 train-loss: 0.003005842364823016\n",
      "[LOG 20210201-13:01:57] training :: epoch: 602 train-loss: 0.0029920739963507424\n",
      "[LOG 20210201-13:01:57] training :: epoch: 603 train-loss: 0.002975688871139517\n",
      "[LOG 20210201-13:01:57] training :: epoch: 604 train-loss: 0.002962176401454669\n",
      "[LOG 20210201-13:01:58] training :: epoch: 605 train-loss: 0.002949581168090495\n",
      "[LOG 20210201-13:01:58] training :: epoch: 606 train-loss: 0.0029352083802223206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:01:58] training :: epoch: 607 train-loss: 0.0029235832715550293\n",
      "[LOG 20210201-13:01:59] training :: epoch: 608 train-loss: 0.0029102462576702237\n",
      "[LOG 20210201-13:01:59] training :: epoch: 609 train-loss: 0.0028988466007061875\n",
      "[LOG 20210201-13:01:59] training :: epoch: 610 train-loss: 0.0028870209173944136\n",
      "[LOG 20210201-13:01:59] training :: epoch: 611 train-loss: 0.002875930307289729\n",
      "[LOG 20210201-13:02:00] training :: epoch: 612 train-loss: 0.002863979128261025\n",
      "[LOG 20210201-13:02:00] training :: epoch: 613 train-loss: 0.0028559755122002503\n",
      "[LOG 20210201-13:02:00] training :: epoch: 614 train-loss: 0.002844355104920956\n",
      "[LOG 20210201-13:02:01] training :: epoch: 615 train-loss: 0.0028383326358520067\n",
      "[LOG 20210201-13:02:01] training :: epoch: 616 train-loss: 0.00282823140374743\n",
      "[LOG 20210201-13:02:01] training :: epoch: 617 train-loss: 0.0028207832051870916\n",
      "[LOG 20210201-13:02:02] training :: epoch: 618 train-loss: 0.0028126875046067513\n",
      "[LOG 20210201-13:02:02] training :: epoch: 619 train-loss: 0.002805663890635165\n",
      "[LOG 20210201-13:02:02] training :: epoch: 620 train-loss: 0.0027979246716803084\n",
      "[LOG 20210201-13:02:02] training :: epoch: 621 train-loss: 0.002792379165139909\n",
      "[LOG 20210201-13:02:03] training :: epoch: 622 train-loss: 0.0027853629659288204\n",
      "[LOG 20210201-13:02:03] training :: epoch: 623 train-loss: 0.002781157149001956\n",
      "[LOG 20210201-13:02:03] training :: epoch: 624 train-loss: 0.00277520555895395\n",
      "[LOG 20210201-13:02:04] training :: epoch: 625 train-loss: 0.002772699656466452\n",
      "[LOG 20210201-13:02:04] training :: epoch: 626 train-loss: 0.0027668608549552467\n",
      "[LOG 20210201-13:02:04] training :: epoch: 627 train-loss: 0.0027656254466049946\n",
      "[LOG 20210201-13:02:04] training :: epoch: 628 train-loss: 0.0027617021427991297\n",
      "[LOG 20210201-13:02:05] training :: epoch: 629 train-loss: 0.0027595907001971053\n",
      "[LOG 20210201-13:02:05] training :: epoch: 630 train-loss: 0.0027575571484004077\n",
      "[LOG 20210201-13:02:05] training :: epoch: 631 train-loss: 0.0027556412298089038\n",
      "[LOG 20210201-13:02:05] training :: epoch: 632 train-loss: 0.0027544286299067047\n",
      "[LOG 20210201-13:02:06] training :: epoch: 633 train-loss: 0.002753008774911555\n",
      "[LOG 20210201-13:02:06] training :: epoch: 634 train-loss: 0.002753479862943865\n",
      "[LOG 20210201-13:02:06] training :: epoch: 635 train-loss: 0.002753190669374397\n",
      "[LOG 20210201-13:02:07] training :: epoch: 636 train-loss: 0.002753704132583852\n",
      "[LOG 20210201-13:02:07] training :: epoch: 637 train-loss: 0.0027544584680491914\n",
      "[LOG 20210201-13:02:07] training :: epoch: 638 train-loss: 0.0027560781723318193\n",
      "[LOG 20210201-13:02:07] training :: epoch: 639 train-loss: 0.002760447722931321\n",
      "[LOG 20210201-13:02:08] training :: epoch: 640 train-loss: 0.002760853045261823\n",
      "[LOG 20210201-13:02:08] training :: epoch: 641 train-loss: 0.002763902669987426\n",
      "[LOG 20210201-13:02:08] training :: epoch: 642 train-loss: 0.002770893813039248\n",
      "[LOG 20210201-13:02:09] training :: epoch: 643 train-loss: 0.0027729441465523383\n",
      "[LOG 20210201-13:02:09] training :: epoch: 644 train-loss: 0.0027789948149942434\n",
      "[LOG 20210201-13:02:09] training :: epoch: 645 train-loss: 0.0027841610045960317\n",
      "[LOG 20210201-13:02:09] training :: epoch: 646 train-loss: 0.002790356654888735\n",
      "[LOG 20210201-13:02:10] training :: epoch: 647 train-loss: 0.002793570255072644\n",
      "[LOG 20210201-13:02:10] training :: epoch: 648 train-loss: 0.0028010592975008944\n",
      "[LOG 20210201-13:02:10] training :: epoch: 649 train-loss: 0.002808604693899934\n",
      "[LOG 20210201-13:02:10] training :: epoch: 650 train-loss: 0.0028149838416049113\n",
      "[LOG 20210201-13:02:11] training :: epoch: 651 train-loss: 0.002822748510740124\n",
      "[LOG 20210201-13:02:11] training :: epoch: 652 train-loss: 0.0028339417902036356\n",
      "[LOG 20210201-13:02:11] training :: epoch: 653 train-loss: 0.00284067784042026\n",
      "[LOG 20210201-13:02:12] training :: epoch: 654 train-loss: 0.002850500069773541\n",
      "[LOG 20210201-13:02:12] training :: epoch: 655 train-loss: 0.00286115667460343\n",
      "[LOG 20210201-13:02:12] training :: epoch: 656 train-loss: 0.002871605415398685\n",
      "[LOG 20210201-13:02:13] training :: epoch: 657 train-loss: 0.0028843494723192775\n",
      "[LOG 20210201-13:02:13] training :: epoch: 658 train-loss: 0.0028958708638898456\n",
      "[LOG 20210201-13:02:13] training :: epoch: 659 train-loss: 0.0029068389871659186\n",
      "[LOG 20210201-13:02:14] training :: epoch: 660 train-loss: 0.002917667501606047\n",
      "[LOG 20210201-13:02:14] training :: epoch: 661 train-loss: 0.002931462636647316\n",
      "[LOG 20210201-13:02:14] training :: epoch: 662 train-loss: 0.0029443080644481457\n",
      "[LOG 20210201-13:02:15] training :: epoch: 663 train-loss: 0.0029588584394122544\n",
      "[LOG 20210201-13:02:15] training :: epoch: 664 train-loss: 0.002972492939219452\n",
      "[LOG 20210201-13:02:15] training :: epoch: 665 train-loss: 0.002986438751507264\n",
      "[LOG 20210201-13:02:16] training :: epoch: 666 train-loss: 0.0030031540574362646\n",
      "[LOG 20210201-13:02:16] training :: epoch: 667 train-loss: 0.0030195668447189606\n",
      "[LOG 20210201-13:02:17] training :: epoch: 668 train-loss: 0.003033906678096033\n",
      "[LOG 20210201-13:02:17] training :: epoch: 669 train-loss: 0.0030513935489580035\n",
      "[LOG 20210201-13:02:17] training :: epoch: 670 train-loss: 0.0030689049028576566\n",
      "[LOG 20210201-13:02:18] training :: epoch: 671 train-loss: 0.00308694419128677\n",
      "[LOG 20210201-13:02:18] training :: epoch: 672 train-loss: 0.0031053739725253903\n",
      "[LOG 20210201-13:02:18] training :: epoch: 673 train-loss: 0.0031252119224518538\n",
      "[LOG 20210201-13:02:19] training :: epoch: 674 train-loss: 0.0031435808542972575\n",
      "[LOG 20210201-13:02:19] training :: epoch: 675 train-loss: 0.0031625626859470056\n",
      "[LOG 20210201-13:02:19] training :: epoch: 676 train-loss: 0.0031833609637732688\n",
      "[LOG 20210201-13:02:19] training :: epoch: 677 train-loss: 0.0032022993098228024\n",
      "[LOG 20210201-13:02:20] training :: epoch: 678 train-loss: 0.003224722244060383\n",
      "[LOG 20210201-13:02:20] training :: epoch: 679 train-loss: 0.003245572210289538\n",
      "[LOG 20210201-13:02:21] training :: epoch: 680 train-loss: 0.0032686897589323614\n",
      "[LOG 20210201-13:02:21] training :: epoch: 681 train-loss: 0.003289559820237068\n",
      "[LOG 20210201-13:02:21] training :: epoch: 682 train-loss: 0.0033125473598304847\n",
      "[LOG 20210201-13:02:22] training :: epoch: 683 train-loss: 0.0033359937459373702\n",
      "[LOG 20210201-13:02:22] training :: epoch: 684 train-loss: 0.00336027280606616\n",
      "[LOG 20210201-13:02:22] training :: epoch: 685 train-loss: 0.0033843030239670323\n",
      "[LOG 20210201-13:02:23] training :: epoch: 686 train-loss: 0.0034111109830868933\n",
      "[LOG 20210201-13:02:23] training :: epoch: 687 train-loss: 0.00343669699325871\n",
      "[LOG 20210201-13:02:23] training :: epoch: 688 train-loss: 0.003461103823680717\n",
      "[LOG 20210201-13:02:24] training :: epoch: 689 train-loss: 0.003486955090640829\n",
      "[LOG 20210201-13:02:24] training :: epoch: 690 train-loss: 0.0035119872683515917\n",
      "[LOG 20210201-13:02:24] training :: epoch: 691 train-loss: 0.0035404342566975034\n",
      "[LOG 20210201-13:02:25] training :: epoch: 692 train-loss: 0.003568425335778067\n",
      "[LOG 20210201-13:02:25] training :: epoch: 693 train-loss: 0.0035957741694381605\n",
      "[LOG 20210201-13:02:25] training :: epoch: 694 train-loss: 0.003624503375389255\n",
      "[LOG 20210201-13:02:25] training :: epoch: 695 train-loss: 0.0036529839808981004\n",
      "[LOG 20210201-13:02:26] training :: epoch: 696 train-loss: 0.003683557456287627\n",
      "[LOG 20210201-13:02:26] training :: epoch: 697 train-loss: 0.0037122537793878177\n",
      "[LOG 20210201-13:02:26] training :: epoch: 698 train-loss: 0.003741928394167469\n",
      "[LOG 20210201-13:02:27] training :: epoch: 699 train-loss: 0.003774057634962866\n",
      "[LOG 20210201-13:02:27] training :: epoch: 700 train-loss: 0.0038073168924221625\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:02:32] backtest :: total return: 2.6189, monthly-sharpe: 1.1133, daily-sharpe: 0.7575.\n",
      "[LOG 20210201-13:02:33] training :: epoch: 701 train-loss: 0.003836852041646265\n",
      "[LOG 20210201-13:02:34] training :: epoch: 702 train-loss: 0.0038685768466586103\n",
      "[LOG 20210201-13:02:34] training :: epoch: 703 train-loss: 0.003903861202371235\n",
      "[LOG 20210201-13:02:34] training :: epoch: 704 train-loss: 0.003932892676227941\n",
      "[LOG 20210201-13:02:35] training :: epoch: 705 train-loss: 0.003966459046261242\n",
      "[LOG 20210201-13:02:35] training :: epoch: 706 train-loss: 0.004001094682070498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:02:35] training :: epoch: 707 train-loss: 0.004035265014793437\n",
      "[LOG 20210201-13:02:36] training :: epoch: 708 train-loss: 0.004071558843581722\n",
      "[LOG 20210201-13:02:36] training :: epoch: 709 train-loss: 0.004107613170232911\n",
      "[LOG 20210201-13:02:36] training :: epoch: 710 train-loss: 0.004143030257322467\n",
      "[LOG 20210201-13:02:36] training :: epoch: 711 train-loss: 0.004177566725187576\n",
      "[LOG 20210201-13:02:37] training :: epoch: 712 train-loss: 0.0042146652972755525\n",
      "[LOG 20210201-13:02:37] training :: epoch: 713 train-loss: 0.004253592754069429\n",
      "[LOG 20210201-13:02:37] training :: epoch: 714 train-loss: 0.004287252202630043\n",
      "[LOG 20210201-13:02:38] training :: epoch: 715 train-loss: 0.00432440061838581\n",
      "[LOG 20210201-13:02:38] training :: epoch: 716 train-loss: 0.004363104360751235\n",
      "[LOG 20210201-13:02:38] training :: epoch: 717 train-loss: 0.0044027946423739195\n",
      "[LOG 20210201-13:02:38] training :: epoch: 718 train-loss: 0.004441430971312981\n",
      "[LOG 20210201-13:02:39] training :: epoch: 719 train-loss: 0.004482127045496152\n",
      "[LOG 20210201-13:02:39] training :: epoch: 720 train-loss: 0.00451873242855072\n",
      "[LOG 20210201-13:02:39] training :: epoch: 721 train-loss: 0.004559999349741981\n",
      "[LOG 20210201-13:02:40] training :: epoch: 722 train-loss: 0.004601710637171681\n",
      "[LOG 20210201-13:02:40] training :: epoch: 723 train-loss: 0.004639749827149969\n",
      "[LOG 20210201-13:02:40] training :: epoch: 724 train-loss: 0.004683300046823346\n",
      "[LOG 20210201-13:02:41] training :: epoch: 725 train-loss: 0.004723699691777046\n",
      "[LOG 20210201-13:02:41] training :: epoch: 726 train-loss: 0.0047658172686799215\n",
      "[LOG 20210201-13:02:42] training :: epoch: 727 train-loss: 0.004809131218980138\n",
      "[LOG 20210201-13:02:42] training :: epoch: 728 train-loss: 0.0048514360454506595\n",
      "[LOG 20210201-13:02:42] training :: epoch: 729 train-loss: 0.00489558893829011\n",
      "[LOG 20210201-13:02:42] training :: epoch: 730 train-loss: 0.004938553624714796\n",
      "[LOG 20210201-13:02:43] training :: epoch: 731 train-loss: 0.004981510430717697\n",
      "[LOG 20210201-13:02:43] training :: epoch: 732 train-loss: 0.005024346397616542\n",
      "[LOG 20210201-13:02:43] training :: epoch: 733 train-loss: 0.005069942141954715\n",
      "[LOG 20210201-13:02:44] training :: epoch: 734 train-loss: 0.005111863979926476\n",
      "[LOG 20210201-13:02:44] training :: epoch: 735 train-loss: 0.005157762296641102\n",
      "[LOG 20210201-13:02:44] training :: epoch: 736 train-loss: 0.0052026149888451285\n",
      "[LOG 20210201-13:02:45] training :: epoch: 737 train-loss: 0.00524942372710659\n",
      "[LOG 20210201-13:02:45] training :: epoch: 738 train-loss: 0.005294758092182187\n",
      "[LOG 20210201-13:02:45] training :: epoch: 739 train-loss: 0.0053423334522029525\n",
      "[LOG 20210201-13:02:46] training :: epoch: 740 train-loss: 0.005388040722419436\n",
      "[LOG 20210201-13:02:46] training :: epoch: 741 train-loss: 0.005435628476194465\n",
      "[LOG 20210201-13:02:46] training :: epoch: 742 train-loss: 0.005479656607629015\n",
      "[LOG 20210201-13:02:47] training :: epoch: 743 train-loss: 0.005527418679915941\n",
      "[LOG 20210201-13:02:47] training :: epoch: 744 train-loss: 0.005574569947874317\n",
      "[LOG 20210201-13:02:47] training :: epoch: 745 train-loss: 0.005624392374347036\n",
      "[LOG 20210201-13:02:48] training :: epoch: 746 train-loss: 0.005673775664315774\n",
      "[LOG 20210201-13:02:48] training :: epoch: 747 train-loss: 0.00571853187508308\n",
      "[LOG 20210201-13:02:48] training :: epoch: 748 train-loss: 0.005766845039593486\n",
      "[LOG 20210201-13:02:49] training :: epoch: 749 train-loss: 0.0058170621319172475\n",
      "[LOG 20210201-13:02:49] training :: epoch: 750 train-loss: 0.0058670374385725995\n",
      "[LOG 20210201-13:02:49] training :: epoch: 751 train-loss: 0.005914000221169912\n",
      "[LOG 20210201-13:02:50] training :: epoch: 752 train-loss: 0.0059630104089872195\n",
      "[LOG 20210201-13:02:50] training :: epoch: 753 train-loss: 0.006010476750536607\n",
      "[LOG 20210201-13:02:50] training :: epoch: 754 train-loss: 0.006064791733828874\n",
      "[LOG 20210201-13:02:51] training :: epoch: 755 train-loss: 0.006112501508771227\n",
      "[LOG 20210201-13:02:51] training :: epoch: 756 train-loss: 0.006163716746064333\n",
      "[LOG 20210201-13:02:51] training :: epoch: 757 train-loss: 0.006214597208711963\n",
      "[LOG 20210201-13:02:52] training :: epoch: 758 train-loss: 0.006262141136595836\n",
      "[LOG 20210201-13:02:52] training :: epoch: 759 train-loss: 0.00631172894141995\n",
      "[LOG 20210201-13:02:52] training :: epoch: 760 train-loss: 0.0063659080769866705\n",
      "[LOG 20210201-13:02:53] training :: epoch: 761 train-loss: 0.006414347417795887\n",
      "[LOG 20210201-13:02:53] training :: epoch: 762 train-loss: 0.006466115018925988\n",
      "[LOG 20210201-13:02:53] training :: epoch: 763 train-loss: 0.006517186223600919\n",
      "[LOG 20210201-13:02:53] training :: epoch: 764 train-loss: 0.006567599950358272\n",
      "[LOG 20210201-13:02:54] training :: epoch: 765 train-loss: 0.006616530957846687\n",
      "[LOG 20210201-13:02:54] training :: epoch: 766 train-loss: 0.006669472580632338\n",
      "[LOG 20210201-13:02:54] training :: epoch: 767 train-loss: 0.00672056542064708\n",
      "[LOG 20210201-13:02:55] training :: epoch: 768 train-loss: 0.006772919629628842\n",
      "[LOG 20210201-13:02:55] training :: epoch: 769 train-loss: 0.006824649386824324\n",
      "[LOG 20210201-13:02:55] training :: epoch: 770 train-loss: 0.006873471357931311\n",
      "[LOG 20210201-13:02:55] training :: epoch: 771 train-loss: 0.006927492186570397\n",
      "[LOG 20210201-13:02:56] training :: epoch: 772 train-loss: 0.006977493677718135\n",
      "[LOG 20210201-13:02:56] training :: epoch: 773 train-loss: 0.007031083662206164\n",
      "[LOG 20210201-13:02:57] training :: epoch: 774 train-loss: 0.007081171318602104\n",
      "[LOG 20210201-13:02:57] training :: epoch: 775 train-loss: 0.007132484613416286\n",
      "[LOG 20210201-13:02:57] training :: epoch: 776 train-loss: 0.007184482054211772\n",
      "[LOG 20210201-13:02:58] training :: epoch: 777 train-loss: 0.007234774171732939\n",
      "[LOG 20210201-13:02:58] training :: epoch: 778 train-loss: 0.007287545398307534\n",
      "[LOG 20210201-13:02:58] training :: epoch: 779 train-loss: 0.007340414539122811\n",
      "[LOG 20210201-13:02:59] training :: epoch: 780 train-loss: 0.007390399553024998\n",
      "[LOG 20210201-13:02:59] training :: epoch: 781 train-loss: 0.007444417319045617\n",
      "[LOG 20210201-13:02:59] training :: epoch: 782 train-loss: 0.0074948706091023404\n",
      "[LOG 20210201-13:03:00] training :: epoch: 783 train-loss: 0.007544623902783944\n",
      "[LOG 20210201-13:03:00] training :: epoch: 784 train-loss: 0.007598864774291332\n",
      "[LOG 20210201-13:03:00] training :: epoch: 785 train-loss: 0.0076484056726957746\n",
      "[LOG 20210201-13:03:01] training :: epoch: 786 train-loss: 0.007698747186133495\n",
      "[LOG 20210201-13:03:01] training :: epoch: 787 train-loss: 0.007749458900294625\n",
      "[LOG 20210201-13:03:01] training :: epoch: 788 train-loss: 0.007800276713589063\n",
      "[LOG 20210201-13:03:02] training :: epoch: 789 train-loss: 0.00785376324963111\n",
      "[LOG 20210201-13:03:02] training :: epoch: 790 train-loss: 0.007903789241726581\n",
      "[LOG 20210201-13:03:02] training :: epoch: 791 train-loss: 0.007955462277795259\n",
      "[LOG 20210201-13:03:03] training :: epoch: 792 train-loss: 0.008006143491142072\n",
      "[LOG 20210201-13:03:03] training :: epoch: 793 train-loss: 0.008055875973346142\n",
      "[LOG 20210201-13:03:03] training :: epoch: 794 train-loss: 0.00810785609512375\n",
      "[LOG 20210201-13:03:04] training :: epoch: 795 train-loss: 0.008156489974890765\n",
      "[LOG 20210201-13:03:04] training :: epoch: 796 train-loss: 0.008206857045969138\n",
      "[LOG 20210201-13:03:04] training :: epoch: 797 train-loss: 0.008256319372986373\n",
      "[LOG 20210201-13:03:05] training :: epoch: 798 train-loss: 0.008308765621712575\n",
      "[LOG 20210201-13:03:05] training :: epoch: 799 train-loss: 0.00835445405055697\n",
      "[LOG 20210201-13:03:05] training :: epoch: 800 train-loss: 0.008406863189660586\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:03:10] backtest :: total return: 2.6189, monthly-sharpe: 1.1133, daily-sharpe: 0.7575.\n",
      "[LOG 20210201-13:03:12] training :: epoch: 801 train-loss: 0.008454160334972234\n",
      "[LOG 20210201-13:03:13] training :: epoch: 802 train-loss: 0.008502931262438115\n",
      "[LOG 20210201-13:03:13] training :: epoch: 803 train-loss: 0.008552057955127496\n",
      "[LOG 20210201-13:03:13] training :: epoch: 804 train-loss: 0.008602427999274088\n",
      "[LOG 20210201-13:03:14] training :: epoch: 805 train-loss: 0.008648803135236869\n",
      "[LOG 20210201-13:03:14] training :: epoch: 806 train-loss: 0.00869514406300508\n",
      "[LOG 20210201-13:03:14] training :: epoch: 807 train-loss: 0.008746063730751093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:03:15] training :: epoch: 808 train-loss: 0.008792432419096049\n",
      "[LOG 20210201-13:03:15] training :: epoch: 809 train-loss: 0.008844009528939541\n",
      "[LOG 20210201-13:03:15] training :: epoch: 810 train-loss: 0.008890355507341715\n",
      "[LOG 20210201-13:03:16] training :: epoch: 811 train-loss: 0.008935101724301394\n",
      "[LOG 20210201-13:03:16] training :: epoch: 812 train-loss: 0.008978734222742228\n",
      "[LOG 20210201-13:03:16] training :: epoch: 813 train-loss: 0.009028056779733071\n",
      "[LOG 20210201-13:03:17] training :: epoch: 814 train-loss: 0.009073250783750644\n",
      "[LOG 20210201-13:03:17] training :: epoch: 815 train-loss: 0.009118109081800167\n",
      "[LOG 20210201-13:03:17] training :: epoch: 816 train-loss: 0.009165725885675503\n",
      "[LOG 20210201-13:03:18] training :: epoch: 817 train-loss: 0.00920712765162954\n",
      "[LOG 20210201-13:03:18] training :: epoch: 818 train-loss: 0.009254417883662077\n",
      "[LOG 20210201-13:03:18] training :: epoch: 819 train-loss: 0.009300003508822275\n",
      "[LOG 20210201-13:03:19] training :: epoch: 820 train-loss: 0.009341119072185112\n",
      "[LOG 20210201-13:03:19] training :: epoch: 821 train-loss: 0.00938772320604095\n",
      "[LOG 20210201-13:03:19] training :: epoch: 822 train-loss: 0.009427627548575401\n",
      "[LOG 20210201-13:03:20] training :: epoch: 823 train-loss: 0.009470429808761064\n",
      "[LOG 20210201-13:03:20] training :: epoch: 824 train-loss: 0.009515893609764485\n",
      "[LOG 20210201-13:03:20] training :: epoch: 825 train-loss: 0.009558187439464606\n",
      "[LOG 20210201-13:03:21] training :: epoch: 826 train-loss: 0.009600245752013646\n",
      "[LOG 20210201-13:03:21] training :: epoch: 827 train-loss: 0.009641611160567174\n",
      "[LOG 20210201-13:03:21] training :: epoch: 828 train-loss: 0.009681639942125632\n",
      "[LOG 20210201-13:03:22] training :: epoch: 829 train-loss: 0.009723148810175749\n",
      "[LOG 20210201-13:03:22] training :: epoch: 830 train-loss: 0.009763744194060564\n",
      "[LOG 20210201-13:03:22] training :: epoch: 831 train-loss: 0.009802236651571898\n",
      "[LOG 20210201-13:03:23] training :: epoch: 832 train-loss: 0.009843464116924085\n",
      "[LOG 20210201-13:03:23] training :: epoch: 833 train-loss: 0.009877687570853876\n",
      "[LOG 20210201-13:03:23] training :: epoch: 834 train-loss: 0.009919529852385704\n",
      "[LOG 20210201-13:03:23] training :: epoch: 835 train-loss: 0.009956507549549524\n",
      "[LOG 20210201-13:03:24] training :: epoch: 836 train-loss: 0.009995194044537269\n",
      "[LOG 20210201-13:03:24] training :: epoch: 837 train-loss: 0.010031606607998792\n",
      "[LOG 20210201-13:03:24] training :: epoch: 838 train-loss: 0.010070436849043919\n",
      "[LOG 20210201-13:03:25] training :: epoch: 839 train-loss: 0.010104555135162977\n",
      "[LOG 20210201-13:03:25] training :: epoch: 840 train-loss: 0.010141337863527812\n",
      "[LOG 20210201-13:03:25] training :: epoch: 841 train-loss: 0.010177078871772839\n",
      "[LOG 20210201-13:03:26] training :: epoch: 842 train-loss: 0.010213861671777872\n",
      "[LOG 20210201-13:03:26] training :: epoch: 843 train-loss: 0.010244477217873702\n",
      "[LOG 20210201-13:03:26] training :: epoch: 844 train-loss: 0.010281017372527948\n",
      "[LOG 20210201-13:03:27] training :: epoch: 845 train-loss: 0.010317556416759124\n",
      "[LOG 20210201-13:03:27] training :: epoch: 846 train-loss: 0.010350243761562385\n",
      "[LOG 20210201-13:03:27] training :: epoch: 847 train-loss: 0.01038211419318731\n",
      "[LOG 20210201-13:03:28] training :: epoch: 848 train-loss: 0.010412457721451154\n",
      "[LOG 20210201-13:03:28] training :: epoch: 849 train-loss: 0.010449976624491123\n",
      "[LOG 20210201-13:03:28] training :: epoch: 850 train-loss: 0.010474463423284201\n",
      "[LOG 20210201-13:03:28] training :: epoch: 851 train-loss: 0.010507463907393126\n",
      "[LOG 20210201-13:03:29] training :: epoch: 852 train-loss: 0.010536868769962054\n",
      "[LOG 20210201-13:03:29] training :: epoch: 853 train-loss: 0.010569502742817769\n",
      "[LOG 20210201-13:03:29] training :: epoch: 854 train-loss: 0.010597292799502611\n",
      "[LOG 20210201-13:03:30] training :: epoch: 855 train-loss: 0.010629741618266473\n",
      "[LOG 20210201-13:03:30] training :: epoch: 856 train-loss: 0.010659040775723182\n",
      "[LOG 20210201-13:03:30] training :: epoch: 857 train-loss: 0.01068733729278812\n",
      "[LOG 20210201-13:03:31] training :: epoch: 858 train-loss: 0.010712600169846645\n",
      "[LOG 20210201-13:03:31] training :: epoch: 859 train-loss: 0.010740591069826713\n",
      "[LOG 20210201-13:03:31] training :: epoch: 860 train-loss: 0.010765313170850277\n",
      "[LOG 20210201-13:03:31] training :: epoch: 861 train-loss: 0.01079441667892612\n",
      "[LOG 20210201-13:03:32] training :: epoch: 862 train-loss: 0.010821163117025908\n",
      "[LOG 20210201-13:03:32] training :: epoch: 863 train-loss: 0.010845213304631986\n",
      "[LOG 20210201-13:03:32] training :: epoch: 864 train-loss: 0.010870210324915556\n",
      "[LOG 20210201-13:03:33] training :: epoch: 865 train-loss: 0.01089225451533611\n",
      "[LOG 20210201-13:03:33] training :: epoch: 866 train-loss: 0.010921882107280768\n",
      "[LOG 20210201-13:03:33] training :: epoch: 867 train-loss: 0.010939083885974608\n",
      "[LOG 20210201-13:03:34] training :: epoch: 868 train-loss: 0.010966370312067179\n",
      "[LOG 20210201-13:03:34] training :: epoch: 869 train-loss: 0.010984021203162579\n",
      "[LOG 20210201-13:03:35] training :: epoch: 870 train-loss: 0.011008437508000778\n",
      "[LOG 20210201-13:03:35] training :: epoch: 871 train-loss: 0.011030978773935484\n",
      "[LOG 20210201-13:03:35] training :: epoch: 872 train-loss: 0.011051718181429\n",
      "[LOG 20210201-13:03:36] training :: epoch: 873 train-loss: 0.011073271743953228\n",
      "[LOG 20210201-13:03:36] training :: epoch: 874 train-loss: 0.011093422089918302\n",
      "[LOG 20210201-13:03:36] training :: epoch: 875 train-loss: 0.011112607227495084\n",
      "[LOG 20210201-13:03:37] training :: epoch: 876 train-loss: 0.01113048317627265\n",
      "[LOG 20210201-13:03:37] training :: epoch: 877 train-loss: 0.011147076801325265\n",
      "[LOG 20210201-13:03:37] training :: epoch: 878 train-loss: 0.011168479704513\n",
      "[LOG 20210201-13:03:37] training :: epoch: 879 train-loss: 0.01118737949918096\n",
      "[LOG 20210201-13:03:38] training :: epoch: 880 train-loss: 0.011206122401815195\n",
      "[LOG 20210201-13:03:38] training :: epoch: 881 train-loss: 0.011222431651101662\n",
      "[LOG 20210201-13:03:38] training :: epoch: 882 train-loss: 0.011238675922728501\n",
      "[LOG 20210201-13:03:39] training :: epoch: 883 train-loss: 0.011252877553208517\n",
      "[LOG 20210201-13:03:39] training :: epoch: 884 train-loss: 0.011269087759921184\n",
      "[LOG 20210201-13:03:39] training :: epoch: 885 train-loss: 0.011287231571399249\n",
      "[LOG 20210201-13:03:40] training :: epoch: 886 train-loss: 0.011298698408959003\n",
      "[LOG 20210201-13:03:40] training :: epoch: 887 train-loss: 0.01131502900702449\n",
      "[LOG 20210201-13:03:40] training :: epoch: 888 train-loss: 0.011327669991610142\n",
      "[LOG 20210201-13:03:40] training :: epoch: 889 train-loss: 0.011342431418597698\n",
      "[LOG 20210201-13:03:41] training :: epoch: 890 train-loss: 0.01135438891987388\n",
      "[LOG 20210201-13:03:41] training :: epoch: 891 train-loss: 0.01136808075870459\n",
      "[LOG 20210201-13:03:41] training :: epoch: 892 train-loss: 0.011382694571064068\n",
      "[LOG 20210201-13:03:42] training :: epoch: 893 train-loss: 0.01139195105777337\n",
      "[LOG 20210201-13:03:42] training :: epoch: 894 train-loss: 0.011404011900035234\n",
      "[LOG 20210201-13:03:42] training :: epoch: 895 train-loss: 0.011414979297954302\n",
      "[LOG 20210201-13:03:42] training :: epoch: 896 train-loss: 0.011427764386798326\n",
      "[LOG 20210201-13:03:43] training :: epoch: 897 train-loss: 0.011434684435908612\n",
      "[LOG 20210201-13:03:43] training :: epoch: 898 train-loss: 0.011445192835078789\n",
      "[LOG 20210201-13:03:43] training :: epoch: 899 train-loss: 0.011452453246769996\n",
      "[LOG 20210201-13:03:44] training :: epoch: 900 train-loss: 0.011463866604921909\n",
      "stock_lstm_backtest\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "[LOG 20210201-13:03:49] backtest :: total return: 2.6189, monthly-sharpe: 1.1133, daily-sharpe: 0.7575.\n",
      "[LOG 20210201-13:03:50] training :: epoch: 901 train-loss: 0.011474779198089471\n",
      "[LOG 20210201-13:03:51] training :: epoch: 902 train-loss: 0.011480895444177665\n",
      "[LOG 20210201-13:03:51] training :: epoch: 903 train-loss: 0.011487677944107698\n",
      "[LOG 20210201-13:03:51] training :: epoch: 904 train-loss: 0.01149470111928307\n",
      "[LOG 20210201-13:03:51] training :: epoch: 905 train-loss: 0.011501309999193136\n",
      "[LOG 20210201-13:03:52] training :: epoch: 906 train-loss: 0.011505899807581535\n",
      "[LOG 20210201-13:03:52] training :: epoch: 907 train-loss: 0.011511768071124187\n",
      "[LOG 20210201-13:03:52] training :: epoch: 908 train-loss: 0.011520498468039127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20210201-13:03:52] training :: epoch: 909 train-loss: 0.011527747703859439\n",
      "[LOG 20210201-13:03:53] training :: epoch: 910 train-loss: 0.011529213462311488\n",
      "[LOG 20210201-13:03:53] training :: epoch: 911 train-loss: 0.011533541963077508\n",
      "[LOG 20210201-13:03:53] training :: epoch: 912 train-loss: 0.011538454546378208\n",
      "[LOG 20210201-13:03:54] training :: epoch: 913 train-loss: 0.01153933102838122\n",
      "[LOG 20210201-13:03:54] training :: epoch: 914 train-loss: 0.01154509956876819\n",
      "[LOG 20210201-13:03:54] training :: epoch: 915 train-loss: 0.01154886891778845\n",
      "[LOG 20210201-13:03:55] training :: epoch: 916 train-loss: 0.011550842175403466\n",
      "[LOG 20210201-13:03:55] training :: epoch: 917 train-loss: 0.011550667588240825\n"
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "lstm_model.train()\n",
    "\n",
    "# determine experiment timestamp\n",
    "exp_timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(0, no_epochs):\n",
    "\n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "            \n",
    "    # iterate over mini-batches\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        \n",
    "        # push mini-batch data to computation device\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "        # predict sequence output\n",
    "        prediction_batch = lstm_model(input_batch)\n",
    "        \n",
    "        # calculate batch loss\n",
    "        batch_loss = loss_function(prediction_batch, target_batch)\n",
    "\n",
    "        # run backward gradient calculation\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch loss\n",
    "        train_mini_batch_losses.append(batch_loss.data.item())\n",
    "            \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] training :: epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "        \n",
    "    # print epoch and save models\n",
    "    if epoch % train_checkpoint_epoch == 0 and epoch > 0:\n",
    "        \n",
    "        # determine the model predictions\n",
    "        train_predictions_list = compute_model_predictions(lstm_model, train_sequences_input)\n",
    "        \n",
    "        # convert the model predictions into trading signals\n",
    "        signal_data = compute_model_signals(train_predictions_list, train_stock_sequence_target_date, signal_threshold)\n",
    "        \n",
    "        # determine and prepare the in-sample market data \n",
    "        market_data = compute_market_data(stock_data_price, signal_data)\n",
    "        \n",
    "        # run the model backtest\n",
    "        backtest_result_stats, backtest_result_details = model_backtest(signal_data, market_data)\n",
    "        \n",
    "        # print the backtest performance \n",
    "        now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('\\n[LOG {}] backtest :: total return: {}, monthly-sharpe: {}, daily-sharpe: {}.'.format(str(now), str(np.round(backtest_result_stats['total_return'], 4)), str(np.round(backtest_result_stats['monthly_sharpe'], 4)), str(np.round(backtest_result_stats['daily_sharpe'], 4))))\n",
    "      \n",
    "        # save a checkoint of the current model parameters \n",
    "        model_name = '{}_many2one_lstm_model_{}.pth'.format(str(exp_timestamp), str(no_epochs))\n",
    "        torch.save(lstm_model.state_dict(), os.path.join(models_directory, model_name))\n",
    "        \n",
    "        # visualize the model in-sample predictions\n",
    "        filename = '{}_many2one_lstm_predictions_{}.png'.format(str(exp_timestamp), str(no_epochs))\n",
    "        plot_model_predictions(train_stock_sequence_target_date, train_sequences_target, train_predictions_list, epoch, filename)\n",
    "        \n",
    "        # collect and log the current training statistics\n",
    "        timestamp = dt.datetime.utcnow().strftime('%Y.%m.%d-%H:%M:%S')\n",
    "        training_statistics.iloc[int(epoch/train_checkpoint_epoch)-1] = [str(timestamp), str(epoch), str(mini_batch_size), str(learning_rate), str(np.round(train_epoch_loss, 8)), str(signal_threshold), str(backtest_result_stats['trade_signals']), str(np.round(backtest_result_stats['total_return'], 4)), str(np.round(backtest_result_stats['monthly_sharpe'], 4)), str(np.round(backtest_result_stats['daily_sharpe'], 4))]\n",
    "        \n",
    "        # save the log of the current experiment statistics\n",
    "        file_name = '{}_many2one_lstm_statistics_ep_{}.csv'.format(str(exp_timestamp), str(no_epochs))\n",
    "        training_statistics.to_csv(os.path.join(statistics_directory, file_name), sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, fantastic. The **training error** is **nicely going down**. We could train the network a couple more epochs until the error converges. But let's stay with the 1000 training epochs for now and continue with evaluating our trained model. Upon successful training let's visualize and inspect the training loss per training epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' prediction error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Prediction Error $\\mathcal{L}^{MSE}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Prediction Error $L^{MSE}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will conduct a visual comparison of the predicted daily returns to the actual ('true') daily returns. The comparison will encompass the daily returns of the in-sample time period as well as the returns of the out-of-sample time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Import and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will **import the trained model** that exhibits the **highest in-sample performance**. In addition, we will use the imported model to determine daily return predictions based on the population of **validation (\"out-of-sample\") input return** sequences. Finally, we will visualize the daily validation return **predictions** of the trained model in contrast to the validation (\"out-of-sample\") **target returns**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Import Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting our evaluation, let's **import the trained model** that exhibits the **highest in-sample performance** (or any other already trained model we aim to evaluate). Remember, that we stored a checkpoint of all model parameters for  each training epoch to our local model directory. Now, we will load the best performing checkpoint saved (which will hopefully also yield a good out-of-sample performance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the pre-trained model architecture\n",
    "lstm_model_trained = LSTMNet_Many2One().to(device)\n",
    "\n",
    "# set the pre-trained model name we aim to load\n",
    "lstm_model_name_trained = 'https://raw.githubusercontent.com/financial-data-science/CFDS-Notebooks/master/lab_15/models/best_many2one_lstm_model_10000.pth'\n",
    "\n",
    "# read pretrained model from the remote location\n",
    "lstm_model_trained_bytes = urllib.request.urlopen(lstm_model_name_trained)\n",
    "\n",
    "# load tensor from io.BytesIO object\n",
    "lstm_model_trained_buffer = io.BytesIO(lstm_model_trained_bytes.read())\n",
    "\n",
    "# load the pre-trained model paramaters \n",
    "lstm_model_trained.load_state_dict(torch.load(lstm_model_trained_buffer, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all keys (\"model parameters\") matched successfully. This indicates that the trained model was successfully imported into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Derive Model Return Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will conduct an **out-of-sample** validation of the trained and imported neural network model. Therefore, we will use the **trained model** to determine daily return predictions based on the population of **validation (\"out-of-sample\") input return** sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sequence output\n",
    "valid_predictions = lstm_model_trained(valid_sequences_input.to(device))\n",
    "\n",
    "# collect prediction batch results\n",
    "valid_predictions_list = valid_predictions.cpu().detach().numpy()[:, -1].tolist()\n",
    "\n",
    "# collect target batch results\n",
    "valid_targets_list = (valid_sequences_target).numpy()[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the daily validation predictions of the trained model in contrast to the **validation (\"out-of-sample\") target return** sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot ground-truth out-of-sample returns\n",
    "ax.plot(valid_stock_sequence_target_date[:, -1], valid_targets_list, color='C1', label='groundtruth (green)')\n",
    "\n",
    "# plot predicted out-of-sample returns\n",
    "ax.plot(valid_stock_sequence_target_date[:, -1], valid_predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set x-axis limits\n",
    "ax.set_xlim(valid_stock_sequence_target_date[:, -1].min(), valid_stock_sequence_target_date[:, -1].max())\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# set plot title\n",
    "plt.title('LSTM NN Many2One Out-of-Sample Prediction vs. Ground-Truth Market Prices', fontsize=10)\n",
    "\n",
    "# set axis labels\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[market price]', fontsize=8)\n",
    "\n",
    "# set axis ticks fontsize\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will backtest the python `bt` library. Python `bt` is a flexible, backtest framework that can be used to test quantitative trading strategies. In general, backtesting is the process of testing a strategy over a given data set (more details about the `bt` library can be found via: https://pmorissette.github.io/bt/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Derive Model Trade Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will the validation (\"out-of-sample\") **future return predictions $\\hat{r}_{t+1}$** into **trade signals** $\\phi(\\hat{r}_{t+1})$. Thereby, we will, similarly as above, interpret any positive future return prediction greater or equal a predefined threshold $t$ as a **\"long\" (buy)** signal, formally denoted by $r_{t+1} \\geq t$. Correspondingly, we will interpret any negative future return prediction lower or equal a predefined threshold $-t$, formally denoted by $r_{t+1} \\leq -t$ as a **\"short\" (sell)** signal. Formally, the out-of-sample trading signal are derived according to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "$\n",
    "\\\\\n",
    "\\phi(\\hat{r}_{t+1})=\n",
    "\\begin{cases}\n",
    "1.0 & \\textrm{(\"long signal\")}, & for & \\hat{r}_{t+1} > t\\\\\n",
    "-1.0 & \\textrm{(\"short signal\")}, & for & \\hat{r}_{t+1} < -t\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{r}_{t+1}$ denotes a by the model predicted future return at time $t+1$. To achieve this, let's specify the trade signal threshold $t$ as introduced above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_signal_threshold = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a following step, let's compute the trade signals $\\phi(\\hat{r}_{t+1})$ accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and prepare the model signal data \n",
    "valid_signal_data = pd.DataFrame(valid_predictions_list, columns=['PREDICTIONS'], index=valid_stock_sequence_target_date[:, -1])\n",
    "\n",
    "# derive trading signals from model predictions\n",
    "valid_signal_data['SIGNAL'] = np.where(valid_signal_data['PREDICTIONS'] >= valid_signal_threshold, 1.0, 0.0)\n",
    "valid_signal_data['SIGNAL'] = np.where(valid_signal_data['PREDICTIONS'] <= -valid_signal_threshold, -1.0, valid_signal_data['SIGNAL'])\n",
    "\n",
    "# offset the signal data\n",
    "valid_signal_data = valid_signal_data.set_index(valid_signal_data['SIGNAL'].index - pd.DateOffset(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top 10 rows of the prepared and offset trade signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_signal_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also **visually inspect the trade signals** derived from the model's out-of-sample return predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(valid_signal_data['SIGNAL'], lw=1.0, color='C3', label='LSTM trade signals')\n",
    "    \n",
    "# set axis ranges\n",
    "ax.set_xlim([valid_signal_data.index[0], valid_signal_data.index[-1]])\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_ylabel('[lstm trade signal]', fontsize=10)\n",
    "\n",
    "# rotate x-axis ticks\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set plot title\n",
    "plt.title('Invesco QQQ Trust Series 1 - LSTM \\'Many to One\\' Trade Signals', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful trade signal generation, let's determine the number of signal changes (trades to be executed) within the out-of-sample timeframe **03/2016** until **12/2017**, corresponding to a total in-sample timeframe of approx. **21 months** (9 + 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine number of signal changes\n",
    "len(list(itertools.groupby(valid_signal_data['SIGNAL'], lambda x: x > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average around **7** signal changes (trades) per month (148 signal changes / 21 months) within the out-of-sample time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Prepare Backtest Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare and trim the validation (\"out-of-sample\") market data of QQQ so that it can be utilized by the `bt` backtesting library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stock_market_data = pd.DataFrame(stock_data_price['QQQ'])\n",
    "valid_stock_market_data = valid_stock_market_data.rename(columns={'QQQ': 'PRICE'})\n",
    "valid_stock_market_data = valid_stock_market_data.set_index(pd.to_datetime(stock_data_price.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top 5 rows of the prepared validation QQQ market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stock_market_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim the prepared market data to the out-of-sample time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stock_market_data = valid_stock_market_data[valid_stock_market_data.index >= valid_signal_data.index[0]]\n",
    "valid_stock_market_data = valid_stock_market_data[valid_stock_market_data.index <= valid_signal_data.index[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the trimmed validation (\"out-of-sample\") market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the valiation market data\n",
    "ax.plot(valid_stock_market_data['PRICE'], color='#9b59b6')\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_ylabel('[QQQ closing price]', fontsize=10)\n",
    "\n",
    "# rotate x-tick labels\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels and limits\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([valid_stock_market_data.index[0], valid_stock_market_data.index[-1]])\n",
    "ax.set_ylabel('[adj. closing price]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('Invesco QQQ Trust Series 1 - Daily Historical Adjusted Returns', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's calculate the total return gained by following a simple **\"buy and hold\"** strategy in the out-of-sample time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(valid_stock_market_data.iloc[0]['PRICE'] - valid_stock_market_data.iloc[-1]['PRICE']) / valid_stock_market_data.iloc[0]['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, with such a simple strategy we would have been able to yield a total return of approx. **37.85%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. Prepare Model Backtest Strategy and Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared (i) the **validation trading signals** as well as (ii) the **validation market data** let's implement, similarly as already done above, the LSTM based trading strategy for the **out-of-sample time period**. We will name the strategy to be implemented `LSTMStrategyValid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStrategyValid(bt.Algo):\n",
    "    \n",
    "    def __init__(self, signals):\n",
    "        \n",
    "        # set class signals\n",
    "        self.signals = signals\n",
    "        \n",
    "    def __call__(self, target):\n",
    "        \n",
    "        if target.now in self.signals.index[1:]:\n",
    "            \n",
    "            # get actual signal\n",
    "            signal = self.signals[target.now]\n",
    "            \n",
    "            # set target weights according to signal\n",
    "            target.temp['weights'] = dict(PRICE=signal)\n",
    "            \n",
    "        # return True since we want to move on to the next timestep\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the **LSTM based trading strategy** of the out-of-sample time period using the prepared trade signals (`valid_signal_data`) of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_strategy = bt.Strategy('lstm', [bt.algos.SelectAll(), LSTMStrategy(valid_signal_data['SIGNAL']), bt.algos.Rebalance()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will **initialize the backtest** using both (i) the strategy and (ii) prepared out-of-sample market data (`valid_stock_market_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm = bt.Backtest(strategy=lstm_strategy, data=valid_stock_market_data, name='stock_lstm_backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's also prepare trade signals of a **\"baseline\" buy-and-hold trading strategy** for comparison purposes. Our buy-and-hold strategy sends a \"long\" (+1.0) signal at each time step of the out-of-sample time frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the lstm signal data\n",
    "valid_signal_data_base = valid_signal_data.copy(deep=True) \n",
    "\n",
    "# reset all trade signals to 1 \"long\"\n",
    "valid_signal_data_base['SIGNAL'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the **baseline trading strategy** of the out-of-sample time period using the prepared baseline trade signals `valid_signal_data_base`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the baseline trading strategy\n",
    "base_strategy = bt.Strategy('baseline', [bt.algos.SelectAll(), LSTMStrategy(valid_signal_data_base['SIGNAL']), bt.algos.Rebalance()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will **initialize the backtest** using both (i) the baseline strategy and (ii) prepared out-of-sample market data (`valid_stock_market_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the baseline backtest\n",
    "backtest_base = bt.Backtest(strategy=base_strategy, data=valid_stock_market_data, name='stock_base_backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. Evaluate Backtest Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful preparation of both backtests, let's run the backtests for each trading strategy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results = bt.run(backtest_lstm, backtest_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that went smooth. Let's now inspect and compare the individual backtest results and performance measures respectively. To print the backtest results inside of the notebook we will make use of the `display()` function provided by the `bt` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we collect the **detailed backtest performance** per timestep of the LSTM trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm_details = backtest_lstm.strategy.prices.to_frame(name='Rel. EQUITY')\n",
    "backtest_lstm_details['Abs. EQUITY'] = backtest_lstm.strategy.values # equity per timestep\n",
    "backtest_lstm_details['CASH'] = backtest_lstm.strategy.cash # cash per timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first 10 rows of the LSTM trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's visualize the **monthly returns** obtained by the LSTM based trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot heatmap of monthly returns generated by the strategy\n",
    "ax = sns.heatmap(backtest_lstm.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[month]', fontsize=10)\n",
    "ax.set_ylabel('[year]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "ax.set_title('Invesco QQQ Trust Series 1 - Monthly Returns LSTM Strategy', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also collect the **detailed backtest performance per timestep** of the baseline \"buy-and-hold\" trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_base_details = backtest_base.strategy.prices.to_frame(name='Rel. EQUITY')\n",
    "backtest_base_details['Abs. EQUITY'] = backtest_base.strategy.values # equity per timestep\n",
    "backtest_base_details['CASH'] = backtest_base.strategy.cash # cash per timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first 10 rows of the \"buy-and-hold\" trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_base_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's visualize the **monthly returns** obtained by the \"buy-and-hold\" trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot heatmap of monthly returns generated by the strategy\n",
    "ax = sns.heatmap(backtest_base.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[month]', fontsize=10)\n",
    "ax.set_ylabel('[year]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "ax.set_title('Invesco QQQ Trust Series 1 - Monthly Returns \\'buy-and-hold\\' Strategy', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also visually compare the **equity progression** of both trading 'strategies over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot lstm strategy equity progression \n",
    "ax.plot(backtest_lstm_details['Rel. EQUITY'], color='C1',lw=1.0, label='lstm strategy (green)')\n",
    "\n",
    "# plot baseline strategy equity progression\n",
    "ax.plot(backtest_base_details['Rel. EQUITY'], color='C2',lw=1.0, label='base strategy (red)')\n",
    "\n",
    "# rotate x-tick labels\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim(valid_stock_sequence_target_date[:, -1].min(), valid_stock_sequence_target_date[:, -1].max())\n",
    "ax.set_ylabel('[relative equity progression]', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# set plot title\n",
    "plt.title('Invesco QQQ Trust Series 1 - LSTM \\'Many to Many\\' Backtest Equity Progression', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Evaluation of Shallow vs. Deep RNN Models.**\n",
    "\n",
    "> Download the daily closing prices of the Invesco QQQ within the time frame starting from 01/01/1990 until 12/31/2019. In addition to the architecture of the lab notebook, evaluate further (more shallow as well as more deep) RNN architectures by either: **(1) re-moving/adding layers of LSTM cells**, and/or **(2) increasing/decreasing the dimensionality of the LSTM cells hidden state**. Train your model (using architectures you selected) for at least 20'000 training epochs but keep the following parameters unchanged (a) sequence length: 5 time-steps (days) and (b) train vs. test fraction: 0.9.\n",
    "\n",
    "> Analyze the prediction performance of the trained models in terms of training time and prediction accuracy. Furthermore, backtest the out-of-sample signals predicted by each of your models and evaluate them in terms of total return and equity progression. Which of your architecture results in the best performing model, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Training and Evaluation of Models Learned from Additional Stocks.**\n",
    "\n",
    "> Download the daily closing prices of at least **two additional NASDAQ stocks** (e.g., Alphabet, Facebook) within the time frame starting from 01/01/1990 until 12/31/2017. Pls. select two stocks that you are interested in to investigate (e.g. stocks that you may occasionally trade yourself). Learn an ’optimal’ **many-to-one RNN model** of both stocks and backtest their corresponding trade signals by following the approach outlined in the lab notebook regarding the QQQ. Pls. keep the train vs. test dataset fraction fixed to 0.9, all other parameters of the data preparation and model training can be changed.\n",
    "\n",
    "> Analyse the performance of the learned models in terms of their prediction accuracy as well as their out-of-sample backtest performance (e. g. the total return and equity progression). What architectures and corresponding training parameters result in the best performing models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, a step by step introduction into **design, implementation, training and evaluation** of a LSTM neural network based trading strategy is presented. In addition to the previous \"one-to-one\" training setup, a \"many-to-one\" training setup is introduced that allows to learn pattern or features across distinct input time series.\n",
    "\n",
    "The strategy trades a specific financial instrument based on its historical market prices. The degree of success of the implemented strategy is evaluated based in its backtest performance with particular focus on (1) the strategy's **total return** as well as (2) its **equity progression** over time. The code provided in this lab may provides a blueprint for the development and testing of more complex trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip3 install nbconvert\n",
    "!pip3 install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script cfds_lab_15.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
